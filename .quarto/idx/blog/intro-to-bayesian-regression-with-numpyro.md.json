{"title":"Bayesian Regression with numpyro","markdown":{"yaml":{"title":"Bayesian Regression with numpyro","date":"2024-02-16","categories":["python","bayesian stats","modeling"],"draft":false,"summary":"A simple example of regression with `numpyro`","images":["/static/images/numpyro_regression/sim_data.png","/static/images/numpyro_regression/numpyro_reg_graph.svg","/static/images/numpyro_regression/plot_trace.png","/static/images/numpyro_regression/plot_lm.png"],"layout":"PostLayout"},"headingText":"A Quick Example of Bayesian Regression with `numpyro`","containsRefs":false,"markdown":"\n\n\n## Why does this exist?\n\nAs a practitioner, learning to apply Bayesian techniques (on nights/weekends) is daunting. You need to simultaneously walk down two learning paths:\n\n1. Learning the math and the concepts\n2. Learning to express these concepts in code (using a PPL)\n\nIt's like studying astrodynamics while learning the controls of a spaceship. You're learning how to navigate in 3 dimensions while also learning how the spaceship works. But the reward for this effort is that we get to travel further than we could have before.\n\n### Learning the code\n\nI primarily write in Python, so there already exist several choices of Probabilistic Programming Languages (PPLs) and supporting libraries. `pymc` wins in terms of initial ease of use but the non-trivial examples look intimidating. Further, `pymc` has been undergoing several transitions lately that I just want something a little more stable.\n\nI chose `numpyro` because of how it seems to fit that 'sweet spot' between having a flexible low-level API and ease of use. Moreover, it personally looked more approachable to me once I understood some of the basic primitives.\n\nHowever, as a prerequisite to learning `numpyro` we also need some understanding of `jax`, `xarray`, and `pyro`. You also need to know `arviz` for handy plotting of bayesian models, but luckily `arviz` is compatible with other PPL's.\n\nI'm assuming you already understand linear regression but want to implement a Bayesian regression using `numpyro`.\n\nTo keep this short I'll be mainly focusing on `numpyro` usage rather than the theory.\n\n- [A Quick Example of Bayesian Regression with `numpyro`](#a-quick-example-of-bayesian-regression-with-numpyro)\n  - [Why does this exist?](#why-does-this-exist)\n    - [Learning the code](#learning-the-code)\n- [0. Our Simulated Data](#0-our-simulated-data)\n- [1. Specify the model on paper](#1-specify-the-model-on-paper)\n  - [Starting from the additive error model](#starting-from-the-additive-error-model)\n  - [Telling the data story with a joint probability distribution](#telling-the-data-story-with-a-joint-probability-distribution)\n  - [Add in our priors to our model](#add-in-our-priors-to-our-model)\n- [2. Specify the model in `numpyro`](#2-specify-the-model-in-numpyro)\n  - [Specify priors with `numpyro.sample()` and `numpyro.distributions`](#specify-priors-with-numpyrosample-and-numpyrodistributions)\n  - [View the Model with `render_model()`](#view-the-model-with-render_model)\n  - [`pymc` as a contrast](#pymc-as-a-contrast)\n- [3. Inference with `MCMC` and `NUTS`](#3-inference-with-mcmc-and-nuts)\n- [4. Results](#4-results)\n  - [Get a summary table with `.print_summary()` or `az.summary()`](#get-a-summary-table-with-print_summary-or-azsummary)\n  - [Inspect the trace with `az.plot_trace()`](#inspect-the-trace-with-azplot_trace)\n  - [Posterior Predictive](#posterior-predictive)\n  - [Plot the fitted regression line with `az.plot_lm`](#plot-the-fitted-regression-line-with-azplot_lm)\n- [Full Code](#full-code)\n\n# 0. Our Simulated Data\n\nWe'll use a toy dataset for this simple linear regression example, borrowed from the `pymc` example from [GLM:Linear Regression](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/GLM_linear.html)\n\n```python\nimport numpy as np\n# Simulate Data\nnp.random.seed(42)\n\nsize = 200\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\ntrue_regression_line = true_intercept + true_slope * x\n# Add noise term\ny = true_regression_line + np.random.normal(0, 0.5, size)\n\nplt.scatter(x, y, alpha=0.8)\nplt.plot(x, true_regression_line, c='r', label='True Regression Line')\nplt.legend();\n```\n\n![img](/static/images/numpyro_regression/sim_data.png)\n\n# 1. Specify the model on paper\n\nBefore you can write any code, you should draw out your understanding of the model. My workflow starts with a \"pen and paper\" expression of the model which then gets translated into `numpyro` later.\n\n### Starting from the additive error model\n\nYou may have learned the 'additive error' form of linear regression and think of a the model as:\n\n$$\n\\hat{y}_i = \\alpha + \\beta x_i + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma)\n$$\n\nwhere $\\hat{y}$ is an estimate of the conditional mean of $y$ given the data, or $\\mathbb{E}[y_i|x_i]$.\n\n### Telling the data story with a joint probability distribution\n\nNow, with Bayesian Inference, we must move beyond the additive error model and think more deeply into the data generating process and how to represent the model in terms of parameters and distributions. Ultimately what we aim to achieve is a fully specified model as a joint probability distribution.\n\nWe can restate the additive error model as the following:\n\n$$\n\\begin{aligned}\ny_i &\\sim N(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i\n\\end{aligned}\n$$\n\nWhat are the parameters in the additive error model? We can see that $\\alpha$, $\\beta$, and $\\sigma$ are the three unknown (_unobserved_) parameters while $x$ is simply our observed price data that we treat as a constant.\n\nFurther, we wish to encode our prior knowledge and get more educated guesses about what these parameters might be.\n\n### Add in our priors to our model\n\nRecall that with Bayesian inference, we're dealing with distributions. Our prior knowledge is codified in the prior distributions that we assign for each of our unobserved/latent variables ($\\alpha, \\beta, \\sigma$ in this case).\n\nHere is an example of our now fully specified model with prior distributions for each of the three parameters:\n\n$$\n\\begin{aligned}\ny_i &\\sim N(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i\\\\\n\\alpha &\\sim \\mathcal{N}(0,20)\\\\\n\\beta &\\sim \\mathcal{N}(0,20)\\\\\n\\sigma &\\sim \\operatorname{HalfCauchy}(10)\n\\end{aligned}\n$$\n\nYour choice of prior is incredibly important to your model but it's outside the scope of this intro. Our main goal is getting the model/machine running to the point where you can inspect and play around with it.\n\n# 2. Specify the model in `numpyro`\n\nOnce we have our model on paper, we can codify it using `numpyro` as a python function with JAX arrays `x` and `y` as inputs:\n\n```python\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS\nimport jax.numpy as jnp\nfrom jax import random\n\ndef numpyro_model(x, y):\n    # Specify priors with numpyro.sample()\n    intercept = numpyro.sample(\"alpha\", dist.Normal(0,20))\n    slope = numpyro.sample(\"beta\", dist.Normal(0,20))\n    sigma = numpyro.sample(\"sigma\", dist.HalfCauchy(10))\n\n    # Likelihood function, or p(y|mu, sigma, x)\n    mu = numpyro.deterministic('mu', intercept + slope * x)\n    # Note we set obs=y\n    likelihood = numpyro.sample(\"y\", dist.Normal(mu, sigma), obs=y)\n```\n\n### Specify priors with `numpyro.sample()` and `numpyro.distributions`\n\nEvery random variable gets a `numpyro.sample()` call which requires a prior distribution from `numpyro.distributions` as an argument. `mu` is simply a function of the other random variables and our data `x`.\n\nIf we wanted to save the values of `mu` for later, we could wrap it in a `numpyro.deterministic()` function like `mu = numpyro.deterministic('mu', intercept + slope * x)` but isn't required. In this case, I'll make it deterministic so we can use it for plotting, later.\n\nFor the `likelihood` random variable (our `y` in this case), we add in the parameter `obs=y`. When we want to do prediction (prior predictive checks or posterior prediction) you can set `y=None` in the function call.\n\n### View the Model with `render_model()`\n\nWe can use the `numpyro.render_model()` function to display the graphical model and how our data story is shaping up:\n\n```python\nnumpyro.render_model(numpyro_model, model_args=(x,y))\n```\n\n![img](/static/images/numpyro_regression/numpyro_reg_graph.svg)\n\nLight circles represent the unobserved/latent variables in our model, and we're interested in their posterior distribuitons. The dark circle represents our observed variables.\n\n### `pymc` as a contrast\n\nFor those curious how the same model looks in `pymc`:\n\n```python\n# model specifications in PyMC are wrapped in a `with` statement\nwith pm.Model() as model:\n    # Define priors\n    intercept = pm.Normal(\"alpha\", 0, sigma=20)\n    slope = pm.Normal(\"beta\", 0, sigma=20)\n    sigma = pm.HalfCauchy(\"sigma\", beta=10)\n\n    # Define likelihood\n    mu = intercept + slope * x\n    likelihood = pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y)\n```\n\nWhich is just a little less verbose than our `numpyro` model when it comes to specification.\n\n# 3. Inference with `MCMC` and `NUTS`\n\nFor users familiar with `pymc`, then at this point we're done and we can run our Markov Chain Monte Carlo (MCMC) algorithm of choice (in this case, we'll use the NUTS sampler) to get samples from the posterior distribution and make our model useful.\n\nWith `pymc` being a more 'batteries-included' library, we would simply do:\n\n```python\nwith model:\n    # draw 1000 posterior samples using NUTS sampling\n    idata = pm.sample(1000, nuts_sampler='pymc')\n```\n\nAnd we would get an `arviz`-friendly inference data object `idata` out that we can then inspect.\n\nIn `numpyro`, inference is slightly more involved. The equivalent to the `pymc` 'Inference button' would look something like:\n\n```python\n# Instantiate a `MCMC` object using a NUTS sampler\nmcmc = MCMC(sampler=NUTS(numpyro_model),\n            num_warmup=1000,\n            num_samples=1000,\n            num_chains=4)\n\n# Run the MCMC sampler and collect samples\nmcmc.run(rng_key=random.PRNGKey(seed=42),\n         x=x,\n         y=y)\n```\n\n`MCMC` acts are our entry point into the MCMC algorithms to sample from the posterior. In `numpyro` the model specification is separate from the 'inference engine' to give us the flexibility to choose from different inference methods (i.e. MCMC vs stochastic variational inference) and for tuning/control over the inference process.\n\nIf we want to get an `arviz.InferenceData` object like what we'd have from `pymc`, we can use the `arviz.from_numpyro` function:\n\n```python\n# Convert the `MCMC` object to an arviz.InferenceData object\nidata = az.from_numpyro(mcmc)\n```\n\nWhich will let us do more of data visualization with `arviz`.\n\n# 4. Results\n\nAt this point, we can collect samples from the posterior, inspect how our MCMC performed, and interpret the results. `numpyro` has some built in methods to help here, but most users will use the cross-language `arviz` library.\n\n### Get a summary table with `.print_summary()` or `az.summary()`\n\nWe can use either the `.print_summary()` method or the `az.summary()` function from `arviz` to get a table of the model results.\n\n```python\n# Get a summary of all non-deterministic random variables\nmcmc.print_summary()\n```\n\n```\n\n                 mean       std    median      5.0%     95.0%     n_eff     r_hat\n  intercept      0.92      0.07      0.92      0.82      1.03   1942.24      1.00\n      sigma      0.47      0.02      0.47      0.43      0.51   1866.17      1.00\n      slope      2.11      0.11      2.11      1.92      2.30   1907.32      1.00\n\nNumber of divergences: 0\n```\n\nThis table looks like the familiar regression summary table except for two columns: `n_eff` and `r_hat`.\n\n- `n_eff`: Number of effective samples, an indicator of how well the MCMC was able to sample from the posterior. Higher is better.\n- `r_hat`: Gelman-Rubin diagnostic $\\hat{R}$, an indicator of sampler convergence. Ideally $\\hat{R} \\approx 1.00$.\n\n### Inspect the trace with `az.plot_trace()`\n\nWe need to check to see how well our MCMC sampler performed. Aside from the `n_eff` and `r_hat` indicators from the summary table, we can use the `az.plot_trace` function to get a summary plot:\n\n```python\naz.plot_trace(mcmc, var_names=['intercept', 'slope', 'sigma'], figsize=(9,9));\n```\n\n![img](/static/images/numpyro_regression/plot_trace.png)\n\nThe plots on the left show the samples from the posterior distribution for each chain and the plots on the right trace the path of our sampler across the posterior distribution. The plots of right should lack a distinctive trend and ideally shouldn't have long flat lines going left to right. We see our happy fuzzy caterpillars on the right so our MCMC performed well, and if we saw something different it would indicate that something is wrong (poor prior specification, for instance).\n\nFor more examples of what we can use `arviz` to do, see the [arviz example gallery](https://python.arviz.org/en/stable/examples/index.html).\n\n### Posterior Predictive\n\nThe `Predictive` helper class will output posterior _predictive_ samples if you give it `posterior_samples` as an argument (for prior predictive sampling, we just let it know the number of samples)\n\n```python\n## get representative sample of posterior\nposterior_samples = mcmc.get_samples()\n\n# forecasted marginal aka posterior predictive\npredictive = Predictive(numpyro_model, posterior_samples=posterior_samples)\n# Note that we run the model with y=None\nposterior_predictive = predictive(random.PRNGKey(1), x=x, y=None)\n```\n\n### Plot the fitted regression line with `az.plot_lm`\n\nFor the last part of this intro we can plot the fitted line along with the highest density predictive interval (hpdi) by creating an `arviz.InferenceData` object from our model and passing along our `posterior_predictive` samples:\n\n```python\n# Convert the `MCMC` object into InferenceData and add in the `posterior_predictive`\n# as an additional dataset that we can use for our `arviz` plots\nidata = az.from_numpyro(mcmc, posterior_predictive=posterior_predictive)\n```\n\nThen we use `az.plot_lm` on this `arviz.InferenceData` object:\n\n```python\n# Customize arviz style and plot\naz.style.use('arviz-bluish')\naz.plot_lm(y='y', idata=idata, y_model='mu', kind_pp='hdi',\n           kind_model='hdi', figsize=(6,6), textsize=9)\n```\n\n![img](/static/images/numpyro_regression/plot_lm.png)\n\nHere we set the plot to display the `hdi` or the 90% highest density interval that captures our uncertainty in our parameter estimates (aka the _epistemic_ uncertainty).\n\n# Full Code\n\n```python\nimport numpy as np\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS\nimport jax.numpy as jnp\nfrom jax import random\n\n# 0. Simulate Data\n# ---\nnp.random.seed(42)\n\nsize = 200\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\ntrue_regression_line = true_intercept + true_slope * x\n# Add noise term\ny = true_regression_line + np.random.normal(0, 0.5, size)\n\n# Plot simulated data\nplt.scatter(x, y, alpha=0.8)\nplt.plot(x, true_regression_line, c='r', label='True Regression Line')\nplt.legend();\n\n# 2. Specify the model in numpyro\n# ---\ndef numpyro_model(x, y):\n    # Specify priors with numpyro.sample()\n    intercept = numpyro.sample(\"alpha\", dist.Normal(0,20))\n    slope = numpyro.sample(\"beta\", dist.Normal(0,20))\n    sigma = numpyro.sample(\"sigma\", dist.HalfCauchy(10))\n\n    # Likelihood function, or p(y|mu, sigma, x)\n    mu = numpyro.deterministic('mu', intercept + slope * x)\n    # Note we set obs=y\n    likelihood = numpyro.sample(\"y\", dist.Normal(mu, sigma), obs=y)\n\n# 3. Inference with MCMC and NUTS\n# ---\n# Instantiate a `MCMC` object using a NUTS sampler\nmcmc = MCMC(sampler=NUTS(numpyro_model),\n            num_warmup=1000,\n            num_samples=1000,\n            num_chains=4)\n\n# Run the MCMC sampler and collect samples\nmcmc.run(rng_key=random.PRNGKey(seed=42),\n         x=x,\n         y=y)\n\n# 4. Results\n# ---\n# Get a summary of all non-deterministic random variables\nmcmc.print_summary()\n\n## get representative sample of posterior\nposterior_samples = mcmc.get_samples()\n\n# forecasted marginal aka posterior predictive\npredictive = Predictive(numpyro_model, posterior_samples=posterior_samples)\n# Note that we run the model with y=None\nposterior_predictive = predictive(random.PRNGKey(1), x=x, y=None)\n\n# Convert the `MCMC` object into InferenceData and add in the `posterior_predictive`\n# as an additional dataset that we can use for our `arviz` plots\nidata = az.from_numpyro(mcmc, posterior_predictive=posterior_predictive)\n\n# Customize arviz style and plot\naz.style.use('arviz-bluish')\naz.plot_lm(y='y', idata=idata, y_model='mu', kind_pp='hdi',\n           kind_model='hdi', figsize=(6,6), textsize=9)\n```\n","srcMarkdownNoYaml":"\n\n# A Quick Example of Bayesian Regression with `numpyro`\n\n## Why does this exist?\n\nAs a practitioner, learning to apply Bayesian techniques (on nights/weekends) is daunting. You need to simultaneously walk down two learning paths:\n\n1. Learning the math and the concepts\n2. Learning to express these concepts in code (using a PPL)\n\nIt's like studying astrodynamics while learning the controls of a spaceship. You're learning how to navigate in 3 dimensions while also learning how the spaceship works. But the reward for this effort is that we get to travel further than we could have before.\n\n### Learning the code\n\nI primarily write in Python, so there already exist several choices of Probabilistic Programming Languages (PPLs) and supporting libraries. `pymc` wins in terms of initial ease of use but the non-trivial examples look intimidating. Further, `pymc` has been undergoing several transitions lately that I just want something a little more stable.\n\nI chose `numpyro` because of how it seems to fit that 'sweet spot' between having a flexible low-level API and ease of use. Moreover, it personally looked more approachable to me once I understood some of the basic primitives.\n\nHowever, as a prerequisite to learning `numpyro` we also need some understanding of `jax`, `xarray`, and `pyro`. You also need to know `arviz` for handy plotting of bayesian models, but luckily `arviz` is compatible with other PPL's.\n\nI'm assuming you already understand linear regression but want to implement a Bayesian regression using `numpyro`.\n\nTo keep this short I'll be mainly focusing on `numpyro` usage rather than the theory.\n\n- [A Quick Example of Bayesian Regression with `numpyro`](#a-quick-example-of-bayesian-regression-with-numpyro)\n  - [Why does this exist?](#why-does-this-exist)\n    - [Learning the code](#learning-the-code)\n- [0. Our Simulated Data](#0-our-simulated-data)\n- [1. Specify the model on paper](#1-specify-the-model-on-paper)\n  - [Starting from the additive error model](#starting-from-the-additive-error-model)\n  - [Telling the data story with a joint probability distribution](#telling-the-data-story-with-a-joint-probability-distribution)\n  - [Add in our priors to our model](#add-in-our-priors-to-our-model)\n- [2. Specify the model in `numpyro`](#2-specify-the-model-in-numpyro)\n  - [Specify priors with `numpyro.sample()` and `numpyro.distributions`](#specify-priors-with-numpyrosample-and-numpyrodistributions)\n  - [View the Model with `render_model()`](#view-the-model-with-render_model)\n  - [`pymc` as a contrast](#pymc-as-a-contrast)\n- [3. Inference with `MCMC` and `NUTS`](#3-inference-with-mcmc-and-nuts)\n- [4. Results](#4-results)\n  - [Get a summary table with `.print_summary()` or `az.summary()`](#get-a-summary-table-with-print_summary-or-azsummary)\n  - [Inspect the trace with `az.plot_trace()`](#inspect-the-trace-with-azplot_trace)\n  - [Posterior Predictive](#posterior-predictive)\n  - [Plot the fitted regression line with `az.plot_lm`](#plot-the-fitted-regression-line-with-azplot_lm)\n- [Full Code](#full-code)\n\n# 0. Our Simulated Data\n\nWe'll use a toy dataset for this simple linear regression example, borrowed from the `pymc` example from [GLM:Linear Regression](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/GLM_linear.html)\n\n```python\nimport numpy as np\n# Simulate Data\nnp.random.seed(42)\n\nsize = 200\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\ntrue_regression_line = true_intercept + true_slope * x\n# Add noise term\ny = true_regression_line + np.random.normal(0, 0.5, size)\n\nplt.scatter(x, y, alpha=0.8)\nplt.plot(x, true_regression_line, c='r', label='True Regression Line')\nplt.legend();\n```\n\n![img](/static/images/numpyro_regression/sim_data.png)\n\n# 1. Specify the model on paper\n\nBefore you can write any code, you should draw out your understanding of the model. My workflow starts with a \"pen and paper\" expression of the model which then gets translated into `numpyro` later.\n\n### Starting from the additive error model\n\nYou may have learned the 'additive error' form of linear regression and think of a the model as:\n\n$$\n\\hat{y}_i = \\alpha + \\beta x_i + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma)\n$$\n\nwhere $\\hat{y}$ is an estimate of the conditional mean of $y$ given the data, or $\\mathbb{E}[y_i|x_i]$.\n\n### Telling the data story with a joint probability distribution\n\nNow, with Bayesian Inference, we must move beyond the additive error model and think more deeply into the data generating process and how to represent the model in terms of parameters and distributions. Ultimately what we aim to achieve is a fully specified model as a joint probability distribution.\n\nWe can restate the additive error model as the following:\n\n$$\n\\begin{aligned}\ny_i &\\sim N(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i\n\\end{aligned}\n$$\n\nWhat are the parameters in the additive error model? We can see that $\\alpha$, $\\beta$, and $\\sigma$ are the three unknown (_unobserved_) parameters while $x$ is simply our observed price data that we treat as a constant.\n\nFurther, we wish to encode our prior knowledge and get more educated guesses about what these parameters might be.\n\n### Add in our priors to our model\n\nRecall that with Bayesian inference, we're dealing with distributions. Our prior knowledge is codified in the prior distributions that we assign for each of our unobserved/latent variables ($\\alpha, \\beta, \\sigma$ in this case).\n\nHere is an example of our now fully specified model with prior distributions for each of the three parameters:\n\n$$\n\\begin{aligned}\ny_i &\\sim N(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i\\\\\n\\alpha &\\sim \\mathcal{N}(0,20)\\\\\n\\beta &\\sim \\mathcal{N}(0,20)\\\\\n\\sigma &\\sim \\operatorname{HalfCauchy}(10)\n\\end{aligned}\n$$\n\nYour choice of prior is incredibly important to your model but it's outside the scope of this intro. Our main goal is getting the model/machine running to the point where you can inspect and play around with it.\n\n# 2. Specify the model in `numpyro`\n\nOnce we have our model on paper, we can codify it using `numpyro` as a python function with JAX arrays `x` and `y` as inputs:\n\n```python\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS\nimport jax.numpy as jnp\nfrom jax import random\n\ndef numpyro_model(x, y):\n    # Specify priors with numpyro.sample()\n    intercept = numpyro.sample(\"alpha\", dist.Normal(0,20))\n    slope = numpyro.sample(\"beta\", dist.Normal(0,20))\n    sigma = numpyro.sample(\"sigma\", dist.HalfCauchy(10))\n\n    # Likelihood function, or p(y|mu, sigma, x)\n    mu = numpyro.deterministic('mu', intercept + slope * x)\n    # Note we set obs=y\n    likelihood = numpyro.sample(\"y\", dist.Normal(mu, sigma), obs=y)\n```\n\n### Specify priors with `numpyro.sample()` and `numpyro.distributions`\n\nEvery random variable gets a `numpyro.sample()` call which requires a prior distribution from `numpyro.distributions` as an argument. `mu` is simply a function of the other random variables and our data `x`.\n\nIf we wanted to save the values of `mu` for later, we could wrap it in a `numpyro.deterministic()` function like `mu = numpyro.deterministic('mu', intercept + slope * x)` but isn't required. In this case, I'll make it deterministic so we can use it for plotting, later.\n\nFor the `likelihood` random variable (our `y` in this case), we add in the parameter `obs=y`. When we want to do prediction (prior predictive checks or posterior prediction) you can set `y=None` in the function call.\n\n### View the Model with `render_model()`\n\nWe can use the `numpyro.render_model()` function to display the graphical model and how our data story is shaping up:\n\n```python\nnumpyro.render_model(numpyro_model, model_args=(x,y))\n```\n\n![img](/static/images/numpyro_regression/numpyro_reg_graph.svg)\n\nLight circles represent the unobserved/latent variables in our model, and we're interested in their posterior distribuitons. The dark circle represents our observed variables.\n\n### `pymc` as a contrast\n\nFor those curious how the same model looks in `pymc`:\n\n```python\n# model specifications in PyMC are wrapped in a `with` statement\nwith pm.Model() as model:\n    # Define priors\n    intercept = pm.Normal(\"alpha\", 0, sigma=20)\n    slope = pm.Normal(\"beta\", 0, sigma=20)\n    sigma = pm.HalfCauchy(\"sigma\", beta=10)\n\n    # Define likelihood\n    mu = intercept + slope * x\n    likelihood = pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y)\n```\n\nWhich is just a little less verbose than our `numpyro` model when it comes to specification.\n\n# 3. Inference with `MCMC` and `NUTS`\n\nFor users familiar with `pymc`, then at this point we're done and we can run our Markov Chain Monte Carlo (MCMC) algorithm of choice (in this case, we'll use the NUTS sampler) to get samples from the posterior distribution and make our model useful.\n\nWith `pymc` being a more 'batteries-included' library, we would simply do:\n\n```python\nwith model:\n    # draw 1000 posterior samples using NUTS sampling\n    idata = pm.sample(1000, nuts_sampler='pymc')\n```\n\nAnd we would get an `arviz`-friendly inference data object `idata` out that we can then inspect.\n\nIn `numpyro`, inference is slightly more involved. The equivalent to the `pymc` 'Inference button' would look something like:\n\n```python\n# Instantiate a `MCMC` object using a NUTS sampler\nmcmc = MCMC(sampler=NUTS(numpyro_model),\n            num_warmup=1000,\n            num_samples=1000,\n            num_chains=4)\n\n# Run the MCMC sampler and collect samples\nmcmc.run(rng_key=random.PRNGKey(seed=42),\n         x=x,\n         y=y)\n```\n\n`MCMC` acts are our entry point into the MCMC algorithms to sample from the posterior. In `numpyro` the model specification is separate from the 'inference engine' to give us the flexibility to choose from different inference methods (i.e. MCMC vs stochastic variational inference) and for tuning/control over the inference process.\n\nIf we want to get an `arviz.InferenceData` object like what we'd have from `pymc`, we can use the `arviz.from_numpyro` function:\n\n```python\n# Convert the `MCMC` object to an arviz.InferenceData object\nidata = az.from_numpyro(mcmc)\n```\n\nWhich will let us do more of data visualization with `arviz`.\n\n# 4. Results\n\nAt this point, we can collect samples from the posterior, inspect how our MCMC performed, and interpret the results. `numpyro` has some built in methods to help here, but most users will use the cross-language `arviz` library.\n\n### Get a summary table with `.print_summary()` or `az.summary()`\n\nWe can use either the `.print_summary()` method or the `az.summary()` function from `arviz` to get a table of the model results.\n\n```python\n# Get a summary of all non-deterministic random variables\nmcmc.print_summary()\n```\n\n```\n\n                 mean       std    median      5.0%     95.0%     n_eff     r_hat\n  intercept      0.92      0.07      0.92      0.82      1.03   1942.24      1.00\n      sigma      0.47      0.02      0.47      0.43      0.51   1866.17      1.00\n      slope      2.11      0.11      2.11      1.92      2.30   1907.32      1.00\n\nNumber of divergences: 0\n```\n\nThis table looks like the familiar regression summary table except for two columns: `n_eff` and `r_hat`.\n\n- `n_eff`: Number of effective samples, an indicator of how well the MCMC was able to sample from the posterior. Higher is better.\n- `r_hat`: Gelman-Rubin diagnostic $\\hat{R}$, an indicator of sampler convergence. Ideally $\\hat{R} \\approx 1.00$.\n\n### Inspect the trace with `az.plot_trace()`\n\nWe need to check to see how well our MCMC sampler performed. Aside from the `n_eff` and `r_hat` indicators from the summary table, we can use the `az.plot_trace` function to get a summary plot:\n\n```python\naz.plot_trace(mcmc, var_names=['intercept', 'slope', 'sigma'], figsize=(9,9));\n```\n\n![img](/static/images/numpyro_regression/plot_trace.png)\n\nThe plots on the left show the samples from the posterior distribution for each chain and the plots on the right trace the path of our sampler across the posterior distribution. The plots of right should lack a distinctive trend and ideally shouldn't have long flat lines going left to right. We see our happy fuzzy caterpillars on the right so our MCMC performed well, and if we saw something different it would indicate that something is wrong (poor prior specification, for instance).\n\nFor more examples of what we can use `arviz` to do, see the [arviz example gallery](https://python.arviz.org/en/stable/examples/index.html).\n\n### Posterior Predictive\n\nThe `Predictive` helper class will output posterior _predictive_ samples if you give it `posterior_samples` as an argument (for prior predictive sampling, we just let it know the number of samples)\n\n```python\n## get representative sample of posterior\nposterior_samples = mcmc.get_samples()\n\n# forecasted marginal aka posterior predictive\npredictive = Predictive(numpyro_model, posterior_samples=posterior_samples)\n# Note that we run the model with y=None\nposterior_predictive = predictive(random.PRNGKey(1), x=x, y=None)\n```\n\n### Plot the fitted regression line with `az.plot_lm`\n\nFor the last part of this intro we can plot the fitted line along with the highest density predictive interval (hpdi) by creating an `arviz.InferenceData` object from our model and passing along our `posterior_predictive` samples:\n\n```python\n# Convert the `MCMC` object into InferenceData and add in the `posterior_predictive`\n# as an additional dataset that we can use for our `arviz` plots\nidata = az.from_numpyro(mcmc, posterior_predictive=posterior_predictive)\n```\n\nThen we use `az.plot_lm` on this `arviz.InferenceData` object:\n\n```python\n# Customize arviz style and plot\naz.style.use('arviz-bluish')\naz.plot_lm(y='y', idata=idata, y_model='mu', kind_pp='hdi',\n           kind_model='hdi', figsize=(6,6), textsize=9)\n```\n\n![img](/static/images/numpyro_regression/plot_lm.png)\n\nHere we set the plot to display the `hdi` or the 90% highest density interval that captures our uncertainty in our parameter estimates (aka the _epistemic_ uncertainty).\n\n# Full Code\n\n```python\nimport numpy as np\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS\nimport jax.numpy as jnp\nfrom jax import random\n\n# 0. Simulate Data\n# ---\nnp.random.seed(42)\n\nsize = 200\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\ntrue_regression_line = true_intercept + true_slope * x\n# Add noise term\ny = true_regression_line + np.random.normal(0, 0.5, size)\n\n# Plot simulated data\nplt.scatter(x, y, alpha=0.8)\nplt.plot(x, true_regression_line, c='r', label='True Regression Line')\nplt.legend();\n\n# 2. Specify the model in numpyro\n# ---\ndef numpyro_model(x, y):\n    # Specify priors with numpyro.sample()\n    intercept = numpyro.sample(\"alpha\", dist.Normal(0,20))\n    slope = numpyro.sample(\"beta\", dist.Normal(0,20))\n    sigma = numpyro.sample(\"sigma\", dist.HalfCauchy(10))\n\n    # Likelihood function, or p(y|mu, sigma, x)\n    mu = numpyro.deterministic('mu', intercept + slope * x)\n    # Note we set obs=y\n    likelihood = numpyro.sample(\"y\", dist.Normal(mu, sigma), obs=y)\n\n# 3. Inference with MCMC and NUTS\n# ---\n# Instantiate a `MCMC` object using a NUTS sampler\nmcmc = MCMC(sampler=NUTS(numpyro_model),\n            num_warmup=1000,\n            num_samples=1000,\n            num_chains=4)\n\n# Run the MCMC sampler and collect samples\nmcmc.run(rng_key=random.PRNGKey(seed=42),\n         x=x,\n         y=y)\n\n# 4. Results\n# ---\n# Get a summary of all non-deterministic random variables\nmcmc.print_summary()\n\n## get representative sample of posterior\nposterior_samples = mcmc.get_samples()\n\n# forecasted marginal aka posterior predictive\npredictive = Predictive(numpyro_model, posterior_samples=posterior_samples)\n# Note that we run the model with y=None\nposterior_predictive = predictive(random.PRNGKey(1), x=x, y=None)\n\n# Convert the `MCMC` object into InferenceData and add in the `posterior_predictive`\n# as an additional dataset that we can use for our `arviz` plots\nidata = az.from_numpyro(mcmc, posterior_predictive=posterior_predictive)\n\n# Customize arviz style and plot\naz.style.use('arviz-bluish')\naz.plot_lm(y='y', idata=idata, y_model='mu', kind_pp='hdi',\n           kind_model='hdi', figsize=(6,6), textsize=9)\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"intro-to-bayesian-regression-with-numpyro.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.52","theme":{"light":"cosmo"},"title-block-banner":true,"title":"Bayesian Regression with numpyro","date":"2024-02-16","categories":["python","bayesian stats","modeling"],"draft":false,"summary":"A simple example of regression with `numpyro`","images":["/static/images/numpyro_regression/sim_data.png","/static/images/numpyro_regression/numpyro_reg_graph.svg","/static/images/numpyro_regression/plot_trace.png","/static/images/numpyro_regression/plot_lm.png"],"layout":"PostLayout"},"extensions":{"book":{"multiFile":true}}}},"draft":false,"projectFormats":["html"]}