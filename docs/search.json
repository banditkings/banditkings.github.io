[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Nelson Tang",
    "section": "",
    "text": "Why Am I Writing This?\nIn a previous career, I found myself leading convoys in Kabul during one of the deadliest years of the war. There were plenty of threats - especially bad guys hoping to hurt us and anyone nearby. As a convoy commander, I was ultimately responsible for my team’s safety, and I relied heavily on our route-planning strategy to steer clear of danger.\nWe were lucky, I made it home and everyone under my command made it home safely…but was it because I was a good decision maker?\nWas I really equipped to make sound decisions in such high-stakes situations?\nAfter years of reflection, I realized that I relied too heavily on instinct and intuition. I didn’t acknowledge the potential benefits of quantitative methods to help me form more effective strategies, spot alternative options, and better understand uncertainty and risk. Undoing this mental model has been a long and arduous process.\nToday, I’m searching for advice and strategies I could have given my younger self, and to be a better decision maker as a mountaineer. What models can I use to make more effective decisions and manage risk?\nI’ll try to share the insights I’ve gained, along with other tips and tricks that come my way. If you’re willing to lend a hand, let’s connect and navigate this journey together.\n\n\nWhy Bandit Kings?\nOh, and if you’re wondering about the name of this website, “Bandit Kings” was inspired by a Koei game that simulated historical events in ancient China - a game that was, in turn, influenced by the classic novel, “The Water Margin.” The book is also responsible for the creation of the fantastic Suikoden series."
  },
  {
    "objectID": "blog/2021-11-25-pandas-conditional-merging.html",
    "href": "blog/2021-11-25-pandas-conditional-merging.html",
    "title": "Python Pandas - Merging With Wildcards and Conditions",
    "section": "",
    "text": "Updated 28 Jan 2022 to use a newer pandas crossjoin method with .merge(how='cross'), which works on pandas &gt; 1.2\nWhat happens when you want to merge (join) two dataframes together, but only if certain conditions are true? This is easy to do when you are simply looking for matching key-value pairs in two tables, but I had a real life scenario where I had complex combinations of joins to consider."
  },
  {
    "objectID": "blog/2021-11-25-pandas-conditional-merging.html#the-challenge",
    "href": "blog/2021-11-25-pandas-conditional-merging.html#the-challenge",
    "title": "Python Pandas - Merging With Wildcards and Conditions",
    "section": "The Challenge",
    "text": "The Challenge\nHere’s a simplified version of the issue I was facing:\n\nThe date in the left table was between two dates (a start and end date) in the second table\n…AND the values in two other columns matched each other, OR the column on the right table was equal to ‘ANY’ (aka a ‘wildcard’ value)\n\nFor a concrete example, let’s say you’re working for an apparel retailer, and you offer limited-time promotions where the customer can apply for a rebate if they buy a certain brand of jacket during the promotion period. The wrinkle: you also offer a global discount across ANY brand of jacket bought in a separate promotion.\nYou have a table of sales volume and you’re trying to map that data with a table of promotions that you’re offering:\n\nsales_volume_table\n\n\n\ndate\nquantity\nbrand\n\n\n\n\n2021-11-15\n1\nOutdoor\n\n\n2021-11-20\n2\nLeisure\n\n\n2021-11-25\n3\nAthletic\n\n\n2021-11-26\n2\nOutdoor\n\n\n\n\n\npromos_table\n\n\n\nstart_date\nend_date\nbrand\nrebate_per_unit\n\n\n\n\n2021-11-01\n2021-11-25\nANY\n3\n\n\n2021-11-25\n2021-11-26\nOutdoor\n5\n\n\n2021-12-29\n2021-12-30\nLeisure\n10\n\n\n\nYou can’t do a simple left join because of the ‘ANY’ option on the right table. One way of dealing with this is modifying the data in promos_table so that it covers all possible brand categories (i.e. Outdoor, Leisure, Athletic) but for the sake of argument let’s imagine that’s not feasible in the real-world example."
  },
  {
    "objectID": "blog/2021-11-25-pandas-conditional-merging.html#merge-everything-you-think-youll-need-and-sort-it-out-later",
    "href": "blog/2021-11-25-pandas-conditional-merging.html#merge-everything-you-think-youll-need-and-sort-it-out-later",
    "title": "Python Pandas - Merging With Wildcards and Conditions",
    "section": "Merge Everything You Think You’ll Need and Sort it Out Later",
    "text": "Merge Everything You Think You’ll Need and Sort it Out Later\nThe simplest thing I found is to merge everything you think you’ll need and then filter it out later. I tried dictionaries and set logic, but couldn’t find anything faster than doing the big join.\nSpecifically, you can do a Cartesian Product (aka a Cross Join), and here’s a great example from StackOverflow when faced with having to merge two pandas dataframes with a wildcard value. I’ll walk through each step below using the StackOverflow example and our sample scenario:\nimport pandas as pd\n\n# Create our two dataframes\nsales_volume_table = pd.DataFrame.from_dict([\n    {'date':'2021-11-15', 'quantity':1, 'brand':'Outdoor'},\n    {'date':'2021-11-20', 'quantity':2, 'brand':'Leisure'},\n    {'date':'2021-11-25', 'quantity':3, 'brand':'Athletic'},\n    {'date':'2021-11-26', 'quantity':2, 'brand':'Outdoor'},\n])\n\npromos_table = pd.DataFrame.from_dict([\n    {'start_date':'2021-11-01', 'end_date':'2021-11-25',\n    'brand':'ANY', 'rebate_per_unit':3},\n    {'start_date':'2021-11-25', 'end_date':'2021-11-26',\n    'brand':'Outdoor', 'rebate_per_unit':5},\n])\n\n# Create a column to join on and save the results with a Cartesian Product\nresults = sales_volume_table.merge(promos_table, how='cross')\nThe Cartesian Product matches every row in the right dataframe with every row in the left dataframe. Here’s the output below:\n\n\n\n\n\n\n\n\n\n\n\n\ndate\nquantity\nbrand_x\nstart_date\nend_date\nbrand_y\nrebate_per_unit\n\n\n\n\n2021-11-15\n1\nOutdoor\n2021-11-01\n2021-11-25\nANY\n3\n\n\n2021-11-15\n1\nOutdoor\n2021-11-25\n2021-11-26\nOutdoor\n5\n\n\n2021-11-20\n2\nLeisure\n2021-11-01\n2021-11-25\nANY\n3\n\n\n2021-11-20\n2\nLeisure\n2021-11-25\n2021-11-26\nOutdoor\n5\n\n\n2021-11-25\n3\nAthletic\n2021-11-01\n2021-11-25\nANY\n3\n\n\n2021-11-25\n3\nAthletic\n2021-11-25\n2021-11-26\nOutdoor\n5\n\n\n2021-11-26\n2\nOutdoor\n2021-11-01\n2021-11-25\nANY\n3\n\n\n2021-11-26\n2\nOutdoor\n2021-11-25\n2021-11-26\nOutdoor\n5\n\n\n\nAnd then once the results are joined together in this way, you can then apply all of your conditions using pandas indexing. I like using the query method since it’s a little easier to read.\n# Filter the results where the two columns match, OR the right column is 'ANY'\nresults = results.query(\"brand_x == brand_y | brand_y=='ANY'\")\n\n\n\n\n\n\n\n\n\n\n\n\ndate\nquantity\nbrand_x\nstart_date\nend_date\nbrand_y\nrebate_per_unit\n\n\n\n\n2021-11-15\n1\nOutdoor\n2021-11-01\n2021-11-25\nANY\n3\n\n\n2021-11-15\n1\nOutdoor\n2021-11-25\n2021-11-26\nOutdoor\n5\n\n\n2021-11-20\n2\nLeisure\n2021-11-01\n2021-11-25\nANY\n3\n\n\n2021-11-25\n3\nAthletic\n2021-11-01\n2021-11-25\nANY\n3\n\n\n2021-11-26\n2\nOutdoor\n2021-11-01\n2021-11-25\nANY\n3\n\n\n2021-11-26\n2\nOutdoor\n2021-11-25\n2021-11-26\nOutdoor\n5\n\n\n\nBut we’re not done yet - we still need to filter out the dates that are relevant, so we edit our above query to incorporate additional conditions, and do a little cleanup:\n# Instead of one long string, we can break up the query into multiple parts\ncondition1 = \"(brand_x == brand_y | brand_y=='ANY')\"\ncondition2 = \"(start_date &lt;= date &lt;= end_date)\"\nqry = condition1 + \" & \" + condition2\nresults = results.query(qry)\nWhich gives us our result (skipping the part where you drop some columns for clarity)\n\n\n\n\n\n\n\n\n\n\n\n\ndate\nquantity\nbrand_x\nstart_date\nend_date\nbrand_y\nrebate_per_unit\n\n\n\n\n2021-11-15\n1\nOutdoor\n2021-11-01\n2021-11-25\nANY\n3\n\n\n2021-11-20\n2\nLeisure\n2021-11-01\n2021-11-25\nANY\n3\n\n\n2021-11-25\n3\nAthletic\n2021-11-01\n2021-11-25\nANY\n3\n\n\n2021-11-26\n2\nOutdoor\n2021-11-25\n2021-11-26\nOutdoor\n5"
  },
  {
    "objectID": "blog/2021-11-25-pandas-conditional-merging.html#performance",
    "href": "blog/2021-11-25-pandas-conditional-merging.html#performance",
    "title": "Python Pandas - Merging With Wildcards and Conditions",
    "section": "Performance",
    "text": "Performance\nSince we had \\(4\\) rows in the left dataframe and \\(2\\) in the right, the result of the Cartesian Product is \\(4 \\times 2\\) or \\(8\\) rows long. Something to keep in mind if your datasets get large. So if you had 5K rows of sales data and 1K rows of promotions, you’d end up with 5M rows of data after this join.\nBut you don’t need to do a full Cartesian Product here, the key idea is to get the superset of all the data that would be relevant and then filter it down. In my real-world case I had 55k rows and 15k rows of promotions, and I had about 12 different conditions to check (a mix of date and wildcards). I started with nested for-loops, dictionaries, and set logic and it took about 30s on my 2018 MBP, but with some smarter filtering and joining with this method I was able to get the same operation done in &lt; 10 seconds."
  },
  {
    "objectID": "blog/2021-11-25-pandas-conditional-merging.html#sql-is-faster",
    "href": "blog/2021-11-25-pandas-conditional-merging.html#sql-is-faster",
    "title": "Python Pandas - Merging With Wildcards and Conditions",
    "section": "SQL Is Faster",
    "text": "SQL Is Faster\nBut instead of doing this in Pandas, it turns out this is trivial to do in SQL. A Cartesian Product of two tables in SQL is simply:\nSELECT *\nFROM sales_volume, promos\nAnd it’s even simpler to do your filtering with a WHERE clause, making the entire statement:\nSELECT *\nFROM sales_volume, promos\nWHERE (sales_volume.brand=promos.brand or promos.brand='ANY')\nAND (start_date &lt;= date AND date &lt;= end_date)\nOn my computer the pandas merging and filtering took about 4.7 ms while the sql query took 700 µs in SQLite, so a little under 7x improvement in performance."
  },
  {
    "objectID": "blog/2021-11-25-pandas-conditional-merging.html#key-takeaway",
    "href": "blog/2021-11-25-pandas-conditional-merging.html#key-takeaway",
    "title": "Python Pandas - Merging With Wildcards and Conditions",
    "section": "Key Takeaway",
    "text": "Key Takeaway\nThe bottom line to take away from this is to solve a problem when you want to conditionally join two dataframes and handle things like wildcards, the easiest thing to do is to big join and filter it down from there. Or use SQL instead of Pandas.\n\nFinal Python/Pandas Result:\nimport pandas as pd\n\n# Create our DataFrames\nsales_volume_table = pd.DataFrame.from_dict([\n    {'date':'2021-11-15', 'quantity':1, 'brand':'Outdoor'},\n    {'date':'2021-11-20', 'quantity':2, 'brand':'Leisure'},\n    {'date':'2021-11-25', 'quantity':3, 'brand':'Athletic'},\n    {'date':'2021-11-26', 'quantity':2, 'brand':'Outdoor'},\n])\n\npromos_table = pd.DataFrame.from_dict([\n    {'start_date':'2021-11-01', 'end_date':'2021-11-25',\n     'brand':'ANY', 'rebate_per_unit':3},\n    {'start_date':'2021-11-25', 'end_date':'2021-11-26',\n     'brand':'Outdoor', 'rebate_per_unit':5},\n])\n\n# Merge it all. Notice you don't need to set a join key!\nresults = sales_volume_table.merge(promos_table, how='cross')\n# And Filter it Down\ncondition1 = \"(brand_x == brand_y | brand_y=='ANY')\"\ncondition2 = \"(start_date &lt;= date &lt;= end_date)\"\nqry = condition1 + \" & \" + condition2\nresults = results.query(qry)\n\n\nSame Thing, but Faster with SQL and SQLite\nimport sqlite3\n\n# Create a SQL connection to a local SQLite database\ncon = sqlite3.connect(\"sales.db\")\n\n# Add dataframes as tables in this database\nsales_volume_table.to_sql(\"sales_volume\", con, index=False, if_exists='replace')\npromos_table.to_sql(\"promos\", con, index=False, if_exists='replace')\n\n# Results in a 5x speedup from pandas!\nsql = \"\"\"SELECT *\nFROM sales_volume, promos\nWHERE (sales_volume.brand=promos.brand or promos.brand='ANY')\nAND (start_date &lt;= date AND date &lt;= end_date)\"\"\"\n\nresults = pd.read_sql_query(sql, con)\nresults\n\n\nBonus: Even Faster with SQL and DuckDB\nRather than use SQLite, you could instead use DuckDB, which is a fast in-memory database technology that lets you query dataframes, CSV files, and Parquet files as super-high performance SQL. Here’s a snippet:\nimport duckdb\n\n# connect to an in-memory database\ncon = duckdb.connect()\n\nsql = \"\"\"SELECT *\nFROM sales_volume_table, promos_table\nWHERE (sales_volume_table.brand=promos_table.brand or promos_table.brand='ANY')\nAND (start_date &lt;= date AND date &lt;= end_date)\"\"\"\n\n# query the Pandas DataFrames \"sales_volume_table\" and \"promos_table\"\nresults = con.execute(sql).df()\nresults"
  },
  {
    "objectID": "blog/2021-04-16-plotly-in-markdown.html",
    "href": "blog/2021-04-16-plotly-in-markdown.html",
    "title": "Adding Plotly Charts to Markdown",
    "section": "",
    "text": "Quick reference that shows options to add plotly charts to a markdown document. This assumes that your markdown interpreter can handle raw html (like Jekyll or Hugo)."
  },
  {
    "objectID": "blog/2021-04-16-plotly-in-markdown.html#add-raw-html-output-to-markdown",
    "href": "blog/2021-04-16-plotly-in-markdown.html#add-raw-html-output-to-markdown",
    "title": "Adding Plotly Charts to Markdown",
    "section": "Add raw html output to markdown",
    "text": "Add raw html output to markdown\nThanks to this article from http://www.kellieottoboni.com - I updated the syntax to use the latest version of plotly because you don’t have to fiddle with plotly offline versions anymore."
  },
  {
    "objectID": "blog/2021-04-16-plotly-in-markdown.html#steps",
    "href": "blog/2021-04-16-plotly-in-markdown.html#steps",
    "title": "Adding Plotly Charts to Markdown",
    "section": "Steps:",
    "text": "Steps:\n\n1. Create a Plotly figure object:\n# From https://plotly.com/python/line-charts/\nimport plotly.express as px\n\ndf = px.data.gapminder().query(\"continent=='Oceania'\")\nfig = px.line(df, x=\"year\", y=\"lifeExp\", color='country')\nfig.show()\n\n\n2. Get HTML from the figure object\nThen you want to get the raw html using either the to_html() method or the write_html method, with some flags that should honestly be default:\n# Output html that you can copy paste\nfig.to_html(full_html=False, include_plotlyjs='cdn')\n# Saves a html doc that you can copy paste\nfig.write_html(\"output.html\", full_html=False, include_plotlyjs='cdn')\nThese two flags:\n\ninclude_plotlyjs='cdn'\nfull_html=False\n\nhelps you avoid creating a huge html document and from including 3MB of the plotly javascript stuff - total overkill.\n\n\n3. Copy and paste directly into your markdown\n\nJekyll: When I was using Jekyll, I could paste in the raw html directly into markdown and it would parse it.\nHugo: Check your Hugo template, but you’ll need to use or make a shortcode for raw html, like this example from “Make with Hugo”:\n\n\nNote: Make sure that your flavor of markdown supports raw html.\n\nHere’s the output, copied and pasted into a markdown document\n                        \n                                            \nTested on Plotly 4.14.3\nReference:\n\nInspiration from http://www.kellieottoboni.com\nPlotly Documentation with additional details on html output"
  },
  {
    "objectID": "blog/2021-05-08-dev-environment-in-wsl.html",
    "href": "blog/2021-05-08-dev-environment-in-wsl.html",
    "title": "Getting Started with WSL for Data Analysis",
    "section": "",
    "text": "img\nI spent half an afternoon playing around with WSL2 since I’m considering switching from Mac to PC for work, primarily due to my work’s heavy reliance on the MS SSAS Tabular models as the ‘system of the truth’ for our data analysis. You can query these models through python with a little work on a Windows PC but I couldn’t get all of the dependencies to work on my Mac.\nSo here we are. I’m saving this as my checklist of making a decent dev environment using WSL2 where I can do a lot of my ad-hoc analysis in Python/Julia.\nHere are my requirements:"
  },
  {
    "objectID": "blog/2021-05-08-dev-environment-in-wsl.html#installing-wsl-and-linux-distro",
    "href": "blog/2021-05-08-dev-environment-in-wsl.html#installing-wsl-and-linux-distro",
    "title": "Getting Started with WSL for Data Analysis",
    "section": "Installing WSL and Linux Distro",
    "text": "Installing WSL and Linux Distro\nFollow the official docs from Microsoft to enable WSL2 and install a Linux distribution. I tried both Debian and Ubuntu and chose Ubuntu from the Microsoft Store.\n\nWSL Installation Instructions\nWindows Terminal\n\nI use One Half Dark Theme with Acrylic"
  },
  {
    "objectID": "blog/2021-05-08-dev-environment-in-wsl.html#download-and-install-fonts",
    "href": "blog/2021-05-08-dev-environment-in-wsl.html#download-and-install-fonts",
    "title": "Getting Started with WSL for Data Analysis",
    "section": "Download and Install Fonts",
    "text": "Download and Install Fonts\nI downloaded these manually as .ttf files in Windows since we’ll be using them in windows programs (VSCode, Windows Terminal).\n\nFiraCode NF Download Link"
  },
  {
    "objectID": "blog/2021-05-08-dev-environment-in-wsl.html#set-up-zshell",
    "href": "blog/2021-05-08-dev-environment-in-wsl.html#set-up-zshell",
    "title": "Getting Started with WSL for Data Analysis",
    "section": "Set up Zshell",
    "text": "Set up Zshell\nLet’s switch from bash to zshell, grab oh my zsh, download fonts, set up Powerlevel10k.\n# Update apt-get if this is a fresh Linux distro install\nsudo apt-get update\n# Install Git\nsudo apt-get install git\n# Install Zshell\nsudo apt-get install zsh\n# Install oh my zsh\nsh -c \"$(wget https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)\"\n# Install powerlevel10k\ngit clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k\n# Install tree\nsudo apt-get install tree\nIf you run into trouble:\n\nZshell docs\noh my zsh docs\nPowerlevel10k docs\n\n\nAdd personal macros\ntouch ~/.personal_macros\necho 'source ~/.personal_macros' &gt;&gt; ~/.zshrc\necho 'alias open=\"explorer.exe\"' &gt;&gt; ~/.personal_macros"
  },
  {
    "objectID": "blog/2021-05-08-dev-environment-in-wsl.html#programming-languages",
    "href": "blog/2021-05-08-dev-environment-in-wsl.html#programming-languages",
    "title": "Getting Started with WSL for Data Analysis",
    "section": "Programming Languages",
    "text": "Programming Languages\nAfter the shell’s installed, time to add Python through Conda and Julia.\n\nMiniconda\nI will use Miniconda just for the package and environment management, don’t need the rest of the Anaconda stuff.\n\nFollow the linux instructions for miniconda here\n\nsh -c \"$(wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh 0O -)\"\nsh Miniconda3-latest-Linux-x86_64.sh\n\n\nJulia Setup\nFor julia install we’ll use the jill.py method which is a community supported cross-platform way to simply install the latest version of Julia. Handles symlinks so you don’t have to futz around with PATH and all the other frustrations with getting started.\npip install jill --user -U\necho 'export PATH=$PATH:/home/$USERNAME/.local/bin' &gt;&gt; ~/.personal_macros\nsource ~/.zshrc\njill install\nHelpful to install a few common packages all at once in Julia. Open up Julia, hit the ] key to go to Pkg management mode and:\n(@v1.6) pkg&gt; add Plots Gadfly DataFrames XLSX CSV RDatasets Parquet Pluto IJulia DataFramesMeta\n\n\nVSCode\nNow, it’s finally time to add VSCode for Windows\n\nDownload VSCode\n\nSet Fonts (FuraMono Nerd Font)\nFollow the recommendations for extensions, but add:\n\nMarkdown Preview Enhanced\n\n\n\n\n\nOriginal Inspiration\nCredit: John Woodruff’s Far More Epic Development Environment Using WSL2"
  },
  {
    "objectID": "blog/intro-to-bayesian-regression-with-numpyro.html",
    "href": "blog/intro-to-bayesian-regression-with-numpyro.html",
    "title": "Bayesian Regression with numpyro",
    "section": "",
    "text": "As a practitioner, learning to apply Bayesian techniques (on nights/weekends) is daunting. You need to simultaneously walk down two learning paths:\n\nLearning the math and the concepts\nLearning to express these concepts in code (using a PPL)\n\nIt’s like studying astrodynamics while learning the controls of a spaceship. You’re learning how to navigate in 3 dimensions while also learning how the spaceship works. But the reward for this effort is that we get to travel further than we could have before.\n\n\nI primarily write in Python, so there already exist several choices of Probabilistic Programming Languages (PPLs) and supporting libraries. pymc wins in terms of initial ease of use but the non-trivial examples look intimidating. Further, pymc has been undergoing several transitions lately that I just want something a little more stable.\nI chose numpyro because of how it seems to fit that ‘sweet spot’ between having a flexible low-level API and ease of use. Moreover, it personally looked more approachable to me once I understood some of the basic primitives.\nHowever, as a prerequisite to learning numpyro we also need some understanding of jax, xarray, and pyro. You also need to know arviz for handy plotting of bayesian models, but luckily arviz is compatible with other PPL’s.\nI’m assuming you already understand linear regression but want to implement a Bayesian regression using numpyro.\nTo keep this short I’ll be mainly focusing on numpyro usage rather than the theory."
  },
  {
    "objectID": "blog/intro-to-bayesian-regression-with-numpyro.html#why-does-this-exist",
    "href": "blog/intro-to-bayesian-regression-with-numpyro.html#why-does-this-exist",
    "title": "Bayesian Regression with numpyro",
    "section": "",
    "text": "As a practitioner, learning to apply Bayesian techniques (on nights/weekends) is daunting. You need to simultaneously walk down two learning paths:\n\nLearning the math and the concepts\nLearning to express these concepts in code (using a PPL)\n\nIt’s like studying astrodynamics while learning the controls of a spaceship. You’re learning how to navigate in 3 dimensions while also learning how the spaceship works. But the reward for this effort is that we get to travel further than we could have before.\n\n\nI primarily write in Python, so there already exist several choices of Probabilistic Programming Languages (PPLs) and supporting libraries. pymc wins in terms of initial ease of use but the non-trivial examples look intimidating. Further, pymc has been undergoing several transitions lately that I just want something a little more stable.\nI chose numpyro because of how it seems to fit that ‘sweet spot’ between having a flexible low-level API and ease of use. Moreover, it personally looked more approachable to me once I understood some of the basic primitives.\nHowever, as a prerequisite to learning numpyro we also need some understanding of jax, xarray, and pyro. You also need to know arviz for handy plotting of bayesian models, but luckily arviz is compatible with other PPL’s.\nI’m assuming you already understand linear regression but want to implement a Bayesian regression using numpyro.\nTo keep this short I’ll be mainly focusing on numpyro usage rather than the theory."
  },
  {
    "objectID": "blog/2019-10-19-odbc-in-osx.html",
    "href": "blog/2019-10-19-odbc-in-osx.html",
    "title": "How to set up ODBC in Mac OS to connect to MS SQL Server for use with Python and R",
    "section": "",
    "text": "My local data mart runs on MS SQL Server, and I want to pull data directly into R or Python for data analysis. This is really easy on Windows with its’ built-in ODBC manager, but I spent a weekend figuring out how to do this after switching to OSX. A lot of documentation out there is old (from 2012), so I decided to make this for anyone still looking for an answer in 2019.\nBelow are my notes for a quickstart setup for getting your MacOS (OSX) machine set up to connect to a Microsoft SQL Server via ODBC for use with R’s ODBC library and Python’s PyODBC library. This assumes you have homebrew installed to manage your packages and you have the necessary admin rights on your machine."
  },
  {
    "objectID": "blog/2019-10-19-odbc-in-osx.html#install-unixodbc",
    "href": "blog/2019-10-19-odbc-in-osx.html#install-unixodbc",
    "title": "How to set up ODBC in Mac OS to connect to MS SQL Server for use with Python and R",
    "section": "1. Install unixodbc",
    "text": "1. Install unixodbc\nI like unixodbc as the ODBC manager, it just works. Install with your shell/terminal:\nbrew install unixodbc\n\nUnixODBC docs: link"
  },
  {
    "objectID": "blog/2019-10-19-odbc-in-osx.html#install-freetds",
    "href": "blog/2019-10-19-odbc-in-osx.html#install-freetds",
    "title": "How to set up ODBC in Mac OS to connect to MS SQL Server for use with Python and R",
    "section": "2. Install FreeTDS",
    "text": "2. Install FreeTDS\nAfter getting an ODBC manager, you’ll need drivers. FreeTDS works. In your shell/terminal:\nbrew install FreeTDS\n\nFreeTDS userguide: link"
  },
  {
    "objectID": "blog/2019-10-19-odbc-in-osx.html#locate-your-odbc-installation-with-odbcinst--j",
    "href": "blog/2019-10-19-odbc-in-osx.html#locate-your-odbc-installation-with-odbcinst--j",
    "title": "How to set up ODBC in Mac OS to connect to MS SQL Server for use with Python and R",
    "section": "3. Locate your odbc installation with odbcinst -j",
    "text": "3. Locate your odbc installation with odbcinst -j\nAfter installing unixodbc as your odbc manager and freeTDS for drivers, you’ll need to edit your connections in the .odbc.ini file. You can find out where this is by using the odbcinst -j command.\nIn your shell/terminal:\nodbcinst -j\n\nunixODBC 2.3.7\nDRIVERS............: /etc/odbcinst.ini\nSYSTEM DATA SOURCES: /etc/odbc.ini\nFILE DATA SOURCES..: /etc/ODBCDataSources\nUSER DATA SOURCES..: /Users/yourname/.odbc.ini\nSQLULEN Size.......: 8\nSQLLEN Size........: 8\nSQLSETPOSIROW Size.: 8\nThe item in USER DATA SOURCES..: is what you’re looking for. Don’t know why, but Python and R like to use that one first before looking elsewhere. Navigate there and edit the .odbc.ini file using your favorite text editor. Here’s nano:\nnano /Users/yourname/.odbc.ini"
  },
  {
    "objectID": "blog/2019-10-19-odbc-in-osx.html#update-the-.odbc.ini-file",
    "href": "blog/2019-10-19-odbc-in-osx.html#update-the-.odbc.ini-file",
    "title": "How to set up ODBC in Mac OS to connect to MS SQL Server for use with Python and R",
    "section": "4. Update the .odbc.ini file",
    "text": "4. Update the .odbc.ini file\nUse your text editor and enter in the required server, port, and the Data Source Name (DSN) in brackets. We’ll use the DSN to connect\n[my_server]\nDescription = my_server\nTDS_Version = 7.4\nDriver = /usr/local/lib/libtdsodbc.so\nServer = YOUR.SERVERNAME.HERE.com\nPort = 1234\n\nWhat is a DSN: link"
  },
  {
    "objectID": "blog/2019-10-19-odbc-in-osx.html#python-to-connect-to-your-ms-sql-server-with-pyodbc",
    "href": "blog/2019-10-19-odbc-in-osx.html#python-to-connect-to-your-ms-sql-server-with-pyodbc",
    "title": "How to set up ODBC in Mac OS to connect to MS SQL Server for use with Python and R",
    "section": "5. Python to connect to your MS SQL Server with pyodbc",
    "text": "5. Python to connect to your MS SQL Server with pyodbc\nimport pandas as pd\nimport pyodbc\n\ncnxn = pyodbc.connect('dsn=my_server;'\n                      'Trusted_Connection=yes;')\nquery= \"yourqueryhere\"\n\nres = pd.read_sql_query(query,cnxn)\n\ncnxn.close()"
  },
  {
    "objectID": "blog/2019-10-19-odbc-in-osx.html#r-to-connect-to-your-ms-sql-server-with-odbc",
    "href": "blog/2019-10-19-odbc-in-osx.html#r-to-connect-to-your-ms-sql-server-with-odbc",
    "title": "How to set up ODBC in Mac OS to connect to MS SQL Server for use with Python and R",
    "section": "6. R to connect to your MS SQL Server with odbc",
    "text": "6. R to connect to your MS SQL Server with odbc\nlibrary(odbc)\ncon &lt;- dbConnect(odbc::odbc(), \"my_server\")\nres &lt;- odbc::dbGetQuery(con, \"your query here\")\nodbc::dbDisconnect(con)"
  },
  {
    "objectID": "blog/2020-11-13-quick-bash-scripting.html",
    "href": "blog/2020-11-13-quick-bash-scripting.html",
    "title": "Quick Bash Script for Complete Newbies",
    "section": "",
    "text": "At work or grad school, I have to juggle a lot of projects and it always takes me a while to remember which directory I was in and what I named the project environment. I wanted ‘shortcuts’ in the terminal that would automate the things I was typing out manually each time. To do this, I saved these custom functions and source them into .zshrc."
  },
  {
    "objectID": "blog/2020-11-13-quick-bash-scripting.html#create-a-file-for-your-custom-scripts",
    "href": "blog/2020-11-13-quick-bash-scripting.html#create-a-file-for-your-custom-scripts",
    "title": "Quick Bash Script for Complete Newbies",
    "section": "1. Create a file for your custom scripts",
    "text": "1. Create a file for your custom scripts\nI store a file for these personal macros, let’s create one here:\ncd /usr/local/bin/\ntouch personal_macros.\nvi personal_macros\nThis navigates to your /usr/local/bin/ directory, creates the personal_macros file with touch and uses vim to open it with vi."
  },
  {
    "objectID": "blog/2020-11-13-quick-bash-scripting.html#write-functions",
    "href": "blog/2020-11-13-quick-bash-scripting.html#write-functions",
    "title": "Quick Bash Script for Complete Newbies",
    "section": "2. Write functions",
    "text": "2. Write functions\nOnce your blank file is created you need to add in your functions. The first line that starts with the #! (shebang) tells bash that this is a zshell script. I don’t remember if this is absolutely necessary but I had it here.\n#!/usr/bin/env zsh\n\nfunction explore() {\n  cd ~/Documents/Analysis\n  conda activate explore\n  code .\n}\n\nfunction workmode() {\n  cd ~/Documents/Analysis/GT/ISYE6501\n  open .\n  open ISYE6501.Rproj\n}\nThese are examples of basic functions. The first one, explore() navigates to a folder and activates a conda environment called explore. Then it opens up a vscode window in the folder.\nThe second function workmode() navigates to another folder, opens an explorer window there, and then will open my ISYE6501.Rproj project in RStudio.\nRemember, when you’re done with vim, type :wq to save your changes and exit.\n:wq"
  },
  {
    "objectID": "blog/2020-11-13-quick-bash-scripting.html#add-these-functions-to-your-.zshrc-file",
    "href": "blog/2020-11-13-quick-bash-scripting.html#add-these-functions-to-your-.zshrc-file",
    "title": "Quick Bash Script for Complete Newbies",
    "section": "3. Add these functions to your .zshrc file",
    "text": "3. Add these functions to your .zshrc file\nAfter you’re done creating the functions it’s time to add them to the .zshrc file so you can call these functions from the terminal.\ncd\nvi .zshrc\nThen you want to add a line to source your personal_macros file by adding in this line to the .zshrc file:\nsource /usr/local/bin/personal_macros\nGo ahead and save and exit vim:\n:wq\nNow you can reload your terminal and the functions will be available to you! Just type explore or workmode and it’ll run through the steps in your script.\nI like to create functions that follow a particular taxonomy. For work projects, I’ll make a function for each project I’m working on, starting with work- so it’s easy to remember and I can utilize tab completion. For example, I’ll have a work-unicorn function when I’m working on the unicorn project and a work-phoenix function when I want to resume work on the phoenix project.\nHappy scripting!"
  },
  {
    "objectID": "blog/2020-08-16-dash-clean-line-chart.html",
    "href": "blog/2020-08-16-dash-clean-line-chart.html",
    "title": "Clean Sparkline-style Line Chart for Dash Cards",
    "section": "",
    "text": "Inspired by the FiscalData.treasury.gov website, I set out to create a clean-looking line chart template that I can insert either as a tooltip or directly into a KPI card.\nExample:\nHere’s the code for the figure, using some dummy random data:\nI like having the layout as a dictionary so we can reuse it if we end up using a bunch of these."
  },
  {
    "objectID": "blog/2020-08-16-dash-clean-line-chart.html#breaking-down-the-layout",
    "href": "blog/2020-08-16-dash-clean-line-chart.html#breaking-down-the-layout",
    "title": "Clean Sparkline-style Line Chart for Dash Cards",
    "section": "Breaking down the layout",
    "text": "Breaking down the layout\nHide the plot background:\n    \"plot_bgcolor\": \"rgba(0, 0, 0, 0)\",\n    \"paper_bgcolor\": \"rgba(0, 0, 0, 0)\",\nHide the y-axis entirely:\n    \"yaxis\": {\"visible\": False},\nFor the x-axis, show only the first and last ticks, and set the tick text to have that 2-level effect, and hide the x-axis title.\n    \"xaxis\": {\n        \"nticks\": 2,\n        \"tickmode\": \"array\",\n        \"tickvals\": [xmin, xmax],\n        \"ticktext\": [f\"{ymin} &lt;br&gt; {xmin}\", f\"{ymax} &lt;br&gt; {xmax}\"],\n        \"title_text\": None\n        },\nAnd finally, hide the legend and clean up the margins to reduce whitespace in the card. If you don’t do this it makes for a really small plot.\n    \"showlegend\": False,\n    \"margin\": {\"l\":4,\"r\":4,\"t\":0, \"b\":0, \"pad\": 4}\nWe also need to set up the config to hide the annoying modebar, and this goes either in the fig.show() call or the dcc.Graph() call, depending if you’re just using Plotly or using this in Dash.\nconfig = {'displayModeBar': False}\nAnd here’s the code for the card:\ncard = dbc.Card(\n    [\n        dbc.CardBody(\n            [\n                html.H4(\"Card title\", className=\"card-title\"),\n                html.P(\n                    \"$10.5 M\",\n                    className=\"card-value\",\n                ),\n                html.P(\n                    \"Target: $10.0 M\",\n                    className=\"card-target\",\n                ),\n                dcc.Graph(figure=fig, config=config),\n                html.Span(\n                    \"Up \",\n                    className=\"card-diff-up\",\n                ),\n                html.Span(\n                    \"5.5% vs Last Year\",\n                    className=\"card-diff-up\",\n                ),\n\n            ]\n        ),\n    ],\n)"
  },
  {
    "objectID": "blog/2019-08-05-Fast-Excel-with-the-Quick-Access-Toolbar.html",
    "href": "blog/2019-08-05-Fast-Excel-with-the-Quick-Access-Toolbar.html",
    "title": "Fast Excel with the Quick Access Toolbar",
    "section": "",
    "text": "Excel is still the lingua franca of Corporate Finance and is blazing fast for quick, ad-hoc analyses. I learned early on to use excel without a mouse, and with the Quick Access Toolbar and some macros, you can even make Excel feel like a video game!\nHere’s how my quick access toolbar is set up. I primarily work with pro forma Profit and Loss statements from our various business units, so my workflow is tailored for quick 5 minute analyses that gets presented to managers as a formatted PnL or data table in an email to go along with a short writeup.\nThe macros are there to convert the value in the cell to a specific $ format with no decimals and red parenthesis for negative values. I use this pattern constantly as a profit and loss analyst to present your analyses quickly to the executive waiting for your insights."
  },
  {
    "objectID": "blog/2019-08-05-Fast-Excel-with-the-Quick-Access-Toolbar.html#macros",
    "href": "blog/2019-08-05-Fast-Excel-with-the-Quick-Access-Toolbar.html#macros",
    "title": "Fast Excel with the Quick Access Toolbar",
    "section": "Macros:",
    "text": "Macros:\nSub Spends_Format()\n\n'\n' Spends_Format Macro\n' Format Spends in $ format with negative values in red parentheses\n'\n\n'\n    Selection.NumberFormat = \"$#,##0_);[Red]($#,##0)\"\nEnd Sub\nSub Format_Pivot()\n'\n' Format_Pivot Macro\n' Formats a column of a pivot in $\n'\n'\n    On Error Resume Next\n    Set pt = ActiveCell.PivotCell.PivotTable\n    On Error GoTo 0\n    If pt Is Nothing Then\n        MsgBox \"No PivotTable selected\", vbInformation, \"Oops...\"\n        Exit Sub\n    End If\n    For Each df In pt.DataFields\n      df.NumberFormat = \"$#,##0_);[Red]($#,##0)\"\n    Next df\n\nEnd Sub\nSub Color_Format()\n'\n' Color_Format Macro\n' Automatically formats the numbers to show blue if it's a constant, black if it's a formula, or green if it's a linked formula\n' Keyboard Shortcut: Ctrl+Shift+C\n'\n\nDim cell As Range, constantCells As Range, formulaCells As Range\nDim cellFormula As String\nWith Selection\n    On Error Resume Next\n        Set constantCells = .SpecialCells(xlCellTypeConstants, xlNumbers)\n        Set formulaCells = .SpecialCells(xlCellTypeFormulas, 21)\n    On Error GoTo 0\nEnd With\n\nIf Not constantCells Is Nothing Then\n    constantCells.Font.ThemeColor = xlThemeColorAccent1\n    constantCells.Font.TintAndShade = 0\nEnd If\n\nIf Not formulaCells Is Nothing Then\n    For Each cell In formulaCells\n        cellFormula = cell.Formula\n        If cellFormula Like \"*.xls*]*!*\" Then\n            cell.Font.ColorIndex = 3\n        ElseIf cellFormula Like \"*!*\" And Not cellFormula Like \"*\\**\" And Not cellFormula Like \"*+*\" And Not cellFormula Like \"*-*\" And Not cellFormula Like \"*/*\" And Not cellFormula Like \"*%*\" And Not cellFormula Like \"*^*\" And Not cellFormula Like \"*&gt;*\" And Not cellFormula Like \"*&lt;*\" And Not cellFormula Like \"*&gt;=*\" And Not cellFormula Like \"*&lt;=*\" And Not cellFormula Like \"*&lt;&gt;*\" And Not cellFormula Like \"*&*\" Then\n            cell.Font.Color = -11489280\n            cell.Font.TintAndShade = 0\n        Else\n            cell.Font.ColorIndex = 0\n        End If\n    Next cell\nEnd If\n\nEnd Sub"
  },
  {
    "objectID": "blog/2021-11-27-updating-julia.html",
    "href": "blog/2021-11-27-updating-julia.html",
    "title": "Updating Your Julia Installation (Feb 2023)",
    "section": "",
    "text": "Out of the box, Julia has a great package manager but it’s missing something like conda or pyenv to manage different versions of Julia, or to update your current installation of Julia to the latest release. See below for what I use on my Mac:\n\nFeb 2023 Update:\nI recently switched out of conda and into pyenv and poetry on my mac, which ended up breaking my previous method of using pyjill to manage my julia installation (which requires python).\nThis was a great opportunity to switch to juliaup, which is written in Rust and offers much of the same functionality without the dependence on python. However, it’s not perfect (particularly to users in PRC, as the author of pyjill notes here) and I’ll be keeping the old instructions of using pyjill for now:\n\n\nClick here for the old pyjill instructions (Python required)\n\n\n\nI used to mess with downloading the binaries and install files directly from the julialang site and also tried homebrew, but both of these options were really clunky when it came to updating your existing installation. Instead, I just use pyjill (Github), which works perfectly on both OSX, Windows, and Linux (note: requires python &gt;=3.6).\n# Install or update the latest version of jill (requires python&gt;=3.6)\npip install jill --user -U\n# export the jill PATH somewhere, if this is your first time installing jill\necho 'export PATH=$PATH:/home/$USERNAME/.local/bin' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n# Install the latest stable release\njill install\nThen the next time you need to update Julia, you only need to do jill install to get the latest."
  },
  {
    "objectID": "blog/2021-11-27-updating-julia.html#step-1-use-jillpyjill-to-download-install-julia",
    "href": "blog/2021-11-27-updating-julia.html#step-1-use-jillpyjill-to-download-install-julia",
    "title": "Updating Your Julia Installation (Feb 2023)",
    "section": "",
    "text": "I used to mess with downloading the binaries and install files directly from the julialang site and also tried homebrew, but both of these options were really clunky when it came to updating your existing installation. Instead, I just use pyjill (Github), which works perfectly on both OSX, Windows, and Linux (note: requires python &gt;=3.6).\n# Install or update the latest version of jill (requires python&gt;=3.6)\npip install jill --user -U\n# export the jill PATH somewhere, if this is your first time installing jill\necho 'export PATH=$PATH:/home/$USERNAME/.local/bin' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n# Install the latest stable release\njill install\nThen the next time you need to update Julia, you only need to do jill install to get the latest."
  },
  {
    "objectID": "blog/2021-11-27-updating-julia.html#step-0-first-time-install-with-juliaup",
    "href": "blog/2021-11-27-updating-julia.html#step-0-first-time-install-with-juliaup",
    "title": "Updating Your Julia Installation (Feb 2023)",
    "section": "Step 0: First-time Install with juliaup",
    "text": "Step 0: First-time Install with juliaup\nThe instructions on the juliaup repo are extensive, but I’ll share a few key commands here:\n\n\nOn a Mac, it’s advised (as of Feb’23) to use the following curl command instead of brew\n\ncurl -fsSL https://install.julialang.org | sh\n\nThis will run an interactive install script and for convenience it will update your PATH and install the latest release and set it as your default if you don’t have Julia already installed.\nThen juliaup status will show you the list of versions you have installed, and show you which is the default version of julia that’s symlinked to julia.\njuliaup status\n\nSwitching environments\nYou can always install an older release, let’s say you want 1.8.4:\n# Add an older version of Julia, let's say 1.8.4\njuliaup add 1.8.4\n# Then to run that version of julia, all you would need to do is add `+1.8.4` when you launch julia, i.e.:\njulia +1.8.4\nYou can also set it as the default channel with juliaup default:\njuliaup default 1.8.4"
  },
  {
    "objectID": "blog/2021-11-27-updating-julia.html#step-1-updating-to-the-latest-release-with-juliaup",
    "href": "blog/2021-11-27-updating-julia.html#step-1-updating-to-the-latest-release-with-juliaup",
    "title": "Updating Your Julia Installation (Feb 2023)",
    "section": "Step 1: Updating to the latest release with juliaup",
    "text": "Step 1: Updating to the latest release with juliaup\njuliaup will automatically check for the latest release every so often when you launch julia, but you can manually update with:\njuliaup update"
  },
  {
    "objectID": "blog/2021-11-27-updating-julia.html#step-2-remove-the-old-.julia-folder-optional-linuxosx-instructions",
    "href": "blog/2021-11-27-updating-julia.html#step-2-remove-the-old-.julia-folder-optional-linuxosx-instructions",
    "title": "Updating Your Julia Installation (Feb 2023)",
    "section": "Step 2: Remove the Old .julia Folder (optional, linux/OSX instructions)",
    "text": "Step 2: Remove the Old .julia Folder (optional, linux/OSX instructions)\nAnother time sink I had was realizing that with a fresh update, I couldn’t add packages anymore to julia because I didn’t have root access to a folder hidden deep within the .julia folder in my user directory. I had to delete it and then julia worked fine after that after rebuilding the registry. This might not be as much of an issue after Julia 1.7, where they changed the manifest and how the package registry handles different Julia versions.\n# Go to your home directory\ncd\n# Delete that sucker with prejudice\nsudo rm -rf .julia"
  },
  {
    "objectID": "blog/2021-11-27-updating-julia.html#step-3-update-jupyter-kernels",
    "href": "blog/2021-11-27-updating-julia.html#step-3-update-jupyter-kernels",
    "title": "Updating Your Julia Installation (Feb 2023)",
    "section": "Step 3: Update Jupyter Kernels",
    "text": "Step 3: Update Jupyter Kernels\nNext we’ll need to remove the old julia kernel from Jupyter with jupyter kernelspec remove {{ kernel name }}:\n# See the list of existing jupyter kernels and find your old install\njupyter kernelspec list\n# My previous one was called julia-1.6, let's remove it\njupyter kernelspec remove julia-1.6\nThen use IJulia to add a new one. Whenever you change the Julia binary you need to have IJulia rebuild in order to register the new kernel to jupyter, so go back to Julia and go to the Pkg prompt:\n# In the Julia REPL, press the ] key to bring up the Pkg prompt\n(@v1.8) pkg&gt; add IJulia\n(@v1.8) pkg&gt; build IJulia\nAt this point, if you’re upgrading from Julia 1.7+ and it’s a minor update (i.e. 1.7.2 to 1.7.3), your base package manifest should still be there and Julia will precompile and build those other packages so they’ll be ready to go - no need to re-add them."
  },
  {
    "objectID": "blog/2021-11-27-updating-julia.html#step-4.-update-your-libraries-if-upgrading-from-julia-1.7",
    "href": "blog/2021-11-27-updating-julia.html#step-4.-update-your-libraries-if-upgrading-from-julia-1.7",
    "title": "Updating Your Julia Installation (Feb 2023)",
    "section": "Step 4. Update Your Libraries (if upgrading from Julia < 1.7)",
    "text": "Step 4. Update Your Libraries (if upgrading from Julia &lt; 1.7)\nFrom here on out you’ll need to probably build and update your other libraries too, and if you use stuff like PyCall(link) where it downloads a distribution of python and hides it in your julia folder, have fun managing all of that! I’ll do a writeup later when I figure this out a little more.\n# A few common libraries that I install for data exploration and analysis\n(@v1.8) pkg&gt; add Revise Plots Gadfly DataFrames DataFramesMeta XLSX CSV RDatasets Parquet"
  },
  {
    "objectID": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html",
    "href": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html",
    "title": "Time Series Forecasting 101 for Finance Analysts",
    "section": "",
    "text": "People with no prior exposure to time series analysis, but may have taken a statistics class or two. Goal is to help you recognize what time series forecasting is and share examples of time series forecasting models used in practice today without using any math."
  },
  {
    "objectID": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#who-this-is-for",
    "href": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#who-this-is-for",
    "title": "Time Series Forecasting 101 for Finance Analysts",
    "section": "",
    "text": "People with no prior exposure to time series analysis, but may have taken a statistics class or two. Goal is to help you recognize what time series forecasting is and share examples of time series forecasting models used in practice today without using any math."
  },
  {
    "objectID": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#what-is-time-series-forecasting",
    "href": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#what-is-time-series-forecasting",
    "title": "Time Series Forecasting 101 for Finance Analysts",
    "section": "What is Time Series Forecasting?",
    "text": "What is Time Series Forecasting?\nTime Series is data that’s indexed by time. In finance, we encounter time series all the time - think of our stock price throughout the course of a week, or a revenue forecast, our historical weekly spending data, etc.\nTime Series Forecasting aims to predict future periods based on available information, and because of the time-based nature of the data, people have had to create novel ways of analyzing and dealing with how the signal changes over time."
  },
  {
    "objectID": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#a-trivial-example-scenario",
    "href": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#a-trivial-example-scenario",
    "title": "Time Series Forecasting 101 for Finance Analysts",
    "section": "A (trivial) example scenario 🍕",
    "text": "A (trivial) example scenario 🍕\nLet’s say you’re a new FP&A analyst and you’re in charge of managing the budget for your finance team. One day your manager comes in and asks for your best guess as to what next month’s spending is going to be. The team’s been working really hard the last few months, so if you’re trending under budget then maybe the team can celebrate with an offsite pizza party. What approach would you take?"
  },
  {
    "objectID": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#tried-and-true-baselines",
    "href": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#tried-and-true-baselines",
    "title": "Time Series Forecasting 101 for Finance Analysts",
    "section": "Tried and True Baselines",
    "text": "Tried and True Baselines\nIf you’ve been in Finance for any length of time, you’ll typically encounter two simple approaches to forecasting: asking experts and applying a simple rule of thumb in excel.\n\nJudgmental Forecasts\nJudgmental Forecasts are important to recognize because they’re everywhere within Finance. It’s forecasting based on human intuition and expert opinion - which has its flaws but can be useful. I won’t go into much detail on this since Decision Quality (DQ) and the field of decision analysis covers this in greater depth.\n\n\nShortcuts and ‘rules of thumb’ (heuristics)\nMore quantitative than judgmental forecasts, heuristics are shortcuts that rely on assumptions and/or simple math to create a forecast. This includes our common ‘Linearity’ models, simple moving averages, and even a back-of-the-envelope financial model you might build in excel where the inputs are ultimately based on human intuition and opinion. While these approaches can be very accurate, they’re often point forecasts and you might not be able to quantify how certain you are in that exact point forecast. In the case of an excel-based financial model, the reasons behind the human inputs are not implicit, resulting in a model that can make you overconfident in the actual forecast or in its ability to support good decision making if the underlying assumptions were flawed (aka garbage in, garbage out)."
  },
  {
    "objectID": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#can-we-do-better",
    "href": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#can-we-do-better",
    "title": "Time Series Forecasting 101 for Finance Analysts",
    "section": "Can We Do Better?",
    "text": "Can We Do Better?\nIn the example scenario, the stakes are low: if you get the forecast wrong, you lose out on a little pizza 🍕. But if the stakes are higher and you have a wealth of available data, can you get a faster, more accurate, and more informative forecast?\nAdvances in computing in the last 80 years have led to new methods in forecasting, starting from simple statistical methods from the 1950’s all the way to the new machine learning and neural network driven approaches of the last 20 years. I’ll highlight a few here:\n\n\n\nimg\n\n\nBelow, I’ll give a brief summary of what these are so you can identify when to use them and what they look like, with a heavier emphasis on the classical techniques and more of a high level mention of the rest. We’ll be applying these different forecasting methods against the count of monthly international air passengers from 1949-1960 from the classic Box-Jenkins ARIMA paper."
  },
  {
    "objectID": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#classic-time-series-forecasts",
    "href": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#classic-time-series-forecasts",
    "title": "Time Series Forecasting 101 for Finance Analysts",
    "section": "Classic Time Series Forecasts",
    "text": "Classic Time Series Forecasts\n\nExponential Smoothing (Holt-Winters)\nExponential Smoothing (sometimes called the Holt-Winters model) is a family of fast, classic methods from the late 1950’s that can make reasonably good forecasts with a minimal amount of data and is very quick and simple to calculate.\nIt starts with ‘Simple’ exponential smoothing, which is like a weighted moving average. But the way that the data is weighted decays exponentially the further in the past that you go, so that more recent data is weighted much, much more than data from the distant past.\nHere’s an example of a Simple Exponential Smoothing forecast:\n\n\n\nimg\n\n\nNotice that in this case, we’ve modeled a 1-year monthly forecast (12 steps ahead), and simple exponential smoothing just results in a straight line. You can modify the Exponential Smoothing Model to incorporate a trend - essentially you add a slope to the line:\n\n\n\nimg\n\n\nWhich gets a slightly better fit. Now we notice that there are clear repetitive peaks and valleys in this signal starting around 1952 and onwards, indicating that there is higher travel from June-August of each year, corresponding with summer in the Northern hemisphere. Once again, Exponential Smoothing can incorporate both the increasing trend and this seasonal component:\n\n\n\nimg\n\n\nThis final model is also called Triple Exponential Smoothing, and is a great example of the power of time series models. You may have noticed that this model is a combination of three different things: the weighted moving average, the linear trend, and the seasonality. With the rest of the examples, you should think of each forecasting model as a combination of multiple underlying building blocks then you can decompose and tweak to get the best forecast possible.\nWhat does it mean to have a ‘good’ forecast? A ‘good’ (accurate) model should closely approximate the curve of the actual/historical data during your testing, indicating that it’s a good fit for the data during your test. I’ll summarize model evaluation later, but if you’re interested you can read this summary here to get a preview.\n\n\nAutoregressive Integrated Moving Average (ARIMA)\nARIMA stands for Autoregressive Integrated Moving Average, and similarly to Exponential Smoothing it performs very well on a small amount of data and is fast. It was state of the art for the 1970’s and still performs very well to this day. Traditionally, ARIMA also required the use of an analyst to use their business acumen to specify some of the parameters in this model, but more recently automated techniques made this less necessary.\n\nAutoRegression What is “AutoRegression”? Think of it as another fancy weighted average, similar to what we saw with Simple Exponential Smoothing. Only this time, the weights aren’t necessarily decaying the further back in time you go - the weights are free to vary, and you use the math behind Linear Regression to calculate the weights based on the data.\n\nUnlike Exponential Smoothing, ARIMA relies on statistical assumptions in the underlying data, and violation of these assumptions will result in a less predictive model. However, in terms of prediction accuracy you can often get a better fit with ARIMA.\nHere’s an example of ARIMA on the Air Passengers set:\n\n\n\nimg\n\n\nNote the implementation of the model I used here includes a 90% Prediction Interval. A Prediction Interval has a different interpretation than a traditional Confidence Interval - a prediction interval describes the probability of the future data point while a confidence interval describes how confident you should be of a true statistical parameter (i.e. the population mean) being contained within that interval. and depending on the model it might not make any assumptions about the underlying distribution of the data."
  },
  {
    "objectID": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#time-series-regression",
    "href": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#time-series-regression",
    "title": "Time Series Forecasting 101 for Finance Analysts",
    "section": "Time Series Regression",
    "text": "Time Series Regression\nRegression in a time series context has some important considerations and differences than the classic Ordinary Least Squares (OLS) Linear Regression that you may have learned in a long-forgotten statistics class. In fact, some of the basic assumptions in Linear Regression are violated in the time series setting (depending on how you use it), but with careful use and some tweaks regression models have many applications in modern time series forecasting.\n\nLinear Regression\nYou can turn any time series into a linear regression problem by using time as a feature or predictor in your model. For instance, you can simply fit a linear trendline (even in excel) to data by using linear regression to estimate the average response based on time:\n\n\n\nimg\n\n\nAnd of course, if you have additional predictors or features you can add them to the model as well, similar to how you would do multiple linear regression. The benefit of using this kind of model is that you can incorporate additional information and other key drivers to improve the accuracy of the model and make it easier to explain the model to stakeholders and use it in decision making. The downside is that predictive accuracy on these models might not be as good as other time series methods, and the forecasts are highly dependent on your ability to identify good ‘key drivers’, which is challenging in practice.\n\n\nDynamic Regression (ARIMAX)\nDynamic Regression, to include ARIMA with eXogenous variables (exogenous means data that’s not part of the signal you’re trying to predict), is a way to combine the predictive accuracy of ARIMA with the explainability of your regression models. ARIMAX is a different kind of combination of models than you’ve seen - it’s actually a combination of two models that are calculated in a sequence.\nSteps in ARIMAX:\n\nFirst, fit a linear regression model to a set of historical predictor variables/features\nCalculate the residuals, which is the difference between the model’s predictions and the historical actuals in the data\nFinally, fit an ARIMA model that best predicts the residuals\nTo create a forecast, you feed in forecasted variables/features to the regression model, and then add in the fitted ARIMA model\n\n\nThe key thing to watch out for in ARIMAX (and all regression models) is the idea of leakage : when data that you wouldn’t have known in advance is used to build the model. So the forecast depends on getting a very good forecast of the features that you’re feeding into the model. And those forecasts might themselves be the result of another time series forecasting process that has its own uncertainty and prediction intervals, and these errors can cascade throughout your model in ways you might not expect.\n\n\n\nDecision Trees\nMore specifically, tree-based methods like Random Forest and their most sophisticated variants like eXtreme Gradient Boosting (XGBoost). These are very popular in competition and tend to outperform more complex models on tabular data. Think of these as more flexible models that can fit hundreds of mini-regression models and adapt to nonlinearities, while still being relatively fast, and they should be a go-to model after first trying the classical statistical models.\nHowever, due to the stochastic (random) nature of these models, you aren’t guaranteed to get the same results every time you run the model. But these models can handle non linearity in the data and can automatically find the best predictors/variables to include in the model and help you understand what’s happening under the hood."
  },
  {
    "objectID": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#advanced-methods",
    "href": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#advanced-methods",
    "title": "Time Series Forecasting 101 for Finance Analysts",
    "section": "Advanced Methods",
    "text": "Advanced Methods\nI won’t cover these in much depth, but are still worth a mention:\n\nProphet\nFacebook open sourced their ‘Prophet’ model, which is intended to be an automated, scalable forecasting model that is suitable for a business analyst to run.\n\n\n\nimg\n\n\nYou can think of Prophet as a fancy combination of several underlying components:\n\nA piece-wise linear trend component (meaning it partitions the data into multiple periods and fits a different line to each period)\nAny seasonal patterns (including any combination of daily, monthly, yearly, weekly, and custom patterns)\nHoliday effects\n\nWhile Prophet is state of the art for situations in which you have a lot of daily data (i.e. user activity on a website), in the context of corporate finance where data comes in on a weekly, monthly, or quarterly basis, Prophet does not perform very well.\n\n\nVector Autoregression\nVector Autoregression is an example of a multivariate model in which you have multiple parallel time series and you want to simultaneously forecast for them. Or, you want to understand how each of the variables and time series influence each other.\n\n\nNeural Networks\nNeural networks have made a tremendous splash in computer vision and language modeling, and within time series data they can be used for vast quantities of data. The key with neural networks is their ability to handle nonlinear relationships, but they require a large amount of data in order to be useful. In practice, corporate finance deals with smaller quantities of tabular data (i.e. data that looks like it came out of table in a spreadsheet) where simpler methods still perform well."
  },
  {
    "objectID": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#lots-of-models",
    "href": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#lots-of-models",
    "title": "Time Series Forecasting 101 for Finance Analysts",
    "section": "Lots of Models!",
    "text": "Lots of Models!\nSo in our short time together we’ve only scratched the surface - there are a bounty of models and novel approaches out there, including judgmental forecasts and rules of thumb that we use today. How do we compare them all?"
  },
  {
    "objectID": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#how-do-we-know-what-better-looks-like",
    "href": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#how-do-we-know-what-better-looks-like",
    "title": "Time Series Forecasting 101 for Finance Analysts",
    "section": "How do we know what ‘Better’ looks like?",
    "text": "How do we know what ‘Better’ looks like?\nIn other words, how do you measure and evaluate model performance? Turns out that’s a question without a straightforward answer, and it’s up to you as the analyst to interpret the results and arrive at a reasonable conclusion. In a future post I’ll discuss the most common ways of measuring forecast performance and talk about the pitfalls and benefits of each approach."
  },
  {
    "objectID": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#further-reference",
    "href": "blog/2022-06-03-Time-Series-Forecasting-101-Finance.html#further-reference",
    "title": "Time Series Forecasting 101 for Finance Analysts",
    "section": "Further Reference",
    "text": "Further Reference\n\nIf you want to learn more, the best introductory resource I can point you towards is Hyndman’s free online textbook Forecasting: Principles and Practice, now in its 3rd edition\nIf you prefer an intermediate resource with Python examples, I highly recommend Practical Time Series Analysis by Aileen Nielsen"
  },
  {
    "objectID": "blog/2022-02-20-fiscal-5253-workweek-calendars-in-pandas.html",
    "href": "blog/2022-02-20-fiscal-5253-workweek-calendars-in-pandas.html",
    "title": "Making Custom Fiscal 52/53 Workweek Calendars in Pandas",
    "section": "",
    "text": "Last run on pandas 1.1.3"
  },
  {
    "objectID": "blog/2022-02-20-fiscal-5253-workweek-calendars-in-pandas.html#references-used",
    "href": "blog/2022-02-20-fiscal-5253-workweek-calendars-in-pandas.html#references-used",
    "title": "Making Custom Fiscal 52/53 Workweek Calendars in Pandas",
    "section": "References Used",
    "text": "References Used\n\nPandas user guide on time series/date functionality\nPandas docs on timedeltas\nPandas Reference Docs on Date offsets\nDocs on Period objects\nPandas frequency strings aka offset aliases\nPandas Info on the FY5253 offset"
  },
  {
    "objectID": "blog/2020-07-02-dash-bootstrap-kpi-card.html",
    "href": "blog/2020-07-02-dash-bootstrap-kpi-card.html",
    "title": "KPI Card with Dash Bootstrap Components",
    "section": "",
    "text": "Updated 2021-07-01 to use Dash 1.19 and fontawesome icons\nDash Bootstrap Components has a fantastic library of plug and play components that make it easy to get started.\nPlotly’s documentation is focused on making tables, charts, and graphs, but for executives I find myself relying on KPI cards like this: \nYou can find better styled examples of these all over the place if you google ‘HTML/CSS dashboard examples.’\nThe magic of using Dash is that you can style these to your heart’s content with CSS and all of the values can be generated with Python, and the whole thing can be reproducible and components can be shared across projects. And the flexbox system is just fine rather than fiddling with positioning in PowerBI.\nHere’s the sketch of the starter KPI dashboard: \nPython Code for your app.py file:\nimport dash  # (version 1.19.0) pip install dash\n\nimport dash_bootstrap_components as dbc\nimport dash_html_components as html\n\nfontawesome_stylesheet = \"https://use.fontawesome.com/releases/v5.8.1/css/all.css\"\n\napp = dash.Dash(external_stylesheets=[dbc.themes.BOOTSTRAP, fontawesome_stylesheet])\n\nheader = html.Div([\n    dbc.Row(dbc.Col(html.H1(\"H1 Text\"))),\n])\n\ncard = dbc.Card([\n        dbc.CardBody([\n                html.H4(\"Card title\", className=\"card-title\"),\n                html.P(\n                    \"$10.5 M\",\n                    className=\"card-value\",\n                ),\n                html.P(\n                    \"Target: $10.0 M\",\n                    className=\"card-target\",\n                ),\n                html.Span([\n                    html.I(className=\"fas fa-arrow-circle-up up\"),\n                    html.Span(\" 5.5% vs Last Year\",\n                    className=\"up\")\n                ])\n            ])\n        ])\n\nrow = html.Div([\n        dbc.CardDeck([\n                card,\n                card,\n                card,\n            ]),\n    ], style={'padding': '25px'}\n)\n\napp.layout = html.Div([\n    header,\n    row,\n    row\n])\n\nif __name__ == \"__main__\":\n    app.run_server(host=\"0.0.0.0\", port=8080, debug=True)\nThe ‘up’ arrow icon is courtesy of fontawesome, which we load with the external stylesheet. Since this is all in python it’s simple to turn this into a function and create conditional (i.e. if) statements to change the icon and color scheme for your data.\nHere is the basic CSS styling that goes with this:\n.card {\n  text-align: center;\n}\n.card .card-title {\n  text-align: left;\n  font-weight: lighter;\n}\n\n.card .card-value {\n  font-size: 2rem;\n}\n\n.card .card-target {\n  font-size: 1rem;\n  font-weight: lighter;\n}\n\n.card .down {\n  color: red;\n}\n\n.card .up {\n  color: green;\n}"
  },
  {
    "objectID": "notebooks/example.html",
    "href": "notebooks/example.html",
    "title": "Example Notebook",
    "section": "",
    "text": "import polars as pl\nimport polars.selectors as cs\npl.Config.set_fmt_str_lengths(100)\n\npolars.config.Config"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Latest Posts",
    "section": "",
    "text": "On a journey to use data and science to help people make better decisions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Trivial Comparison of Python Probabilistic Programming Languages\n\n\n\n\n\n\npython\n\n\nbayesian stats\n\n\nmodeling\n\n\n\nA simple side-by-side comparison of the syntax for several probabilistic programming languages (PPL) using a trival regression example.\n\n\n\n\n\nSep 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRevenue Optimization with Price Elasticities and Scipy\n\n\n\n\n\n\npython\n\n\noptimization\n\n\nmodeling\n\n\n\nAn introductory example of revenue optimization using the minimize function from scipy.optimize\n\n\n\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Regression with numpyro\n\n\n\n\n\n\npython\n\n\nbayesian stats\n\n\nmodeling\n\n\n\nA simple example of regression with numpyro\n\n\n\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA friendly introduction to scan with JAX\n\n\n\n\n\n\npython\n\n\n\nA friendly introduction to scan with JAX and numpyro\n\n\n\n\n\nDec 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Color Palettes from Animated GIFs with Julia\n\n\n\n\n\n\njulia\n\n\ndata visualization\n\n\n\nHow to automatically generate custom color palettes for data visualization from animated GIFs with Julia\n\n\n\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Forecasting 102 for Finance Analysts\n\n\n\n\n\n\npython\n\n\nforecasting\n\n\n\nPart 2 of a first-time training around time series forecasting aimed at Finance Analysts in my company, focused on model evaluation.\n\n\n\n\n\nJun 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Forecasting 101 for Finance Analysts\n\n\n\n\n\n\npython\n\n\nforecasting\n\n\n\nPart 1 of a first-time training around time series forecasting aimed at Finance Analysts in my company, focused on identifying types of models used in modern forecasting.\n\n\n\n\n\nJun 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDash (Plotly) tips - Latex, Google Fonts, and more\n\n\n\n\n\n\ndataviz\n\n\npython\n\n\n\nA few tips on making your Dash application look a little better\n\n\n\n\n\nJun 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nA SQL to Julia DataFrames Cheatsheet\n\n\n\n\n\n\njulia\n\n\nmodeling\n\n\n\nWhat would common SQL expressions look like in Julia DataFrames and Julia DataFramesMeta?\n\n\n\n\n\nMar 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Custom Fiscal 52/53 Workweek Calendars in Pandas\n\n\n\n\n\n\nmodeling\n\n\n\nHow to build custom financial calendars that have 52 or 53 workweeks in a fiscal year (aka the 4-4-5 calendar) using pandas\n\n\n\n\n\nFeb 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUpdating Your Julia Installation (Feb 2023)\n\n\n\n\n\n\njulia\n\n\nworkflow\n\n\n\nWhat it takes to update from a previous install of Julia on Mac/Linux\n\n\n\n\n\nNov 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nPython Pandas - Merging With Wildcards and Conditions\n\n\n\n\n\n\nmodeling\n\n\n\nMerging two dataframes with pandas but only if certain conditions are true.\n\n\n\n\n\nNov 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a Basic Corporate Plotly Template\n\n\n\n\n\n\ndataviz\n\n\n\nTLDR on how to make a corporate themed plotly template\n\n\n\n\n\nAug 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with WSL for Data Analysis\n\n\n\n\n\n\nworkflow\n\n\n\nMy steps for a fresh install of Windows Subsystem for Linux for Data Analysis\n\n\n\n\n\nMay 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAdding Plotly Charts to Markdown\n\n\n\n\n\n\npython\n\n\ndataviz\n\n\nworkflow\n\n\n\nQuick reference that shows options to add plotly charts to a markdown document\n\n\n\n\n\nApr 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n‘Tie Fighter’ Errorbar Charts in Python with Plotnine and Altair\n\n\n\n\n\n\npython\n\n\ndataviz\n\n\n\n\n\n\n\n\n\nDec 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nQuick Bash Script for Complete Newbies\n\n\n\n\n\n\nworkflow\n\n\n\nBasic shell scripting for everyday productivity.\n\n\n\n\n\nNov 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nClean Sparkline-style Line Chart for Dash Cards\n\n\n\n\n\n\ndataviz\n\n\n\nCode snippet to create a clean line chart to go in a Dash Card.\n\n\n\n\n\nAug 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nKPI Card with Dash Bootstrap Components\n\n\n\n\n\n\ndataviz\n\n\npython\n\n\n\nCode snippet to create a starter KPI card with Dash Bootstrap Components\n\n\n\n\n\nJul 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nMake a Basic PnL Dataframe in Pandas\n\n\n\n\n\n\nworkflow\n\n\n\nCode snippet to create a PnL (Profit and Loss Statement) Dataframe in Pandas\n\n\n\n\n\nJun 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Cheat Sheet in R\n\n\n\n\n\n\nmodeling\n\n\n\nGetting started using the forecast package for time series data in R, as quickly as possible and no explanations.\n\n\n\n\n\nJan 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHow to set up ODBC in Mac OS to connect to MS SQL Server for use with Python and R\n\n\n\n\n\n\nworkflow\n\n\n\nMy local data mart runs on MS SQL Server, and I want to pull data directly into R or Python for data analysis. This is really easy on Windows with its’ built-in ODBC manager, but I spent a weekend figuring out how to do this after switching to OSX. A lot of documentation out there is old (from 2012), so I decided to make this for anyone still looking for an answer in 2019.\n\n\n\n\n\nOct 19, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nFast Excel with the Quick Access Toolbar\n\n\n\n\n\nExcel is still the lingua franca of Corporate Finance and is blazing fast for quick, ad-hoc analyses. I learned early on to use excel without a mouse, and with the Quick Access Toolbar and some macros, you can even make Excel feel like a video game!\n\n\n\n\n\nAug 5, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notebooks",
    "section": "",
    "text": "Links to Jupyter Notebooks rendered in HTML and available through nbviewer to review key concepts. I don’t work in Julia every day so I find it useful to keep a record of basic syntax until this becomes muscle memory."
  },
  {
    "objectID": "notes.html#python",
    "href": "notes.html#python",
    "title": "Notebooks",
    "section": "Python",
    "text": "Python\n\nMonte Carlo is Easy and Free in Python [ipynb] [html]\nWaterfall Charts in Plotly - Useful for Financial Planning and Analysis (FP&A) folks [ipynb] [html]\nSimple Examples of Bayesian Networks with Python pgmpy [ipynb] [html]\nUsing pandas to create Fiscal Calendars and 52/53 (aka 4-4-5) Lookup Calendars in Python [ipynb] [html]\nTime Series forecasting cheatsheet with the scikit-time (sktime) library [ipynb] [html]\nBayesian Decision Modeling with pymc4 (Notes from Ravin Kumar’s PyData Global 2020 talk) [ipynb] [html]\n\nRavin Kumar’s original talk [Youtube] [Presentation]\n\nAn intro to Lasso, Ridge, and ElasticNet in sklearn (bonus: support vector regression) [ipynb] [html]\nA bare-bones intro to the statsmodels API with VAR, AR, and linear regression [ipynb] [html]\nAuto ARIMA and ARIMAX/SARIMAX with pmdarima [html]"
  },
  {
    "objectID": "notes.html#learning-causal-impact",
    "href": "notes.html#learning-causal-impact",
    "title": "Notebooks",
    "section": "Learning causal impact",
    "text": "Learning causal impact\n\nWIP collection of notebooks around probabilistic programming with numpyro, forecasting, and causal inference [Github]"
  },
  {
    "objectID": "notes.html#julia",
    "href": "notes.html#julia",
    "title": "Notebooks",
    "section": "Julia",
    "text": "Julia\n\nSQL to Julia Translation for basic sorting, filtering, and aggregating of data [ipynb][html]\nLinear Regression with GLM [ipynb] [html]\nA Julia Project Workflow, i.e. setting up a new environment and project scaffolding [ipynb] [html]\nRandom Sampling from Distributions in Julia [ipynb] [html]\nFirst Impressions of Data Visualization with Makie and AlgebraOfGraphics [ipynb] [html]\nAnimations in Julia with Plots.jl [ipynb] [html]\nAnimations in Julia with Makie.jl [ipynb] [html]\nCyberpunk theme for Julia plots with Makie.jl [ipynb] [html]\nExploring MLJ, a wrapper for lots of machine learning libraries for Julia, similar to python’s scikit-learn [ipynb] [html]\nExample analysis workflow using TidyTuesday data using GadFly, DataFramesMeta, and DuckDB [ipynb] [html]\nCombining Optimization with JuMP and Bayesian Decision Making with Turing [ipynb] [html]"
  },
  {
    "objectID": "blog/2020-01-26-time-series-forecasting-in-r.html",
    "href": "blog/2020-01-26-time-series-forecasting-in-r.html",
    "title": "Time Series Cheat Sheet in R",
    "section": "",
    "text": "Getting started using the forecast package for time series data in R, as quickly as possible and no explanations.\nSource: Forecasting: Principles and Practice"
  },
  {
    "objectID": "blog/2020-01-26-time-series-forecasting-in-r.html#seasonality",
    "href": "blog/2020-01-26-time-series-forecasting-in-r.html#seasonality",
    "title": "Time Series Cheat Sheet in R",
    "section": "Seasonality",
    "text": "Seasonality\n\nggseasonplot(): Create a seasonal plot\nggsubseriesplot(): Create mini plots for each season and show seasonal means"
  },
  {
    "objectID": "blog/2020-01-26-time-series-forecasting-in-r.html#lags-and-acf",
    "href": "blog/2020-01-26-time-series-forecasting-in-r.html#lags-and-acf",
    "title": "Time Series Cheat Sheet in R",
    "section": "Lags and ACF",
    "text": "Lags and ACF\n\ngglagplot(): Plot the time series against lags of itself\nggAcf(): Plot the autocorrelation function (ACF)"
  },
  {
    "objectID": "blog/2020-01-26-time-series-forecasting-in-r.html#white-noise-and-the-ljung-box-test",
    "href": "blog/2020-01-26-time-series-forecasting-in-r.html#white-noise-and-the-ljung-box-test",
    "title": "Time Series Cheat Sheet in R",
    "section": "White Noise and the Ljung-Box Test",
    "text": "White Noise and the Ljung-Box Test\nWhite Noise is another name for a time series of iid data. Purely random. Ideally your model residuals should look like white noise.\nYou can use the Ljung-Box test to check if a time series is white noise, here’s an example with 24 lags:\nBox.test(data, lag = 24, type=\"Lj\")\np-value &gt; 0.05 suggests data are not significantly different than white noise"
  },
  {
    "objectID": "blog/2020-01-26-time-series-forecasting-in-r.html#naive-models",
    "href": "blog/2020-01-26-time-series-forecasting-in-r.html#naive-models",
    "title": "Time Series Cheat Sheet in R",
    "section": "Naive Models",
    "text": "Naive Models\nUseful to benchmark against naive and seasonal naive models.\n\nnaive()\nsnaive()"
  },
  {
    "objectID": "blog/2020-01-26-time-series-forecasting-in-r.html#residuals",
    "href": "blog/2020-01-26-time-series-forecasting-in-r.html#residuals",
    "title": "Time Series Cheat Sheet in R",
    "section": "Residuals",
    "text": "Residuals\nResiduals are the difference between the model’s fitted values and the actual data. Residuals should look like white noise and be:\n\nUncorrelated\nHave mean zero\n\nAnd ideally have:\n\nConstant variance\nA normal distribution\n\ncheckresiduals(): helper function to plot the residuals, plot the ACF and histogram, and do a Ljung-Box test on the residuals."
  },
  {
    "objectID": "blog/2020-01-26-time-series-forecasting-in-r.html#evaluating-model-accuracy",
    "href": "blog/2020-01-26-time-series-forecasting-in-r.html#evaluating-model-accuracy",
    "title": "Time Series Cheat Sheet in R",
    "section": "Evaluating Model Accuracy",
    "text": "Evaluating Model Accuracy\nTrain/Test split with window function:\nwindow(data, start, end): to slice the ts data\nUse accuracy() on the model and test set\naccuracy(model, testset): Provides accuracy measures like MAE, MSE, MAPE, RMSE etc\nBacktesting with one step ahead forecasts, aka “Time series cross validation” can be done with a helper function tsCV().\ntsCV(): returns forecast errors given a forecastfunction that returns a forecast object and number of steps ahead h. At h = 1 the forecast errors will just be the model residuals.\nHere’s an example using the naive() model, forecasting one period ahead:\ntsCV(data, forecastfunction = naive, h = 1)"
  },
  {
    "objectID": "blog/2020-01-26-time-series-forecasting-in-r.html#exponential-models",
    "href": "blog/2020-01-26-time-series-forecasting-in-r.html#exponential-models",
    "title": "Time Series Cheat Sheet in R",
    "section": "Exponential Models",
    "text": "Exponential Models\n\nses(): Simple Exponential Smoothing, implement a smoothing parameter alpha on previous data\nholt(): Holt’s linear trend, SES + trend parameter. Use damped=TRUE for damped trending\nhw(): Holt-Winters method, incorporates linear trend and seasonality. Set seasonal=“additive” for additive version or “multiplicative” for multiplicative version\n\n\nETS Models\nThe forecast package includes a function ets() for your exponential smoothing models. ets() estimates parameters using the likelihood of the data arising from the model, and selects the best model using corrected AIC (AICc)\n\nError = \\(\\{A, M\\}\\)\nTrend = \\(\\{N, A, Ad\\}\\)\nSeasonal = \\(\\{N, A, M\\}\\)"
  },
  {
    "objectID": "blog/2020-01-26-time-series-forecasting-in-r.html#transformations",
    "href": "blog/2020-01-26-time-series-forecasting-in-r.html#transformations",
    "title": "Time Series Cheat Sheet in R",
    "section": "Transformations",
    "text": "Transformations\nMay need to transform the data if it is non-stationary to improve your model prediction. To deal with non-constant variance, you can use a Box-Cox transformation.\nBoxCox(): Box-Cox uses a lambda parameter between -1 and 1 to stabilize the variance. A lambda of 0 performs a natural log, 1/3 does a cube root, etc while 1 does nothing and -1 performs an inverse transformation.\nDifferencing is another transformation that uses differences between observations to model changes rather than the observations themselves."
  },
  {
    "objectID": "blog/2020-01-26-time-series-forecasting-in-r.html#arima",
    "href": "blog/2020-01-26-time-series-forecasting-in-r.html#arima",
    "title": "Time Series Cheat Sheet in R",
    "section": "ARIMA",
    "text": "ARIMA\nParameters: \\((p,d,q)(P,D,Q)m\\)\n\n\n\nParameter\nDescription\n\n\n\n\np\n# of autoregression lags\n\n\nd\n# of lag-1 differences\n\n\nq\n# of Moving Average lags\n\n\nP\n# of seasonal AR lags\n\n\nD\n# of seasonal differences\n\n\nQ\n# of seasonal MA lags\n\n\nm\n# of observations per year\n\n\n\nArima(): Implementation of the ARIMA function, set include.constant = TRUE to include drift aka the constant\nauto.arima(): Automatic implentation of the ARIMA function in forecast. Estimates parameters using maximum likelihood and does a stepwise search between a subset of all possible models. Can take a lambda argument to fit the model to transformed data and the forecasts will be back-transformed onto the original scale. Turn stepwise = FALSE to consider more models at the expense of more time."
  },
  {
    "objectID": "blog/2020-01-26-time-series-forecasting-in-r.html#dynamic-regression",
    "href": "blog/2020-01-26-time-series-forecasting-in-r.html#dynamic-regression",
    "title": "Time Series Cheat Sheet in R",
    "section": "Dynamic Regression",
    "text": "Dynamic Regression\nRegression model with non-seasonal ARIMA errors, i.e. we allow e_t to be an ARIMA process rather than white noise.\nUsage example:\nfit &lt;- auto.arima(data, xreg = xreg_data)\npred &lt;- forecast(fit, xreg = newxreg_data)"
  },
  {
    "objectID": "blog/2020-01-26-time-series-forecasting-in-r.html#dynamic-harmonic-regression",
    "href": "blog/2020-01-26-time-series-forecasting-in-r.html#dynamic-harmonic-regression",
    "title": "Time Series Cheat Sheet in R",
    "section": "Dynamic Harmonic Regression",
    "text": "Dynamic Harmonic Regression\nDynamic Regression with K fourier terms to model seasonality. With higher K the model becomes more flexible.\nPro: Allows for any length seasonality, but assumes seasonal pattern is unchanging. Arima() and auto.arima() may run out of memory at large seasonal periods (i.e. &gt;200).\n# Example with K = 1 and predict 4 periods in the future\nfit &lt;- auto.arima(data, xreg = fourier(data, K = 1),\n                  seasonal = FALSE, lambda = 0)\npred &lt;- forecast(fit, xreg = fourier(data, K = 1, h = 4))"
  },
  {
    "objectID": "blog/2020-01-26-time-series-forecasting-in-r.html#tbats",
    "href": "blog/2020-01-26-time-series-forecasting-in-r.html#tbats",
    "title": "Time Series Cheat Sheet in R",
    "section": "TBATS",
    "text": "TBATS\nAutomated model that combines exponential smoothing, Box-Cox transformations, and Fourier terms. Pro: Automated, allows for complex seasonality that changes over time. Cons: Slow.\n\nT: Trigonemtric terms for seasonality\nB: Box-Cox transformations for heterogeneity\nA: ARMA errors for short term dynamics\nT: Trend (possibly damped)\nS: Seasonal (including multiple and non-integer periods)"
  },
  {
    "objectID": "blog/2022-12-22-color-palette-from-gif-in-julia.html",
    "href": "blog/2022-12-22-color-palette-from-gif-in-julia.html",
    "title": "Creating Color Palettes from Animated GIFs with Julia",
    "section": "",
    "text": "Have you ever watched a movie and thought to yourself: “I bet that scene would make for a great plotting theme.” Yeah, me too!\nRecently I’ve been drawing inspiration from scenes from anime such as Cyberpunk: Edgerunners (on Netflix), where rich greens, bright pinks, and yellows form a coherent color signature. Color palettes can invoke memories of characters or key moments, and I’ve found that incorporating even a few of those colors in my data visualizations has rekindled the joy of opening up my IDE and doing some EDA.\nUnfortunately, creating custom palettes is time intensive and I have more inspiration than I have free time (especially with two young kids!). Existing free online color palette generators such as Coolors, Adobe Color, and Canva reveal some limitations:\nHere’s an example of what I mean:\nI have a GIF captured from a Cyberpunk: Edgerunners music video that I want to use to make a color palette for data viz. There’s two characters in the scene, Lucy and David:\nBecause existing solutions only let you take a static image, we’re forced with taking a single frame from the animation and plug it into a few of them:"
  },
  {
    "objectID": "blog/2022-12-22-color-palette-from-gif-in-julia.html#solution-combine-clustering-with-a-few-extra-steps",
    "href": "blog/2022-12-22-color-palette-from-gif-in-julia.html#solution-combine-clustering-with-a-few-extra-steps",
    "title": "Creating Color Palettes from Animated GIFs with Julia",
    "section": "Solution: Combine clustering with a few extra steps",
    "text": "Solution: Combine clustering with a few extra steps\nGiven these gripes with existing palette generators, I decided to make a little function in Julia to extract coherent color palettes from animated GIFs. Clustering (K-means) can summarize the key colors in the GIF and we can use a few simple rules to make a data viz color palette from those key colors.\nJulia’s Images.jl library is great for working with images (to include animated GIFs), and Julia’s type system and native arrays make it trivial to do math on both static and animated images."
  },
  {
    "objectID": "blog/2022-12-22-color-palette-from-gif-in-julia.html#methods",
    "href": "blog/2022-12-22-color-palette-from-gif-in-julia.html#methods",
    "title": "Creating Color Palettes from Animated GIFs with Julia",
    "section": "Methods",
    "text": "Methods\nK-means clustering is a tried and true method for extracting color palettes from images. The basic idea with K-means is that it will iteratively find \\(K\\) cluster centers from the image, where each cluster center is a color that’s close to a bunch of colors in the image.\n\nRule 1: Ensuring that we use an actual color from the image\nThe downside of K-means is that there’s no guarantee that this color actually appears in the original image!\nWe can overcome this with a simple decision rule that substitutes each cluster center with the nearest closest color that exists in the original image. Since each pixel is just an array of numbers, we can calculate closeness using a simple measure like Euclidean distance.\n\n\nRule 2: Use colors that are different from each other\nI also want to ensure there’s a minimum distance or threshold value between every color in our color palette so that viewers can tell them apart.\nTo make this happen we can use a greedy algorithm, something like the below:\n\nGiven an initial list of \\(K\\) colors from the K-means cluster centers:\n\nInitialize our selected palette with two colors:\n\nStart with the most common color in the image - this’ll be the background color in our data viz\nFind the color with the highest distance from the most common color - this’ll be the text/axis/gridline color since it pops out the most from the background\n\nRule out any colors from the list of remaining cluster centers that don’t meet the minimum distance threshold to any of the selected colors\nFrom the remaining cluster centers, add the color that has the highest mean distance to the colors in the selected palette\nRepeat steps 2 and 3 until there are no more cluster centers to pick from"
  },
  {
    "objectID": "blog/2022-12-22-color-palette-from-gif-in-julia.html#results",
    "href": "blog/2022-12-22-color-palette-from-gif-in-julia.html#results",
    "title": "Creating Color Palettes from Animated GIFs with Julia",
    "section": "Results",
    "text": "Results\nI won’t go through the code (it’s all in this gist), but we can peek at some fun results!\n\nCyberpunk: Edgerunners\nUsing the gist as palette.jl:\ninclude(\"palette.jl\")\n\nfilepath = \"data/edgerunners mv.gif\"\n\n# Set a seed for reproducibility\nRandom.seed!(42)\n# Get the custom color palette\ntop_colors = make_palette(filepath, 0.4; n_clusters=30)\n# Create a summary plot of the results\nfig = create_summary(filepath, top_colors)\nOriginal Image: Source\n\n\n\nimg\n\n\nResult:\nNote: the summary image only shows a single frame from the GIF at the top, but we did indeed use the whole GIF to find the colors\n\n\n\nimg\n\n\nHere we see that our palette creator was able to get both the pink and green in the original image, and set the background to that cool dark blue color. It uses Lucy’s white hair color as the text/grid color and it even picked up the blue of the skin tones.\n\n\nZuko vs Azula from Avatar: The Last Airbender\nOriginal: Source\n\n\n\nimg\n\n\nResult:\n\n\n\nimg\n\n\nWatch Zuko and Azula battle it out in lineplot form!\n\n\nGenji from the Overwatch 2 Cinematic Trailer\nOriginal: Source\n\n\n\nimg\n\n\nResult:\n\n\n\nimg\n\n\nTaking this cool scene from the the Overwatch 2 Cinematic Trailer gets us a color palette that captures a lot of the key colors of the character - the color of the sword, the glowing visor, and Genji’s uniform all made its way into the palette."
  },
  {
    "objectID": "blog/2022-06-02-dash-tips.html",
    "href": "blog/2022-06-02-dash-tips.html",
    "title": "Dash (Plotly) tips - Latex, Google Fonts, and more",
    "section": "",
    "text": "Dash applications need a lot of work to make them look good. Here’s a few ‘how to’ extras that can help out"
  },
  {
    "objectID": "blog/2022-06-02-dash-tips.html#latex-in-markdown",
    "href": "blog/2022-06-02-dash-tips.html#latex-in-markdown",
    "title": "Dash (Plotly) tips - Latex, Google Fonts, and more",
    "section": "Latex in Markdown",
    "text": "Latex in Markdown\nThe latest release of Dash made it much easier to add Latex to your markdown elements. Here’s an example:\ndcc.Markdown(children=\"\"\"\nThis is Latex:\n$$\n\\\\begin{align}\ny &= x \\\\\\\\\n&= 0\n\\\\end{align}\n$$\n\"\"\", mathjax=True)\nWhich gives us:\n\\[\n\\begin{align}\ny &= x \\\\\n&= 0\n\\end{align}\n\\]\n\n\n\nimg\n\n\nThere were only two tricky bits now: you must set the mathjax=True in order to enable it, and you need to add an extra backslash (i.e.  becomes \\\\).\nThis is just an artifact of how it’s rendering escape characters."
  },
  {
    "objectID": "blog/2022-06-02-dash-tips.html#add-custom-google-fonts",
    "href": "blog/2022-06-02-dash-tips.html#add-custom-google-fonts",
    "title": "Dash (Plotly) tips - Latex, Google Fonts, and more",
    "section": "Add Custom Google Fonts",
    "text": "Add Custom Google Fonts\nYou can use CSS for everything in your Dash app except for the charts that Plotly generates. So, adding in Google fonts is relatively straightforward - you can either follow the @import instructions add a new css file to your /assets/ folder or you can add it in as an external stylesheet.\n\nLoad it in to assets:\n\n# \\assets\\fonts.css\n@import url('https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap');\n\nbody {\n    font-family: 'Press Start 2P';\n}\n\nAdd it as an external stylesheet\n\nfrom dash import Dash, html\n\nexternal_stylesheets = [\n    \"https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap\"\n]\n\napp = Dash(__name__, external_stylesheets=external_stylesheets)\n\napp.layout = html.Div([\n    # Title\n    html.H1(children=\"Dashboard Title\", id=\"db-name\",\n            style={\"font-family\": \"'Press Start 2P'\"}),\n])\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n\n\n\nimg"
  },
  {
    "objectID": "blog/2022-06-02-dash-tips.html#always-use-hovermodex-unified-for-line-charts",
    "href": "blog/2022-06-02-dash-tips.html#always-use-hovermodex-unified-for-line-charts",
    "title": "Dash (Plotly) tips - Latex, Google Fonts, and more",
    "section": "Always use hovermode='x unified' for Line Charts",
    "text": "Always use hovermode='x unified' for Line Charts\nFor line charts (I deal with a lot of time series) I always set the hovermode to x unified to allow for line vs line comparisons, and round the decimals to 2 places:\nfig.update_layout(hovermode='x unified')\nYou’ll also likely need to round down the hover text as well (see hover text and formatting)\ngo.Bar(df, x='col1', y='col2', hover_data={'col1':':.2f', 'col2':':.2f'})"
  },
  {
    "objectID": "blog/2022-06-02-dash-tips.html#move-that-legend-to-the-bottom",
    "href": "blog/2022-06-02-dash-tips.html#move-that-legend-to-the-bottom",
    "title": "Dash (Plotly) tips - Latex, Google Fonts, and more",
    "section": "Move that legend to the bottom",
    "text": "Move that legend to the bottom\nThe default Plotly legend is to the right, and on some screens this can steal all the plot real estate. Move the legend to the bottom with this snippet:\nfig.update_layout(legend=dict(yanchor='bottom',\n                              y=-.5,\n                              xanchor='auto',\n                              x=.5))\nAdjust the y position as needed to clear the xaxis label."
  },
  {
    "objectID": "blog/2022-06-02-dash-tips.html#remove-redundant-annotations-and-labels-on-facet-plots",
    "href": "blog/2022-06-02-dash-tips.html#remove-redundant-annotations-and-labels-on-facet-plots",
    "title": "Dash (Plotly) tips - Latex, Google Fonts, and more",
    "section": "Remove redundant annotations and labels on facet plots",
    "text": "Remove redundant annotations and labels on facet plots\nOne downside with using facet_row or facet_col is that plotly will add axis labels for each row or column that make things look messy. For example, if you use facet_row='category' each faceted plot will have an annoying category=blah repeated across all of your plots.\nUsually we just want to keep the part after the = sign, so we can fix that with:\ndef clean_annotations(fig):\n  \"\"\"Removes the annoying 'Feature=' label that clutters plotly graphs when you do facet_row or facet_col\"\"\"\n  fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n  return fig"
  },
  {
    "objectID": "blog/2020-06-30-quick-pnl-dataframe.html",
    "href": "blog/2020-06-30-quick-pnl-dataframe.html",
    "title": "Make a Basic PnL Dataframe in Pandas",
    "section": "",
    "text": "This code creates a practice dataframe for instruction in a format that is familiar with finance analysts. Analysts typically deal with profit and loss statement data in excel spreadsheets, often pulled or aggregated from an ERP source system and this example shows a toy example that has some relevant fields and formatting.\nThis is a very simplified version of the data that would be familiar to analysts. In reality, there would be many more fields and values, depending on the level of granularity that’s available. The columns like ‘201901 ACT’ are time periods in a YYYYQQ format and the last three letters signify whether it is an actual or forecast (POR or Plan of Record) value."
  },
  {
    "objectID": "blog/2020-06-30-quick-pnl-dataframe.html#multi-index-dataframe-slicing",
    "href": "blog/2020-06-30-quick-pnl-dataframe.html#multi-index-dataframe-slicing",
    "title": "Make a Basic PnL Dataframe in Pandas",
    "section": "Multi-Index DataFrame Slicing",
    "text": "Multi-Index DataFrame Slicing\nIt might also be useful to make this a multiindex dataframe for easier grouping/slicing and to reduce the amount of space the dataframe takes up in memory.\ndf.set_index(['ProductLine', 'FunctionalArea', 'AccountL1'], inplace=True)\n\n\n\nimg\n\n\nYou may need to use sets to slice among items in your index. Use slice(None) to skip levels in your index if needed.\ndf.loc[('A', slice(None), 'Cost of Sales')]\nand returns: \nWhich is the same as using the query() method:\ndf.query(\"ProductLine=='A' & AccountL1=='Cost of Sales'\")"
  },
  {
    "objectID": "blog/2020-06-30-quick-pnl-dataframe.html#converting-from-wide-to-long-format-with-pd.melt",
    "href": "blog/2020-06-30-quick-pnl-dataframe.html#converting-from-wide-to-long-format-with-pd.melt",
    "title": "Make a Basic PnL Dataframe in Pandas",
    "section": "Converting from Wide to Long format with pd.melt()",
    "text": "Converting from Wide to Long format with pd.melt()\nAnother useful transformation is to ‘unpivot’ the data with pd.melt().\nid_vars = ['ProductLine', 'FunctionalArea', 'AccountL1']\ndfl= df.reset_index().melt(id_vars=id_vars, var_name='Period', value_name='Value')\n\n\n\nimg"
  },
  {
    "objectID": "blog/2020-06-30-quick-pnl-dataframe.html#join-previous-cycle-for-recons",
    "href": "blog/2020-06-30-quick-pnl-dataframe.html#join-previous-cycle-for-recons",
    "title": "Make a Basic PnL Dataframe in Pandas",
    "section": "Join Previous Cycle for Recons",
    "text": "Join Previous Cycle for Recons\nYou’ll often need to reconcile PnL data against other time periods, like prior quarters, prior years, actuals vs forecasts, and so on.\nIn a spreadsheet you’d manually manipulate and select columns and subsets of data to perform this. In Python, you can perform a SQL-style lookup and join so that you create those reconciliations in a table of data that you can slice and lookup later.\nid_vars = ['ProductLine', 'FunctionalArea', 'AccountL1']\n\n# Pull in data in long format\ndfl = df.reset_index().melt(id_vars=id_vars, var_name='Period', value_name='Value')\n\n# Create new features by parsing text\ndfl['YYYY'] = dfl['Period'].apply(lambda x: x[:4]).astype(int)\ndfl['Q'] = dfl['Period'].apply(lambda x: x[5]).astype(int)\n\n# Use new features to calculate lookup columns\ndfl['Prior_Year_Period'] = ((dfl['YYYY'] - 1) *100 + dfl['Q']).astype(str) + \" ACT\"\n\n# Create a subset to use as the right dataframe\ndf2 = dfl[id_vars + ['Period', 'Value']].rename({'Value': 'PY_Value'}, axis=1)\n\n# Left Join the original dataframe with the right dataframe\ndfNew = dfl.merge(df2,\n                  left_on=id_vars+['Prior_Year_Period'],\n                  right_on=id_vars+['Period'],\n                  how='left',\n                  suffixes=[\"\", \"_DROP\"])\n\n# Drop duplicate columns\ndfNew.drop(dfNew.filter(regex='_DROP$').columns.tolist(),axis=1, inplace=True)\nAnd we see the result works. In this example, we don’t have 201801 ACT data so it’s a np.nan value, but you see the rest of the values filled in as expected for prior year values.\n\n\n\nimg"
  },
  {
    "objectID": "blog/2020-12-28-tie-fighter-error-charts-in-python.html",
    "href": "blog/2020-12-28-tie-fighter-error-charts-in-python.html",
    "title": "‘Tie Fighter’ Errorbar Charts in Python with Plotnine and Altair",
    "section": "",
    "text": "It’s December 2020 and it’s time to check out different python plotting packages. I primarily will use pandas dataframe plotting with either seaborn’s sns.set() or setting plotly as the plotting backend during data exploration, but there’s nothing in python that matches R’s ggplot2. I’ve known about plotnine as the python equivalent but last time I tried it it was still lacking in the feature department. I’m happy to say that it’s totally viable now.\nOne chart that I often used in R was a ‘tie fighter’ (error bar) chart. It’s handy to compare accuracy for various predictive models. Here’s a motivating example from ‘thecodeforest’ where he compares various time serious forecasting models in R. I’m also motivated to stick with ggplot because then I’d be able to quickly make plots in R, python, and in Julia (given how seamless RCall is in Julia and native support of dataframes).\n\nPlotnine\nHere’s what the syntax for a quick tie fighter chart in plotnine using the included mtcars dataset as a toy example:\nfrom plotnine import *\nfrom plotnine.data import mtcars\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import sem\n\n# Calc mean, std error of mpg by number of cylinders and whether they're automatic (0) or manual (1)\ndf = mtcars.groupby(['cyl', 'am'])['mpg'].agg([np.mean, sem]).reset_index()\n\np = (ggplot(df, aes('factor(cyl)', 'mean', color='factor(am)'))\n+ geom_point()\n+ geom_errorbar(aes(ymin = 'mean - sem', ymax = 'mean + sem'))\n+ coord_flip()\n+ labs(title=\"A Tie Fighter Chart\",\n        x = 'Cylinders',\n        y = 'Mean MPG +/-1 SE')\n)\n\np\n\n\n\nimg\n\n\nAnd what I love about ggplot2 (and plotnine) is the consistent syntax and how easy it is to add or remove customization and complexity compared to the other plotting libraries in python:\n# Now it's trivial to add or remove stuff to our plots, even in python!\np + theme(xkcd)\n\n\n\nimg\n\n\nOne thing we’re still missing is the ability to do subtitles and captions, but maybe someday.\n\n\nAltair\nAltair is another opinionated plotting package based on Vega-lite that has a very consistent syntax that’s worth a look.\nHere’s the same example as above, but in Altair:\nimport altair as alt\nfrom plotnine.data import mtcars\n\npoints = alt.Chart(mtcars).mark_point().encode(\n    x='average(mpg):Q',\n    y='cyl:O',\n    color='am:N',\n).properties(height=200)\n\nerror_bars = alt.Chart(mtcars).mark_errorbar(extent='stderr', ticks=True).encode(\n  x=alt.X('average(mpg):Q', scale=alt.Scale(zero=False)),\n  y=alt.Y('cyl:O'),\n  color='am:N',\n)\n\npoints+error_bars\n\n\n\nimg\n\n\nNotice that Altair has baked in aggregation and transformations so I didn’t have to do my groupby and agg in pandas. And similar to what I like in ggplot2/plotnine there’s the ability to quickly add and remove complexity. But as you can see the result was so small on my macbook pro, and I wish there was an easy way to scale the whole thing up to a reasonable aspect ratio (looks like I’d have to scale up each component separately or save as SVG)."
  },
  {
    "objectID": "blog/python_ppls_compared/python-ppls-compared.html",
    "href": "blog/python_ppls_compared/python-ppls-compared.html",
    "title": "A Trivial Comparison of Python Probabilistic Programming Languages",
    "section": "",
    "text": "In order to apply bayesian inference to real world problems, you need to pick a Probabilistic Programming Language (PPL) to express your models as code. There are a number to choose from, and each one has a specific backend that you might need to understand if you need to debug your code.\n\n\nMost Probabilistic Programming Languages (PPLs) in Python are powered by a tensor library under the hood, and this choice can greatly alter your experience. I didn’t come from a deep learning background, but some of the lower level frameworks (pyro, tensorflow probability) use these deep learning frameworks as a backend so at least surface-level understanding with these libraries will be needed when you need to debug your code and help you read others’ code.\nThis is just to say that knowing PyTorch or Tensorflow will be helpful to you and point you towards a specific language, but if you don’t know either of these then you’ll need to pick the one that looks better to you. If you had a lot of free time you could learn multiple PPLs and frameworks to see which one you prefer, but like any programming language it’s best to just pick one to start and become productive with it before moving on to another language.\n\n\n\nPPL\nBackend\n\n\n\n\npymc\npytensor\n\n\npyro\npytorch\n\n\nnumpyro\nJAX\n\n\npystan\nstan\n\n\ntensorflow probability\ntensorflow, keras, JAX\n\n\n\nWe can look at the github star histories too to see what seems to be more popular:\n\n\n\nStar History Chart\n\n\nAt the time of this writing, pymc and pyro are the two leading PPLs (in terms of github stars) but anecdotally I think you’ll find a lot more resources around pymc when it comes to examples.\n\n\n\nBelow we’ll use some examples from pymc, pyro, numpyro, and pystan each fitting a linear regression model so you can look at the syntax. The model is as follows:\n\\[\n\\begin{aligned}\n\\text{intercept} &\\sim \\operatorname{Normal}(0, 20)\\\\\n\\text{slope} &\\sim \\operatorname{Normal}(0, 20)\\\\\n\\text{sigma} &\\sim \\operatorname{HalfCauchy}(10)\\\\\n\\mu &= \\text{intercept} + \\text{slope} * x \\\\\ny &\\sim \\operatorname{Normal}(\\mu, \\sigma)\n\\end{aligned}\n\\]\nThe graph representation of this model (i.e. Plate Notation) is:\n\n\n\nModel Rendering\n\n\nThe dark circle represents the observed variable \\(y\\) and the variables in white are latent or unobserved variables that we wish to infer.\n\n\n\nThe following code will try to infer the hidden parameters from some synthetic data where the true parameters are:\n\nIntercept = 1\nSlope = 2\nSigma = 0.5\n\n\nDataPyMCPyroNumpyroPyStan\n\n\nSee below for sample code to generate the synthetic data.\n\nimport arviz as az\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulate Data\nnp.random.seed(42)\n\nsize = 200\ntrue_intercept = 1\ntrue_slope = 2\ntrue_sigma = 0.5\n\nx = np.linspace(0, 1, size)\n# y = a + b*x\ntrue_regression_line = true_intercept + true_slope * x\n# add noise\ny = true_regression_line + np.random.normal(0, true_sigma, size)\n\nplt.scatter(x, y, alpha=0.8)\nplt.plot(x, true_regression_line, c=\"r\", label=\"True Regression Line\")\nplt.legend();\n\n\n\n\n\n\n\n\n\n\npymc has undergone many changes but remains the easiest path for pythonistas to start building and running models.\n\nimport pymc as pm\n\n# model specifications in PyMC are wrapped in a with-statement\nwith pm.Model() as pymc_model:\n    # Define priors\n    sigma = pm.HalfCauchy(\"sigma\", beta=10)\n    intercept = pm.Normal(\"Intercept\", 0, sigma=20)\n    slope = pm.Normal(\"slope\", 0, sigma=20)\n\n    # Define likelihood\n    mu = intercept + slope * x\n    likelihood = pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y)\n\nWARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n\n\nInference is as simple as calling the pm.sample() function within the model context. pymc also offers additional samplers such as blackjax and numpyro that may be more performant than the default backend.\n\nwith pymc_model:\n    # draw 1000 posterior samples using NUTS and the numpyro backend\n    idata = pm.sample(1000, nuts_sampler=\"numpyro\", chains=2)\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n0.921\n0.068\n0.794\n1.046\n0.003\n0.002\n651.0\n859.0\n1.00\n\n\nsigma\n0.467\n0.024\n0.424\n0.513\n0.001\n0.001\n1075.0\n1005.0\n1.01\n\n\nslope\n2.118\n0.119\n1.908\n2.356\n0.005\n0.003\n652.0\n739.0\n1.00\n\n\n\n\n\n\n\n\n\n\nimport pyro\nimport pyro.distributions as dist\nimport torch\n\n\ndef pyro_model(x, y=None):\n    # Convert the data from numpy array to torch tensors\n    x = torch.tensor(x)\n    if y is not None:\n        y = torch.tensor(y)\n\n    # Model specification\n    sigma = pyro.sample(\"sigma\", dist.HalfCauchy(10))\n    intercept = pyro.sample(\"intercept\", dist.Normal(0, 20))\n    slope = pyro.sample(\"slope\", dist.Normal(0, 20))\n\n    mu = intercept + slope * x\n\n    # likelihood\n    pyro.sample(\"y\", dist.Normal(mu, sigma), obs=y)\n\nIf this were pymc, we’d be done by now! Here, we need to add some extra steps to perform inference while pymc tries to be more ‘batteries included’.\n\nfrom pyro.infer import MCMC, NUTS\n\nnuts_kernel = NUTS(pyro_model)\npyro_mcmc = MCMC(kernel=nuts_kernel, warmup_steps=1000, num_samples=1000, num_chains=2)\n# Run with model args\npyro_mcmc.run(x, y)\n\n\n\n/home/nelsont/.cache/pypoetry/virtualenvs/banditkings-fWuXf1Do-py3.10/lib/python3.10/site-packages/arviz/data/io_pyro.py:158: UserWarning:\n\nCould not get vectorized trace, log_likelihood group will be omitted. Check your model vectorization or set log_likelihood=False\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nintercept\n0.922\n0.066\n0.801\n1.046\n0.002\n0.002\n733.0\n937.0\n1.0\n\n\nsigma\n0.468\n0.023\n0.427\n0.511\n0.001\n0.001\n1022.0\n1170.0\n1.0\n\n\nslope\n2.113\n0.115\n1.911\n2.333\n0.004\n0.003\n761.0\n993.0\n1.0\n\n\n\n\n\n\n\n\n\nnumpyro shares many similarities with pyro but uses a faster jax backend and offers significant performance improvements over pyro. The downside is that numpyro is still under active development and may be missing a lot of functionality that pyro users have.\n\n# Modeling\nimport numpyro\nimport numpyro.distributions as dist\nfrom jax import random\nimport jax.numpy as jnp\nfrom numpyro.infer import MCMC, NUTS\n\n\n# Model specifications in numpyro are in the form of a function\ndef numpyro_model(x, y=None):\n    sigma = numpyro.sample(\"sigma\", dist.HalfCauchy(10))\n    intercept = numpyro.sample(\"Intercept\", dist.Normal(0, 20))\n    slope = numpyro.sample(\"slope\", dist.Normal(0, 20))\n\n    # define likelihood\n    mu = intercept + slope * x\n    likelihood = numpyro.sample(\"y\", dist.Normal(mu, sigma), obs=y)\n\n    return likelihood\n\nInference in numpyro is similar to pyro, with the exception of the added step to set the jax pseudo-random number generator key.\n\n# Inference\nnuts_kernel = NUTS(numpyro_model)\nmcmc = MCMC(nuts_kernel, num_chains=2, num_warmup=1000, num_samples=1000)\n\n# JAX needs an explicit pseudo-random number generator key\nrng_key = random.PRNGKey(seed=42)\n# Finally, run our sampler\nmcmc.run(rng_key, x=x, y=y)\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n0.926\n0.064\n0.801\n1.045\n0.002\n0.002\n830.0\n1070.0\n1.0\n\n\nsigma\n0.468\n0.023\n0.425\n0.512\n0.001\n0.001\n995.0\n899.0\n1.0\n\n\nslope\n2.108\n0.111\n1.883\n2.301\n0.004\n0.003\n820.0\n1145.0\n1.0\n\n\n\n\n\n\n\n\n\nPyStan offers a python interface to stan on Linux or macOS (windows user can use WSL). PyStan 3 is a complete rewrite from PyStan 2 so be careful with using legacy code. The following uses PyStan 3.10.\n\nimport stan\n\n# NOTE: Running pystan in jupyter requires nest_asyncio\nimport nest_asyncio\n\nnest_asyncio.apply()\n\n# Let's silence some warnings\nimport logging\n\n# silence logger, there are better ways to do this\n# see PyStan docs\nlogging.getLogger(\"pystan\").propagate = False\n\nstan_model = \"\"\"\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] x;\n  vector[N] y;\n}\nparameters {\n  real intercept;\n  real slope;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  // priors\n  intercept ~ normal(0, 20);\n  slope ~ normal(0, 20);\n  sigma ~ cauchy(0, 10);\n  // likelihood\n  y ~ normal(intercept + slope * x, sigma);\n}\n\"\"\"\n\n\ndata = {\"N\": len(x), \"x\": x, \"y\": y}\n\n# Build the model in stan\nposterior = stan.build(stan_model, data=data, random_seed=1)\n\n# Inference/Draw samples\nposterior_samples = posterior.sample(num_chains=2, num_samples=1000)\n\nThe result is a stan.fit.Fit object that you can run through arviz with az.summary().\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nintercept\n0.921\n0.065\n0.800\n1.039\n0.002\n0.002\n919.0\n1071.0\n1.0\n\n\nsigma\n0.467\n0.023\n0.421\n0.508\n0.001\n0.001\n828.0\n1050.0\n1.0\n\n\nslope\n2.116\n0.113\n1.913\n2.324\n0.004\n0.003\n902.0\n1066.0\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nI started with pymc for initial concepts and as a first pass, but I quickly hit a point where I needed the flexibility of a lower level language to do the kinds of modeling that I want to do. The numpy-esque syntax of the JAX backend behind numpyro seemed most appealing to me and that’s the path that I’m on."
  },
  {
    "objectID": "blog/python_ppls_compared/python-ppls-compared.html#why-are-backends-important",
    "href": "blog/python_ppls_compared/python-ppls-compared.html#why-are-backends-important",
    "title": "A Trivial Comparison of Python Probabilistic Programming Languages",
    "section": "",
    "text": "Most Probabilistic Programming Languages (PPLs) in Python are powered by a tensor library under the hood, and this choice can greatly alter your experience. I didn’t come from a deep learning background, but some of the lower level frameworks (pyro, tensorflow probability) use these deep learning frameworks as a backend so at least surface-level understanding with these libraries will be needed when you need to debug your code and help you read others’ code.\nThis is just to say that knowing PyTorch or Tensorflow will be helpful to you and point you towards a specific language, but if you don’t know either of these then you’ll need to pick the one that looks better to you. If you had a lot of free time you could learn multiple PPLs and frameworks to see which one you prefer, but like any programming language it’s best to just pick one to start and become productive with it before moving on to another language.\n\n\n\nPPL\nBackend\n\n\n\n\npymc\npytensor\n\n\npyro\npytorch\n\n\nnumpyro\nJAX\n\n\npystan\nstan\n\n\ntensorflow probability\ntensorflow, keras, JAX\n\n\n\nWe can look at the github star histories too to see what seems to be more popular:\n\n\n\nStar History Chart\n\n\nAt the time of this writing, pymc and pyro are the two leading PPLs (in terms of github stars) but anecdotally I think you’ll find a lot more resources around pymc when it comes to examples."
  },
  {
    "objectID": "blog/python_ppls_compared/python-ppls-compared.html#comparing-ppls-wth-a-simple-regression-model",
    "href": "blog/python_ppls_compared/python-ppls-compared.html#comparing-ppls-wth-a-simple-regression-model",
    "title": "A Trivial Comparison of Python Probabilistic Programming Languages",
    "section": "",
    "text": "Below we’ll use some examples from pymc, pyro, numpyro, and pystan each fitting a linear regression model so you can look at the syntax. The model is as follows:\n\\[\n\\begin{aligned}\n\\text{intercept} &\\sim \\operatorname{Normal}(0, 20)\\\\\n\\text{slope} &\\sim \\operatorname{Normal}(0, 20)\\\\\n\\text{sigma} &\\sim \\operatorname{HalfCauchy}(10)\\\\\n\\mu &= \\text{intercept} + \\text{slope} * x \\\\\ny &\\sim \\operatorname{Normal}(\\mu, \\sigma)\n\\end{aligned}\n\\]\nThe graph representation of this model (i.e. Plate Notation) is:\n\n\n\nModel Rendering\n\n\nThe dark circle represents the observed variable \\(y\\) and the variables in white are latent or unobserved variables that we wish to infer."
  },
  {
    "objectID": "blog/python_ppls_compared/python-ppls-compared.html#code",
    "href": "blog/python_ppls_compared/python-ppls-compared.html#code",
    "title": "A Trivial Comparison of Python Probabilistic Programming Languages",
    "section": "",
    "text": "The following code will try to infer the hidden parameters from some synthetic data where the true parameters are:\n\nIntercept = 1\nSlope = 2\nSigma = 0.5\n\n\nDataPyMCPyroNumpyroPyStan\n\n\nSee below for sample code to generate the synthetic data.\n\nimport arviz as az\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulate Data\nnp.random.seed(42)\n\nsize = 200\ntrue_intercept = 1\ntrue_slope = 2\ntrue_sigma = 0.5\n\nx = np.linspace(0, 1, size)\n# y = a + b*x\ntrue_regression_line = true_intercept + true_slope * x\n# add noise\ny = true_regression_line + np.random.normal(0, true_sigma, size)\n\nplt.scatter(x, y, alpha=0.8)\nplt.plot(x, true_regression_line, c=\"r\", label=\"True Regression Line\")\nplt.legend();\n\n\n\n\n\n\n\n\n\n\npymc has undergone many changes but remains the easiest path for pythonistas to start building and running models.\n\nimport pymc as pm\n\n# model specifications in PyMC are wrapped in a with-statement\nwith pm.Model() as pymc_model:\n    # Define priors\n    sigma = pm.HalfCauchy(\"sigma\", beta=10)\n    intercept = pm.Normal(\"Intercept\", 0, sigma=20)\n    slope = pm.Normal(\"slope\", 0, sigma=20)\n\n    # Define likelihood\n    mu = intercept + slope * x\n    likelihood = pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y)\n\nWARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n\n\nInference is as simple as calling the pm.sample() function within the model context. pymc also offers additional samplers such as blackjax and numpyro that may be more performant than the default backend.\n\nwith pymc_model:\n    # draw 1000 posterior samples using NUTS and the numpyro backend\n    idata = pm.sample(1000, nuts_sampler=\"numpyro\", chains=2)\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n0.921\n0.068\n0.794\n1.046\n0.003\n0.002\n651.0\n859.0\n1.00\n\n\nsigma\n0.467\n0.024\n0.424\n0.513\n0.001\n0.001\n1075.0\n1005.0\n1.01\n\n\nslope\n2.118\n0.119\n1.908\n2.356\n0.005\n0.003\n652.0\n739.0\n1.00\n\n\n\n\n\n\n\n\n\n\nimport pyro\nimport pyro.distributions as dist\nimport torch\n\n\ndef pyro_model(x, y=None):\n    # Convert the data from numpy array to torch tensors\n    x = torch.tensor(x)\n    if y is not None:\n        y = torch.tensor(y)\n\n    # Model specification\n    sigma = pyro.sample(\"sigma\", dist.HalfCauchy(10))\n    intercept = pyro.sample(\"intercept\", dist.Normal(0, 20))\n    slope = pyro.sample(\"slope\", dist.Normal(0, 20))\n\n    mu = intercept + slope * x\n\n    # likelihood\n    pyro.sample(\"y\", dist.Normal(mu, sigma), obs=y)\n\nIf this were pymc, we’d be done by now! Here, we need to add some extra steps to perform inference while pymc tries to be more ‘batteries included’.\n\nfrom pyro.infer import MCMC, NUTS\n\nnuts_kernel = NUTS(pyro_model)\npyro_mcmc = MCMC(kernel=nuts_kernel, warmup_steps=1000, num_samples=1000, num_chains=2)\n# Run with model args\npyro_mcmc.run(x, y)\n\n\n\n/home/nelsont/.cache/pypoetry/virtualenvs/banditkings-fWuXf1Do-py3.10/lib/python3.10/site-packages/arviz/data/io_pyro.py:158: UserWarning:\n\nCould not get vectorized trace, log_likelihood group will be omitted. Check your model vectorization or set log_likelihood=False\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nintercept\n0.922\n0.066\n0.801\n1.046\n0.002\n0.002\n733.0\n937.0\n1.0\n\n\nsigma\n0.468\n0.023\n0.427\n0.511\n0.001\n0.001\n1022.0\n1170.0\n1.0\n\n\nslope\n2.113\n0.115\n1.911\n2.333\n0.004\n0.003\n761.0\n993.0\n1.0\n\n\n\n\n\n\n\n\n\nnumpyro shares many similarities with pyro but uses a faster jax backend and offers significant performance improvements over pyro. The downside is that numpyro is still under active development and may be missing a lot of functionality that pyro users have.\n\n# Modeling\nimport numpyro\nimport numpyro.distributions as dist\nfrom jax import random\nimport jax.numpy as jnp\nfrom numpyro.infer import MCMC, NUTS\n\n\n# Model specifications in numpyro are in the form of a function\ndef numpyro_model(x, y=None):\n    sigma = numpyro.sample(\"sigma\", dist.HalfCauchy(10))\n    intercept = numpyro.sample(\"Intercept\", dist.Normal(0, 20))\n    slope = numpyro.sample(\"slope\", dist.Normal(0, 20))\n\n    # define likelihood\n    mu = intercept + slope * x\n    likelihood = numpyro.sample(\"y\", dist.Normal(mu, sigma), obs=y)\n\n    return likelihood\n\nInference in numpyro is similar to pyro, with the exception of the added step to set the jax pseudo-random number generator key.\n\n# Inference\nnuts_kernel = NUTS(numpyro_model)\nmcmc = MCMC(nuts_kernel, num_chains=2, num_warmup=1000, num_samples=1000)\n\n# JAX needs an explicit pseudo-random number generator key\nrng_key = random.PRNGKey(seed=42)\n# Finally, run our sampler\nmcmc.run(rng_key, x=x, y=y)\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n0.926\n0.064\n0.801\n1.045\n0.002\n0.002\n830.0\n1070.0\n1.0\n\n\nsigma\n0.468\n0.023\n0.425\n0.512\n0.001\n0.001\n995.0\n899.0\n1.0\n\n\nslope\n2.108\n0.111\n1.883\n2.301\n0.004\n0.003\n820.0\n1145.0\n1.0\n\n\n\n\n\n\n\n\n\nPyStan offers a python interface to stan on Linux or macOS (windows user can use WSL). PyStan 3 is a complete rewrite from PyStan 2 so be careful with using legacy code. The following uses PyStan 3.10.\n\nimport stan\n\n# NOTE: Running pystan in jupyter requires nest_asyncio\nimport nest_asyncio\n\nnest_asyncio.apply()\n\n# Let's silence some warnings\nimport logging\n\n# silence logger, there are better ways to do this\n# see PyStan docs\nlogging.getLogger(\"pystan\").propagate = False\n\nstan_model = \"\"\"\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] x;\n  vector[N] y;\n}\nparameters {\n  real intercept;\n  real slope;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  // priors\n  intercept ~ normal(0, 20);\n  slope ~ normal(0, 20);\n  sigma ~ cauchy(0, 10);\n  // likelihood\n  y ~ normal(intercept + slope * x, sigma);\n}\n\"\"\"\n\n\ndata = {\"N\": len(x), \"x\": x, \"y\": y}\n\n# Build the model in stan\nposterior = stan.build(stan_model, data=data, random_seed=1)\n\n# Inference/Draw samples\nposterior_samples = posterior.sample(num_chains=2, num_samples=1000)\n\nThe result is a stan.fit.Fit object that you can run through arviz with az.summary().\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nintercept\n0.921\n0.065\n0.800\n1.039\n0.002\n0.002\n919.0\n1071.0\n1.0\n\n\nsigma\n0.467\n0.023\n0.421\n0.508\n0.001\n0.001\n828.0\n1050.0\n1.0\n\n\nslope\n2.116\n0.113\n1.913\n2.324\n0.004\n0.003\n902.0\n1066.0\n1.0"
  },
  {
    "objectID": "blog/python_ppls_compared/python-ppls-compared.html#final-thoughts",
    "href": "blog/python_ppls_compared/python-ppls-compared.html#final-thoughts",
    "title": "A Trivial Comparison of Python Probabilistic Programming Languages",
    "section": "",
    "text": "I started with pymc for initial concepts and as a first pass, but I quickly hit a point where I needed the flexibility of a lower level language to do the kinds of modeling that I want to do. The numpy-esque syntax of the JAX backend behind numpyro seemed most appealing to me and that’s the path that I’m on."
  },
  {
    "objectID": "blog/a-friendly-introduction-to-scan-with-jax.html",
    "href": "blog/a-friendly-introduction-to-scan-with-jax.html",
    "title": "A friendly introduction to scan with JAX",
    "section": "",
    "text": "Learning scan - it’s all over the numpyro time series examples so if I want to implement time series in numpyro from scratch, I’ll need to better understand it.\nIn the context of time series models, scan is helpful because it allows us to iterate over sequences (without a cumbersome for loop) while maintaining state across each iteration.\n\n\nFirst, let’s break down an example scan function into its’ components:\n\n\n\nimg"
  },
  {
    "objectID": "blog/a-friendly-introduction-to-scan-with-jax.html#elements-of-a-scan",
    "href": "blog/a-friendly-introduction-to-scan-with-jax.html#elements-of-a-scan",
    "title": "A friendly introduction to scan with JAX",
    "section": "",
    "text": "First, let’s break down an example scan function into its’ components:\n\n\n\nimg"
  },
  {
    "objectID": "blog/a-friendly-introduction-to-scan-with-jax.html#trivial-example-2-a-little-more-of-the-scan-function-with-compounding-interest",
    "href": "blog/a-friendly-introduction-to-scan-with-jax.html#trivial-example-2-a-little-more-of-the-scan-function-with-compounding-interest",
    "title": "A friendly introduction to scan with JAX",
    "section": "Trivial Example 2: A little more of the scan function with compounding interest",
    "text": "Trivial Example 2: A little more of the scan function with compounding interest\nThe first two parameters in the input function can have arbitrary names but their order matters. The first item is the ‘carried over value’, aka the carry. The second item is the current element in the xs input array.\nSimilarly the input function must have two outputs. The first output becomes the new ‘carried over value’ that will feed back into the input function, while the second output gets appended to an output jax array.\nLet’s have a concrete example in which we calculate compounded interest for a $100 investment with a 4% interest rate:\n# Define the input function to apply at each time step\ndef interest_growth(value, x):\n    value = value * 1.04\n    return value, value # ('carried over value', 'accumulated array')\n\n# Define the initial state and input sequence\ninit_state = 100.0\nT = jnp.array([1, 2, 3, 4, 5])\nT = jnp.arange(1, 30)\n\n# Apply the scan function\nfinal_state, results = lax.scan(f=interest_growth, init=init_state, xs=T)\n\nprint(final_state)\nprint(results)\n311.8648\n[104.       108.159996 112.48639  116.98584  121.66527  126.531876\n 131.59314  136.85686  142.33113  148.02437  153.94534  160.10315\n 166.50726  173.16754  180.09424  187.298    194.78992  202.58151\n 210.68477  219.11215  227.87663  236.99168  246.47134  256.3302\n 266.5834   277.24673  288.33658  299.87003  311.8648  ]\n\n\n\nimg\n\n\nAnd we see the effect of the interest compounding over time."
  },
  {
    "objectID": "blog/a-friendly-introduction-to-scan-with-jax.html#trivial-example-3-scan-in-numpyro-to-sample-from-a-distribution",
    "href": "blog/a-friendly-introduction-to-scan-with-jax.html#trivial-example-3-scan-in-numpyro-to-sample-from-a-distribution",
    "title": "A friendly introduction to scan with JAX",
    "section": "Trivial Example 3: scan in numpyro to sample from a distribution",
    "text": "Trivial Example 3: scan in numpyro to sample from a distribution\nAnd what I’m really interested in is learning scan in the context of doing time series in numpyro.\nnumpyro has it’s own (experimental) implementation of scan which allows us to iteratively sample from numpyro primitives like numpyro.distributions.\nHere’s a simple example of a random walk:\n\\[\ny_{t+1} = y_t + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1)\n\\]\nOr, equivalently:\n\\[\n\\begin{aligned}\ny_t &= \\mu_t\\\\\n\\mu_{t+1} &= \\mu_t + \\epsilon_\\mu, \\quad \\epsilon_\\mu \\sim \\mathcal{N}(0, 1)\\\\\n\\end{aligned}\n\\]\nWe need to use scan because each subsequent value of \\(\\mu_{t+1}\\) depends on its previous value \\(\\mu_{t}\\).\nTo do this within a scan we’ll also need to introduce an effect handler: numpyro.handlers.seed and run the scan function within that context.\n# Import libraries\nimport numpyro\nfrom numpyro.contrib.control_flow import scan\nimport numpyro.distributions as dist\nimport matplotlib.pyplot as plt\n\n# Define our random walk function as an input into `scan`\ndef random_walk(value, x):\n    value = numpyro.sample('x', dist.Normal(value, 1.))\n    # Which is the same as if we were very explicit:\n    # value = value + numpyro.sample('x', dist.Normal(0, 1.))\n    return value, value\n\ninit_0 = 0\nT = jnp.arange(100)\n\n# Need the `seed` effect handler in a context manager because of\n# the need for a PRNG key within `numpyro.sample`\nwith numpyro.handlers.seed(rng_seed=42):\n    final_location, locations = scan(random_walk, init=init_0, xs=T)\n\nplt.plot(T, locations)\nplt.title('Simulated Gaussian Random Walk');\n\n\n\nimg\n\n\n\nAside: What if I didn’t use numpyro.handlers.seed there?\nNote the addition of numpyro.handlers.seed. This is required due to how numpyro requires an explicit PRNG key when you sample from a distribution, so this seed effect handler will automatically do the splits on the key within each loop of the scan function.\nOtherwise, if you set up an explicit key with scan this is what you’d get:\n# What happens if you don't use `numpyro.handlers.seed`?\nprng_key = random.PRNGKey(42)\n\ndef random_walk_nokey(value, x):\n    # Being really explicit here\n    value = value + numpyro.sample('x', dist.Normal(0, 1.), rng_key=prng_key)\n    return value, value\n\ninit_0 = 0\nn_steps = 100\n\nstate, final_result = scan(random_walk_nokey, init=init_0, xs=jnp.arange(n_steps))\n\nplt.plot(jnp.arange(n_steps), final_result);\n\n\n\nimg\n\n\nSo that’s why you need to use the handler!"
  },
  {
    "objectID": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html",
    "href": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html",
    "title": "Time Series Forecasting 102 for Finance Analysts",
    "section": "",
    "text": "In our last conversation, we shared some examples of time series forecasting models ranging from classical time series methods, judgment-based forecasts and rules of thumb, regression-based models, and more. How do you know which one to use, and which one is the best?\nTo start, the definition of ‘best’ depends on the problem you’re trying to solve. Imagine yourself as a revenue analyst, here are the kinds of forecasting questions you’d ask and answer:\n\nWhat’s the expected revenue for product X this quarter?\nWhat would the revenue for product X be if we increased our direct marketing by $Y?\n\nFor the first question, the ‘best’ model is the model that is the most ‘accurate’. For the second question, you’re more interested building the best possible estimate of how those relationships affect the outcome.\nFor this article, I’ll focus on answering the first question, and will answer the second question in a follow-up article."
  },
  {
    "objectID": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html#how-can-i-tell-if-my-forecast-is-any-good",
    "href": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html#how-can-i-tell-if-my-forecast-is-any-good",
    "title": "Time Series Forecasting 102 for Finance Analysts",
    "section": "",
    "text": "In our last conversation, we shared some examples of time series forecasting models ranging from classical time series methods, judgment-based forecasts and rules of thumb, regression-based models, and more. How do you know which one to use, and which one is the best?\nTo start, the definition of ‘best’ depends on the problem you’re trying to solve. Imagine yourself as a revenue analyst, here are the kinds of forecasting questions you’d ask and answer:\n\nWhat’s the expected revenue for product X this quarter?\nWhat would the revenue for product X be if we increased our direct marketing by $Y?\n\nFor the first question, the ‘best’ model is the model that is the most ‘accurate’. For the second question, you’re more interested building the best possible estimate of how those relationships affect the outcome.\nFor this article, I’ll focus on answering the first question, and will answer the second question in a follow-up article."
  },
  {
    "objectID": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html#accuracy---the-basics",
    "href": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html#accuracy---the-basics",
    "title": "Time Series Forecasting 102 for Finance Analysts",
    "section": "Accuracy - The Basics",
    "text": "Accuracy - The Basics\n\nError\nTo help us answer the first question “What’s the expected revenue for product X this quarter?”, you might come up with a number and then measure how your forecast differed from what actually happened after the fact. Here’s an example:\n\\[\n\\text{Error} = \\text{forecast} - \\text{actuals}\n\\]\nSo let’s say that you predicted that revenue will be $26M, but the actuals came in at $25M. So the error is $26M-$25M = $1M.\n\n\n\nproduct\nforecast\nactuals\nerror\n\n\n\n\nProduct 1\n26\n25\n1\n\n\n\n\n\nHow to Lie with Accuracy\nWhile this is a very simple and straightforward example, you’d be surprised how this can get you into trouble.\nHere’s an example that I’ve seen here at work. Let’s say that you are forecasting for not just one product, but two products. Then naively, how would you calculate the total error across all of your products? Let’s say that \\(\\text{Error}_1\\) is the error for product 1 and \\(\\text{Error}_2\\) is the error for product 2, then the Total Error would be the total of these two error numbers:\n\\[\n\\begin{aligned}\n\\text{Total Error} &= \\text{Error}_1 + \\text{Error}_2 \\\\\n&=(\\text{forecast}_1 - \\text{actuals}_1) + (\\text{forecast}_2 - \\text{actuals}_2)\n\\end{aligned}\n\\]\nCan you spot the problem? Imagine that \\(\\text{Error}_2\\) is negative. To make it concrete, imagine that \\(\\text{Error}_1\\) is still $1M and \\(\\text{Error}_2\\) is $22M - $23M = $1M, then the Total Error is $1M - $1M = $0M.\n\n\n\nproduct\nforecast\nactuals\nerror\n\n\n\n\nProduct 1\n26\n25\n1\n\n\nProduct 2\n22\n23\n-1\n\n\nTotal\n48\n48\n0\n\n\n\nGreat, we have a perfect model! Then, if we want to find out how well this model performed on average you can take the mean:\n\\[\n\\begin{aligned}\n\\text{Mean Error} &= \\frac{1}{n}\\text{Total Error} \\\\\n&= \\frac{1}{2} * 0 \\\\\n&= 0\n\\end{aligned}\n\\]\n\nDid we just create the perfect model?\n\nNo! The errors just offset. While you can happily report that your model reported perfect accuracy at the aggregated level, this might make you overconfident in how well your model is truly performing. And as analysts, our north star should be truth.\n\n\nMean Absolute Error\nSo how do we be more honest? First off, you can solve this issue by taking the absolute value of the errors. Then in the previous example, the Total Error becomes $2M instead of $0, and you have a much better measure of the true accuracy of your model.\n\n\n\nproduct\nforecast\nactuals\nabsolute error\n\n\n\n\nProduct 1\n26\n25\n1\n\n\nProduct 2\n22\n23\n1\n\n\nMean\n\n\nMAE: 2/2 = 1\n\n\n\nSo in this example, this better reflects the reality that the average forecast was off by $1M. And this error metric is still fantastic for explainability because it’s easy to understand.\nBut what if the scales of the two products were drastically different? Let’s say, sales in product 1 were in $M (millions) and sales in product 2 were in $K (thousands). Then you would be potentially overestimating or underestimating the average performance of your model!\nWe can solve this by scaling our performance metrics."
  },
  {
    "objectID": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html#scaled-errors",
    "href": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html#scaled-errors",
    "title": "Time Series Forecasting 102 for Finance Analysts",
    "section": "Scaled Errors",
    "text": "Scaled Errors\nIn order to compare forecasts in which the underlying data has widely different scales, you should use a scaled error metric.\n\nMean Absolute Percent Error\nHow many of us have been asked to provide a forecast or a prediction that should be within +/- 2% of the final number? This is a great example of an accuracy measure, and is known as “Mean Absolute Percent Error” or MAPE. Like Mean Absolute Error (MAE), MAPE is very easy to understand, calculate, and explain. It can also be used when you have forecasts at different scales.\n\\[\n\\begin{aligned}\n\\text{MAPE} &= \\frac{100\\%}{n} \\times \\left(\\frac{|\\text{Error}_1|}{\\text{Actuals}_1} + \\frac{|\\text{Error}_2|}{\\text{Actuals}_2} \\right) \\\\\n&= \\frac{100\\%}{2} \\times \\left(\\frac{|1|}{25} + \\frac{|-1|}{23}\\right) \\\\\n&= 4.18\\%\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\nproduct\nforecast\nactuals\nabsolute error\nabsolute percent error\n\n\n\n\nProduct 1\n26\n25\n1\n1/25 = 4%\n\n\nProduct 2\n22\n23\n1\n1/23 = 4.34%\n\n\nMean\n\n\n1\n4.18%\n\n\n\n\n\nCareful with MAPE!\nFor many cases, MAPE is an ideal metric to use because of how easy it is for folks to understand and interpret. However, MAPE isn’t perfect - it can be infinitely large. For example, let’s say that you had a revenue forecast at $9M and the actuals came in at $1M. Then, the percent error is 800%. Then, our data becomes:\n\n\n\n\n\n\n\n\n\n\nproduct\nforecast\nactuals\nabsolute error\nabsolute percent error\n\n\n\n\nProduct 1\n26\n25\n1\n1/25 = 4%\n\n\nProduct 2\n22\n23\n1\n1/23 = 4.34%\n\n\nProduct 3\n9\n1\n8\n8/1 = 800%\n\n\nMean\n\n\n\n269.5%\n\n\n\nNow, instead of making our model look better than it actually is, it’s making it look worse because that one product is skewing the results.\nThis example is due to the non-symmetric nature of MAPE. If you set your forecast to $0 or very nearly zero, then the percent error will be at most 100%. But if you set your forecast infinitely higher than the actuals, then the MAPE would go to infinity too. Here at work I’ve seen scores in the 127,000% MAPE range.\nSo how do you handle this? You might be tempted to just report the total MAPE score and call it good, right?\n\n\n\n\n\n\n\n\n\n\nproduct\nforecast\nactuals\nabsolute error\nabsolute percent error\n\n\n\n\nProduct 1\n26\n25\n1\n4%\n\n\nProduct 2\n22\n23\n1\n4.34%\n\n\nProduct 3\n9\n1\n8\n800%\n\n\nTotal\n57\n49\n10\n10/49 = 16.33%\n\n\n\nSo then you can confidently say that your model is off by +/- 16.33% when you only look at the total aggregated performance. But on average, this doesn’t change the fact that your average model performance accuracy is about 270%. In practice, it’s common to have a single forecast have infinite percent error, which means that the average model performance accuracy goes to infinity too!\nHow would you compare different models if both models have infinite error?"
  },
  {
    "objectID": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html#alternatives-to-mape",
    "href": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html#alternatives-to-mape",
    "title": "Time Series Forecasting 102 for Finance Analysts",
    "section": "Alternatives to MAPE",
    "text": "Alternatives to MAPE\nSo if you have models that both report infinite errors and want to compare them against each other, then there are two approaches: sMAPE and MASE.\n\nsMAPE (Symmetric Mean Absolute Percent Error)\nThe ‘symmetric’ MAPE score (SMAPE) is a MAPE score that is adjusted for the non-symmetric nature of MAPE and bounds the score between 0% and 200%. However, the interpretation becomes more complicated because they aren’t true percentages in the MAPE sense…rather, you can consider the error to range between 0 and 200, with lower being better.\nThis is the preferred metric for state of the art forecasting competitions (the M-series) because it lets you get a score across any kinds of time series.\n\n\nMASE (Mean Absolute Scaled Error)\nAnother common metric is the Mean Absolute Scaled Error (MASE), which compares a model’s accuracy against some baseline model. The most common baseline model is a naive model, where the predicted value is simply the most recent historical value. So for instance, in a naive model where you have actual revenue of $0 this month, you would predict that next month’s revenue would also be $0.\n\n\n\n\nJan\nFeb\nMar\nApr\n\n\n\n\nActuals\n0\n1\n4\n6\n\n\nNaive Prediction\n\n0\n1\n4\n\n\nNaive Model Error\n\n1\n3\n2\n\n\n\nThen, to calculate the scaled error you simply divide your new model’s error by the average Naive error. Here’s an example with a new model that simply predicts 3 for each month after January:\n\n\n\n\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMean\n\n\n\n\nActuals\n0\n1\n4\n6\n\n\n\nNaive Prediction\n\n0\n1\n4\n\n\n\nNaive Model Error\n\n1\n3\n2\n2\n\n\nNew Model\n\n3\n3\n3\n\n\n\nAbsolute Scaled Error\n\n|3-1|/2 = 1\n|3-4|/2 = 1/2\n|3-6|/2 = 3/2\n(1 + 1/2 + 3/2)/3 = 1\n\n\n\nThe final MASE Score in the above example is 1, which indicates that this model does not perform any better or worse than the baseline Naive model. If the MASE score was less than 1, that means that the new model’s accuracy is better than the baseline model, while if the MASE score is above 1, that means that the new model’s accuracy is worse.\nMASE is useful because it can give you an interpretable forecast, especially when you have an existing baseline that you want to compare against. Often a +/- 2% accuracy metric isn’t possible, but if your model offers a measurable improvement against an existing baseline then it might still be worth using your model despite not getting within 2% MAPE of your target!"
  },
  {
    "objectID": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html#summary-of-error-metrics",
    "href": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html#summary-of-error-metrics",
    "title": "Time Series Forecasting 102 for Finance Analysts",
    "section": "Summary of Error Metrics",
    "text": "Summary of Error Metrics\n\n\n\nimg\n\n\n\n\n\n\n\n\n\n\nMetric\nPros\nCons\n\n\n\n\nMAE\nMost Interpretable\nBest if units are the same\n\n\nMAPE\nInterpretable, works across multiple scales\nCan be infinite\n\n\nsMAPE\nScales errors between 0 and 200\nLess interpretable than MAPE\n\n\nMASE\nEasy to interpret, can’t go to infinity\nLess interpretable than MAPE"
  },
  {
    "objectID": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html#validating-the-model-backtesting",
    "href": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html#validating-the-model-backtesting",
    "title": "Time Series Forecasting 102 for Finance Analysts",
    "section": "Validating the Model (Backtesting)",
    "text": "Validating the Model (Backtesting)\nThe above examples all compared a single point forecast for multiple products, but did you just get lucky that one quarter, or does your method consistently deliver accurate results? How would you know?\nThe way to answer this question is to use “time series cross-validation”, also known as “Backtesting” your forecasting method because you’re testing how your model would have performed back in the day.\n\nThe intuition behind this is that you repeatedly calculate the forecast at different periods in your history, but only using the subset of historical data that would have been available to you at the time.\n\nThis diagram from Hyndman’s Forecasting: Principles and Practice explains this concept, where each dot is a point in time. The blue dots represent the historical training data that you use in your algorithm, and the red dot is the point in time in which you forecast:\n\n\n\nimg\n\n\nYou then calculate your forecast accuracy using one of the metrics listed above, and calculate the average over all of the historical backtests.\n\nThe downside of judgmental forecasts\nSo this is the downside of using analyst intuition and judgmental forecasts - most of the time you can’t go back in time and recreate your judgmental forecast using the knowledge available to you at that moment in history. So without taking an algorithmic approach, we often find ourselves stuck unable to find ways to measurably improve our forecasts because we fundamentally can’t determine how ‘good’ our forecasts are!"
  },
  {
    "objectID": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html#conclusion",
    "href": "blog/2022-06-08-Time-Series-Forecasting-102-Finance.html#conclusion",
    "title": "Time Series Forecasting 102 for Finance Analysts",
    "section": "Conclusion",
    "text": "Conclusion\nIn addressing a handful of common error metrics, I’ve left out a lot of details - there are many, many more types of error metrics out there and choosing the right error metric is a challenging task. When evaluating different forecasting models, it’s common to combine and weight the outputs from multiple error metrics and arrive at a weighted ‘score’ in which you base your model selection decision. This is where the art of analysis comes in - not only is it important for you to understand how various models work and how to tweak their performance, you must also impartially create a model selection process that works for you and your business problem.\nI also only covered a problem in which ‘best’ means the model with the highest accuracy score (or conversely, the model with the least amount of error). In a future post, I’ll talk about what it means to use a model to find relationships and help you guide decisionmaking, where accuracy takes a backseat to interpretability!"
  },
  {
    "objectID": "blog/2021-08-01-build-your-first-plotly-template.html",
    "href": "blog/2021-08-01-build-your-first-plotly-template.html",
    "title": "Making a Basic Corporate Plotly Template",
    "section": "",
    "text": "I got a little lost reading the plotly docs on saving a custom plotly template, so here’s a basic skeleton of a template inspired by Nike’s 10K that you can use to customize basic plot colors, fonts, and show you how the template is structured. I also structured the code to be a little more readible for me since I didn’t like their use of both dict and their ‘magic underscore notation’ which made things more confusing. When you make a template IRL they are going to be pretty verbose and magic underscore notation is not helpful.\nHere’s the example. Oh, and you should save it as a file like my_template.py so you can share it as an internal package that people can install with pip or something:"
  },
  {
    "objectID": "blog/2021-08-01-build-your-first-plotly-template.html#layout-choose-fonts-and-colors",
    "href": "blog/2021-08-01-build-your-first-plotly-template.html#layout-choose-fonts-and-colors",
    "title": "Making a Basic Corporate Plotly Template",
    "section": "Layout: Choose Fonts and Colors",
    "text": "Layout: Choose Fonts and Colors\nThe first thing I do when building out a template layout is I get inspiration from a company’s 10k/10Q financial filings and their branding to get their fonts and colors. Google ‘(company name) investor relations’ and that’ll take you to a site where they have a bunch of their financial statements and letters to shareholders and such.\n\nFonts:\n\nEither take a screenshot of the font and run it through a font identifier like FontSquirrell’s matcherator (hit or miss)\nOpen their site in Chrome, highlight the font text and then hit ‘inspect’ to get the Styles for font-family, etc.\n\nColors: I will take a screenshot and run it through a color picker site like imagecolorpicker.com that will try to get the hex codes ordered from left to right based on the proportion of the color in the image. These would then go to your colorway as items in a list in a similar order.\n\nAlternatively, here’s a great article on How to make a color palette from 1 color or this shorter article from Kaggle\n\n\nThere’s a ton of other options to choose from in the layout section, check the official layout Docs."
  },
  {
    "objectID": "blog/2021-08-01-build-your-first-plotly-template.html#data-add-chart-specific-styling",
    "href": "blog/2021-08-01-build-your-first-plotly-template.html#data-add-chart-specific-styling",
    "title": "Making a Basic Corporate Plotly Template",
    "section": "Data: Add chart-specific styling",
    "text": "Data: Add chart-specific styling\nThe data property is used for specific plotly.graph_objects styling, like for bar charts and stuff.\nFor the full list of options for each item in the ‘data’ property, you’ll need to read the plotly documentation for each graph object (i.e. for bar plots look up graph_objects.bar) and go from there."
  },
  {
    "objectID": "blog/2021-08-01-build-your-first-plotly-template.html#protip-practice-with-built-in-plotly-datasets",
    "href": "blog/2021-08-01-build-your-first-plotly-template.html#protip-practice-with-built-in-plotly-datasets",
    "title": "Making a Basic Corporate Plotly Template",
    "section": "Protip: Practice with built-in Plotly datasets",
    "text": "Protip: Practice with built-in Plotly datasets\nPlotly has a bunch of built-in datasets that you should practice with when building POC’s, but the list is hard to find. Here’s the API reference."
  },
  {
    "objectID": "blog/2021-08-01-build-your-first-plotly-template.html#finished-example",
    "href": "blog/2021-08-01-build-your-first-plotly-template.html#finished-example",
    "title": "Making a Basic Corporate Plotly Template",
    "section": "Finished Example:",
    "text": "Finished Example:\nimport plotly.express as px\nimport my_template # Import your template here\n\n# load demo data\nfrom plotly.data import tips\ndf = tips()\ndf = df[['smoker', 'size', 'tip']].groupby(by=['smoker', 'size'], as_index=False).mean()\n\n# Plot with the new template\nfig = px.bar(df, x='size', y='tip', color='smoker',\n             template='nike', title='Template Example')\nfig.show()\n\n\n\nimg\n\n\nTested on Plotly 4.14.3\nReference:\n\nPlotly Theming and Templates docs\nPlotly text and annotation docs\nPlotly Datasets"
  },
  {
    "objectID": "blog/2022-03-27-sql-to-julia.html",
    "href": "blog/2022-03-27-sql-to-julia.html",
    "title": "A SQL to Julia DataFrames Cheatsheet",
    "section": "",
    "text": "A SQL to Julia DataFrames dictionary using DataFrames and DataFramesMeta. In general, I use DataFramesMeta since it abstracts away some lower level nuance and it makes for a tidier workflow when I’m constructing queries."
  },
  {
    "objectID": "blog/2022-03-27-sql-to-julia.html#querying-in-dataframes-and-dataframesmeta",
    "href": "blog/2022-03-27-sql-to-julia.html#querying-in-dataframes-and-dataframesmeta",
    "title": "A SQL to Julia DataFrames Cheatsheet",
    "section": "Querying in DataFrames and DataFramesMeta",
    "text": "Querying in DataFrames and DataFramesMeta\n\n\n\n\n\n\n\n\nSQL Clause\nDataFrames Equivalent\nDataFramesMeta Equivalent\n\n\n\n\nSELECT\nselect\n@select\n\n\nWHERE\nsubset(df, :colnm =&gt; ByRow(x -&gt; x&gt;1))\n@rsubset(df, :colnm &gt; 1)\n\n\nLIKE\nsubset(df, :colnm=&gt;ByRow(x -&gt; occursin(r\"string\", x)))\n@rsubset(df, occursin(r\"string\", :colnm))\n\n\nGROUP BY\ngd = groupby(df, :colnm)combine(gd, :colnm=&gt;sum)\ngd = groupby(df, :colnm)@combine(gd, :newcol=sum(:colnm))or @by(df, :colnm, :newcol=sum(:colnm))\n\n\nSUM(colnm) OVER (PARTITION BY other_colnm) AS newcol\ngd = groupby(df, :colnm)transform(gd, :colnm=&gt;sum)\ngd = groupby(df, :colnm)@transform(gd, :newcol=sum(:colnm))\n\n\nORDER BY colnm ASC\nsort(df, :colnm)\n@orderby(df, :colnm)\n\n\nORDER BY colnm DESC\nsort(df, :colnm, rev=true)\n@orderby(df, sort(:colnm, rev=true)"
  },
  {
    "objectID": "blog/2022-03-27-sql-to-julia.html#joins",
    "href": "blog/2022-03-27-sql-to-julia.html#joins",
    "title": "A SQL to Julia DataFrames Cheatsheet",
    "section": "Joins",
    "text": "Joins\n\n\n\n\n\n\n\nSQL Clause\nDataFrames Equivalent\n\n\n\n\nINNER JOIN\ninnerjoin(df, df2, on=:colnm)innerjoin(df, df2, on=[:left=&gt;:right])\n\n\nLEFT JOIN\nleftjoin(df, df2, on=:colnm)leftjoin(df, df2, on=[:left=&gt;:right])\n\n\nRIGHT JOIN\nrightjoin(df, df2, on=:colnm)rightjoin(df, df2, on=[:left=&gt;:right])\n\n\nOUTER JOIN\nouterjoin(df, df2, on=:colnm)outerjoin(df, df2, on=[:left=&gt;:right])\n\n\nSELECT * FROM table1, table2 aka cartesian product or crossjoin\ncrossjoin"
  },
  {
    "objectID": "blog/2022-03-27-sql-to-julia.html#the-dataset",
    "href": "blog/2022-03-27-sql-to-julia.html#the-dataset",
    "title": "A SQL to Julia DataFrames Cheatsheet",
    "section": "The Dataset",
    "text": "The Dataset\nWe’ll manually create a dataset of employees df:\nusing CSV, DataFramesMeta, Statistics, Dates\n\n# DataFrame(column=data)\ndf = DataFrame(id=1:8,\n               first_name=[\"Michael\", \"Dwight\", \"Angela\", \"Jim\", \"Pam\", \"Oscar\", \"Meredith\", \"Creed\"],\n               last_name=[\"Scott\", \"Schrute\", \"Martin\", \"Halpert\", \"Beesly\", \"Nunez\", \"Palmer\", \"Bratton\"],\n               department=[\"Management & Admin\", \"Sales\", \"Accounting\", \"Sales\", \"Management & Admin\", \"Accounting\",\n                           \"Purchasing\", \"Purchasing\"],\n               salary=[5100, 4200, 3750, 4300, 2200, 3400, 3300, 3200])\n\n8×5 DataFrame\n Row │ id     first_name  last_name  department          salary\n     │ Int64  String      String     String              Int64\n─────┼──────────────────────────────────────────────────────────\n   1 │     1  Michael     Scott      Management & Admin    5100\n   2 │     2  Dwight      Schrute    Sales                 4200\n   3 │     3  Angela      Martin     Accounting            3750\n   4 │     4  Jim         Halpert    Sales                 4300\n   5 │     5  Pam         Beesly     Management & Admin    2200\n   6 │     6  Oscar       Nunez      Accounting            3400\n   7 │     7  Meredith    Palmer     Purchasing            3300\n   8 │     8  Creed       Bratton    Purchasing            3200\nLet’s create a sales database called db_sales with client information (thanks to this site)\n# Parse dates as Date objects\ndates = [\"1-2-2006\", \"1-29-2006\", \"2-1-2006\", \"2-14-2006\", \"3-1-2006\", \"3-20-2006\"]\ndates = parse.(Date, dates, dateformat\"m-d-y\")\n\ndb_sales = DataFrame(id=1:6,\n                     transaction_date=dates,\n                     employee_id=[4, 2, 4, 2, 4, 2],\n                     quantity=[100, 500, 600, 200, 400, 250],\n                     customer=[\"Dunmore High School\", \"Harper Collins\", \"Blue Cross of Pennsylvania\",\n                                \"Apex Technology\", \"Blue Cross of Pennsylvania\",\n                                \"Stone, Cooper, and Grandy: Attorneys at Law\"])\n\n6×5 DataFrame\n Row │ id     transaction_date  employee_id  quantity  customer\n     │ Int64  Date              Int64        Int64     String\n─────┼───────────────────────────────────────────────────────────────────────────────────\n   1 │     1  2006-01-02                  4       100  Dunmore High School\n   2 │     2  2006-01-29                  2       500  Harper Collins\n   3 │     3  2006-02-01                  4       600  Blue Cross of Pennsylvania\n   4 │     4  2006-02-14                  2       200  Apex Technology\n   5 │     5  2006-03-01                  4       400  Blue Cross of Pennsylvania\n   6 │     6  2006-03-20                  2       250  Stone, Cooper, and Grandy: Attor…"
  },
  {
    "objectID": "blog/2022-03-27-sql-to-julia.html#subsetting-rows",
    "href": "blog/2022-03-27-sql-to-julia.html#subsetting-rows",
    "title": "A SQL to Julia DataFrames Cheatsheet",
    "section": "Subsetting Rows",
    "text": "Subsetting Rows\nSubsetting rows is possible in base DataFrames, but the syntax in DataFramesMeta is easier for beginners to follow. The special @rsubset macro saves on having to write anonymous functions so it’s one less syntactical thing to keep typing every time.\n#DataFrames\nsubset(df, :department =&gt; ByRow(x -&gt; occursin(\"Admin\", x)))\n\n#DataFramesMeta\n@rsubset(df, occursin(\"Admin\", :department))\n\n2×5 DataFrame\n Row │ id     first_name  last_name  department          salary\n     │ Int64  String      String     String              Int64\n─────┼──────────────────────────────────────────────────────────\n   1 │     1  Michael     Scott      Management & Admin    5100\n   2 │     5  Pam         Beesly     Management & Admin    2200\n\nMatching Text\nWhat if you want to do some string matching with wildcards, i.e. SQL WHERE clause with the LIKE or % operator?\nWe can use the occursin() function and pass it as an argument to @rsubset, like:\n#DataFrames\nsubset(df, :department =&gt; ByRow(x -&gt; occursin(\"Admin\", x)))\n\n#DataFramesMeta\n@rsubset(df, occursin(\"Admin\", :department))\n\n2×5 DataFrame\n Row │ id     first_name  last_name  department          salary\n     │ Int64  String      String     String              Int64\n─────┼──────────────────────────────────────────────────────────\n   1 │     1  Michael     Scott      Management & Admin    5100\n   2 │     5  Pam         Beesly     Management & Admin    2200\nAdding the r in front of the string lets you use regex to use wildcards and more complex string matching criteria."
  },
  {
    "objectID": "blog/2022-03-27-sql-to-julia.html#aggregation",
    "href": "blog/2022-03-27-sql-to-julia.html#aggregation",
    "title": "A SQL to Julia DataFrames Cheatsheet",
    "section": "Aggregation",
    "text": "Aggregation\ni.e. GROUP BY Column and SUM(column)\nFor regular GROUP BY you first use groupby() and then either combine or @combine, or you can use the @by function as shorthand.\n\nGrouping with @by\n@by(df, :department,\n        :\"Average Salary\" = mean(:salary),\n        :count=length(:salary))\n\n4×2 DataFrame\n Row │ department          Average Salary\n     │ String              Float64\n─────┼────────────────────────────────────\n   1 │ Management & Admin          3650.0\n   2 │ Sales                       4250.0\n   3 │ Accounting                  3575.0\n   4 │ Purchasing                  3250.0\n\n\nGrouping with combine and @combine\ngd = groupby(df, :department)\n#DataFrames\ncombine(gd, :salary =&gt; mean =&gt; :\"Average Salary\",\n            :department =&gt; length =&gt; :count)\n\n#DataFramesMeta\n@combine(gd, :\"Average Salary\" = mean(:salary),\n             :count = length(:department))\n\n4×2 DataFrame\n Row │ department          Average Salary\n     │ String              Float64\n─────┼────────────────────────────────────\n   1 │ Management & Admin          3650.0\n   2 │ Sales                       4250.0\n   3 │ Accounting                  3575.0\n   4 │ Purchasing                  3250.0"
  },
  {
    "objectID": "blog/2022-03-27-sql-to-julia.html#window-functions-with-transform",
    "href": "blog/2022-03-27-sql-to-julia.html#window-functions-with-transform",
    "title": "A SQL to Julia DataFrames Cheatsheet",
    "section": "Window Functions with @transform",
    "text": "Window Functions with @transform\nIn SQL:\nAVG(colnm) OVER (PARTITION BY other_colnm) AS newcol\ni.e. the PARTITION BY clause, similar to groupby but it returns a value for each row in your table after doing the aggregations in each partition.\nIn Julia, you can use the @transform macro to do this after grouping.\n# Example: add a column that\ngd = groupby(df, :department)\n#DataFrames\ntransform(gd, :salary =&gt; mean =&gt; :\"avg_dept_salary\")\n\n#DataFramesMeta\n@transform(gd, :\"avg_dept_salary\"=mean(:salary))\n\n8×6 DataFrame\n Row │ id     first_name  last_name  department          salary  avg_dept_salary\n     │ Int64  String      String     String              Int64   Float64\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │     1  Michael     Scott      Management & Admin    5100          3650.0\n   2 │     2  Dwight      Schrute    Sales                 4200          4250.0\n   3 │     3  Angela      Martin     Accounting            3750          3575.0\n   4 │     4  Jim         Halpert    Sales                 4300          4250.0\n   5 │     5  Pam         Beesly     Management & Admin    2200          3650.0\n   6 │     6  Oscar       Nunez      Accounting            3400          3575.0\n   7 │     7  Meredith    Palmer     Purchasing            3300          3250.0\n   8 │     8  Creed       Bratton    Purchasing            3200          3250.0\nAnd we see indeed that we have the same number of rows as the initial dataset."
  },
  {
    "objectID": "blog/2022-03-27-sql-to-julia.html#putting-together-a-query-with-chain",
    "href": "blog/2022-03-27-sql-to-julia.html#putting-together-a-query-with-chain",
    "title": "A SQL to Julia DataFrames Cheatsheet",
    "section": "Putting together a query with @chain",
    "text": "Putting together a query with @chain\nsales = @chain db_sales begin\n    groupby(:employee_id)\n    @combine(:total_quantity=sum(:quantity),\n             :number_of_customers=length(:customer))\nend\n\nresult = @chain df begin\n                @select(:id, :first_name, :last_name, :department)\n                @rsubset(:department==\"Sales\")\n                leftjoin(sales, on=[:id=&gt;:employee_id])\n                @orderby(sort(:total_quantity, rev=true))  # descending\n          end\n\n2×6 DataFrame\n Row │ id     first_name  last_name  department  total_quantity  number_of_customers\n     │ Int64  String      String     String      Int64?          Int64?\n─────┼───────────────────────────────────────────────────────────────────────────────\n   1 │     4  Jim         Halpert    Sales                 1100                    3\n   2 │     2  Dwight      Schrute    Sales                  950                    3"
  },
  {
    "objectID": "blog/revenue-optimization-with-scipy.html",
    "href": "blog/revenue-optimization-with-scipy.html",
    "title": "Revenue Optimization with Price Elasticities and Scipy",
    "section": "",
    "text": "See also: Scipy Optimization Manual\nFor many simple optimization problems, scipy.optimize.minimize (docs) from scipy is all you need. In this example, we’ll use product price elasticities to recommend prices that will optimize revenue when we have:"
  },
  {
    "objectID": "blog/revenue-optimization-with-scipy.html#price-elasticity-of-demand",
    "href": "blog/revenue-optimization-with-scipy.html#price-elasticity-of-demand",
    "title": "Revenue Optimization with Price Elasticities and Scipy",
    "section": "Price Elasticity of demand",
    "text": "Price Elasticity of demand\nFirst - a refresher on price elasticity of demand. Let’s say that we have a demand curve Q = \\alpha P^{\\beta} which describes the demand Q you would see for any given price P. \\beta is our price elasticity coefficient, and \\alpha represents the intercept (aka the amount of total demand for the product if the price was zero).\nBelow we’ll create some simulated data with an elasticity coefficient \\beta=-1.7 and \\alpha=2000 and show both the demand and revenue curves:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\nfrom typing import List\n\n# price elasticity function\n\ndef demand(P, intercept, coef):\n    \"\"\"\n    Q = aP^{-B}\n    \"\"\"\n    return (intercept * P**(coef))\n\ndef revenue(P, intercept, coef):\n    \"\"\"\n    Set negative because we have a minimization problem,\n    so minimizing _negative_ revenue == maximizing revenue!\n    -P*Q = -P * aP^{-B}\n    \"\"\"\n    return -P*(intercept * P**(coef))\nNext we’ll simulate data and display the plots:\n# We can plot this relationship as follows with some simulated data\nx = np.linspace(10., 60., 50)\nalpha = 2000\nbeta = -1.7\ny = demand(x, alpha, beta)\ny2 = revenue(x, alpha, beta) * -1 # -1 for plotting\nf, ax = plt.subplots(1,2, figsize=(12,4))\n\nax[0].plot(x,y)\nax[0].set_xlabel(\"Price\")\nax[0].set_ylabel(\"Demand\")\nax[0].set_title(\"Demand Curve\")\nax[1].plot(x,y2)\nax[1].set_xlabel(\"Price\")\nax[1].set_ylabel(\"Revenue\")\nax[1].set_title(\"Revenue Curve\")\nf.suptitle(r\"Price Elasticity of Demand with $\\beta=-1.7, \\alpha=2000$\");\n\n\n\nimg\n\n\nNote the negative sign at the end of the revenue function. Since our goal is to maximize revenue but we’re using the scipy.optimize.minimize function, know that minimizing negative revenue is the same as maximizing positive revenue."
  },
  {
    "objectID": "blog/revenue-optimization-with-scipy.html#aside-how-to-interpret-a-log-log-regression-coefficient",
    "href": "blog/revenue-optimization-with-scipy.html#aside-how-to-interpret-a-log-log-regression-coefficient",
    "title": "Revenue Optimization with Price Elasticities and Scipy",
    "section": "Aside: How to interpret a Log-Log Regression Coefficient",
    "text": "Aside: How to interpret a Log-Log Regression Coefficient\nRecall that we can decompose the price elasticity of demand function Q = \\alpha P^{\\beta} into the sum of log terms and solve for the coefficient \\beta in a log-log regression:\n\n\\begin{aligned}\nQ &= \\alpha P^{\\beta} \\\\\n\\rightarrow \\quad \\log(Q) &= \\beta \\log(P) + \\log(\\alpha)\n\\end{aligned}\n\nA common interpretation of \\beta regression coefficient in a log-log model is that a 1\\% change in your predictor is associated with a \\beta \\% change in your outcome variable. So in this example, with \\beta=-1.7, we would say that a 1\\% change in price would lead to a -1.7\\% change in demand. It’s important to note here that this is only an approximation, and moreover this only holds true for an exactly 1\\% increase in price. So it would not be accurate to say that a 10\\% increase in price would lead to a -17\\% change in demand, nor would it be true that lowering the price -1\\% would lead to a 1.7\\% increase in demand!\n\nCalculating impact\nThe formula for a percentage change associated with a d% change in price (holding everything else constant) is given by the following formula:\n\n\\begin{aligned}\n\\text{pct change in outcome} = e^{\\beta \\log(1 + d)} - 1\n\\end{aligned}\n\nwhere d is the % change in price.\nFor example, if we wanted to estimate the percent change in demand Q from a -10\\% decrease (aka d=-0.1) in price for a product with \\beta=-1.7, this would be:\n\n\\begin{aligned}\ne^{-1.7 \\log(0.9)} - 1 \\approx 0.196\n\\end{aligned}\n\nwhich would correspond to a ~19.6\\% increase in sales.\nNotice that this isn’t symmetric, so a 10 percentage point price increase for the same product would be associated with a 15% sales decrease:\ndef calc_change(coef:float, d:float)-&gt;float:\n    \"\"\"\n    Calculate the percentage change in demand given a\n    percent change `d` and elasticity coefficient `coef`\n    \"\"\"\n    return np.exp(coef * np.log(1+d)) - 1\n\ncalc_change(-1.7, 0.1)  # Returns -0.14958, a 15% sales decrease"
  },
  {
    "objectID": "blog/revenue-optimization-with-scipy.html#optimization-for-a-single-product",
    "href": "blog/revenue-optimization-with-scipy.html#optimization-for-a-single-product",
    "title": "Revenue Optimization with Price Elasticities and Scipy",
    "section": "Optimization for a single product",
    "text": "Optimization for a single product\nNext we’ll take our baby steps into the scipy.optimize.minimize function where we try to find the price that maximizes revenue (minimizes negative revenue) for a single product, using the same \\beta=-1.7, \\alpha=2000.\nThey key components for any optimization problem are:\n\nAn objective function\nConstraints\n\nWe already specified our objective function revenue earlier in code, but now let’s write it out explicitly. We’re looking to maximize the revenue function P \\times Q:\n\n\\begin{aligned}\n\\argmax_P P Q &= \\argmin_P -P Q\\\\\n\\text{where} \\quad Q &=\\alpha P ^{\\beta}\n\\end{aligned}\n\nWhich is just our revenue function in math terms. Next, we’ll specify our constraints for this problem.\n\nAdding Inequality Constraints\nWhile scipy has stuff like LinearConstraint, I’m going to use a bunch of inequality constraints that must be either a dictionary or list of dictionaries.\nAn inequality constraint should be a function of the form C_j(x) \\ge 0. So if we want to say that x \\ge 10, then subtract from both sides to get x - 10 \\ge 0.\nThen, we’ll need to create a python function that returns x-10, and feed that in to our constraint dictionary, i.e:\ndef f(x):\n    return x - 10\n\n# scipy.optimize.minimize expects constraints as dicts\n# with a 'type' ('ineq' or 'eq') and 'fun' function\nconstraint = {'type':'ineq', 'fun':f}\n\n# Which is equivalent to:\nconstraint = {'type':'ineq', 'fun':lambda x: x-10}\nAnd for multiple constraints, we simply put them together as a list:\n# For multiple constraints, we make a List[dict]\nconstraints = [{'type':'ineq', 'fun':lambda x: x - 10},\n               {'type':'ineq', 'fun':lambda x: 60 - x}]\nNote that from here out I’m going to be using lambda functions for brevity.\n\n\nMinimize!\nWe’re now ready to optimize our revenue function. Because we’re using minimize, we had set our objective function revenue earlier to return a negative number, which turns this minimization function into a maximization problem. We’ll pass in our initial guess x0=10 and specify the slope and intercept arguments for the revenue function by adding the parameter args=(alpha, beta).\nThis will return a optimization result object where we can display and extract useful information:\n# minimize revenue with initial guess of 10, args as a tuple,\n# and our list of constraints\nres = minimize(revenue, x0=10, args=(alpha, beta), constraints=constraints)\n# Display result:\nres\n     fun: -399.052462993776\n     jac: array([27.93367386])\n message: 'Optimization terminated successfully'\n    nfev: 2\n     nit: 1\n    njev: 1\n  status: 0\n success: True\n       x: array([10.])\nIn the above result, we can see res.x is the optimal price that maximized revenue, and the ‘best’ revenue it found ended up being around $399. So for this highly elastic product, the best option is to reduce price as low as possible.\nNow, you might be wondering, what was the point of that? If we inspect the earlier revenue chart, we notice that this product is highly elastic and decreasing the price down to $10 would give us the highest revenue (around $400). We could have simply looked at the chart and gotten our answer that way, or used a simple min.() to find it. The upside of having scipy.optimize in your toolkit is that you can solve more complicated problems and save you the trouble at eyeballing charts!"
  },
  {
    "objectID": "blog/revenue-optimization-with-scipy.html#optimization-with-multiple-products",
    "href": "blog/revenue-optimization-with-scipy.html#optimization-with-multiple-products",
    "title": "Revenue Optimization with Price Elasticities and Scipy",
    "section": "Optimization with Multiple Products",
    "text": "Optimization with Multiple Products\nNow, we’ll start adding some more complexity and go to optimizing revenue for 3 products instead of just one. Imagine we’re a simple beverage stand and sell 3 products:\n\nan entry-level ‘Potion’\na mid-range ‘Hi-Potion’\na high-end ‘Max-Potion’ for our wealthiest customers\n\nLet’s say that we already have point estimates for the price elasticity coefficients, and our data is summarized as:\n# Imagine we have 3 products with 3 different elasticities:\ncoefs = [-2.5, -1.6, -0.7]\nprices = [10., 20., 30.]\nintercepts = [2000, 2000, 2000]\nproducts = {'product': ['Potion', 'Hi-Potion', 'Mega-Potion'],\n            'price': prices,\n            'elasticity': coefs,\n            'intercept': intercepts}\ndf = pd.DataFrame(products)\ndf\n\n\n\n\nproduct\nprice\nelasticity\nintercept\n\n\n\n\n0\nPotion\n10.0\n-2.5\n2000\n\n\n1\nHi-Potion\n20.0\n-1.6\n2000\n\n\n2\nMega-Potion\n30.0\n-0.7\n2000\n\n\n\nWe’ll update our objective function to now calculate total sales for our products and call it multiple_product_revenue:\ndef multiple_product_revenue(P:List[float], intercepts:List[float], coefs:List[float]):\n    \"\"\"\n    Calculate total revenue from multiple products, given prices,\n    elasticity coefficients, and intercepts\n    \"\"\"\n    return -np.sum([p * intercepts[i] * (p**coefs[i]) for i,p in enumerate(P)])\n\nmultiple_product_revenue(prices, intercepts, coefs) # -5943.07\nWhich is the same as:\n\n\\begin{aligned}\n\\argmin_P \\sum_{i=0}^{2} -P_iQ_i\n\\end{aligned}\n\n\nAdding Constraints\nIn this case, we want to specify that the prices should follow some tiering logic, where each subsequent product should be priced higher than the previous one. For example, a ‘Hi-potion’ should be priced at least $5 higher than the ‘Potion’, and the ‘Mega Potion’ should be priced at least $5 higher than ‘Hi-Potion’.\nFollowing the inequality constraints, this would like:\n\n\\begin{aligned}\nx_2 \\ge x_1 + 10 \\rightarrow \\quad x_2 - x_1 - 10 &\\ge 0\\\\\nx_1 \\ge x_0 + 10 \\rightarrow \\quad x_1 - x_0 - 10 &\\ge 0\n\\end{aligned}\n\nWe also need to specify some upper and lower bounds on this too, otherwise for highly elastic products the optimum will be too low, and conversely for highly inelastic products we want an upper bound or the optimizer will recommend an infinite price.\n\n\\begin{aligned}\nx_0 \\ge 10 \\rightarrow \\quad x_0 - 10 &\\ge 0\\\\\nx_2 \\le 60 \\rightarrow \\quad 60 - x_2 &\\ge 0\\\\\n\\end{aligned}\n\nWe’ll add the rest of the constraints as lambda functions and then add all of the constraints as a list of dictionaries:\n# Multiple Constraints should be a list of dictionaries and we can use lambda functions instead\n\ncons = [{'type':'ineq', 'fun':lambda x: x[2] - x[1] - 10},\n        {'type':'ineq', 'fun':lambda x: x[1] - x[0] - 10},\n        {'type':'ineq', 'fun':lambda x: x[0] - 10},\n        {'type':'ineq', 'fun':lambda x: 60 - x[2]}] # Upper bound\nThen we’ll run our minimize call with this new function:\n# minimize\nmultiple_res = minimize(multiple_product_revenue, x0=prices,\n                        args=(intercepts, coefs), constraints=cons)\nmultiple_res\n     fun: -7225.550739696924\n     jac: array([  9.48681641,   9.94335938, -34.15441895])\n message: 'Optimization terminated successfully'\n    nfev: 8\n     nit: 2\n    njev: 2\n  status: 0\n success: True\n       x: array([10., 20., 60.])\nWe see that it recommended prices of 10, 20, and 60 for our products for an optimal revenue of 7226. Much better than our baseline of 5943!\nRecall that the first two products were highly price elastic so it recommended a lower price, while the product with low price elasticity gets a higher recommended price. And of course, this doesn’t take into account cross-price elasticities, other features that could drive sales, and other real-world concerns, but this should be sufficient."
  },
  {
    "objectID": "blog/revenue-optimization-with-scipy.html#optimize-multiple-products-with-uncertainty-in-our-elasiticity-coefficients",
    "href": "blog/revenue-optimization-with-scipy.html#optimize-multiple-products-with-uncertainty-in-our-elasiticity-coefficients",
    "title": "Revenue Optimization with Price Elasticities and Scipy",
    "section": "Optimize Multiple Products with Uncertainty in our Elasiticity Coefficients",
    "text": "Optimize Multiple Products with Uncertainty in our Elasiticity Coefficients\nWhat if there was uncertainty in our elasticity coefficient estimates (i.e. epistemic uncertainty)?\nWe could simply take the average coefficient estimate and use that as a point estimate, but in our pricing decision we may instead want to choose a different strategy, like minimax or maximin, etc.\nFor this example we’ll maximize expected revenue given a range of various coefficients that are normally distributed:\nnp.random.seed(42)\n\ndef simulate_coefficients(coefs:List[float], num_samples=1000)-&gt;np.ndarray:\n    # Simulate random draws of a number of coefficients\n    coef_samples = np.zeros((num_samples, len(coefs))) # (n,k)\n    std_dev = 0.15\n    for i,c in enumerate(coefs):\n        coef_samples[:, i] = np.random.normal(c, std_dev, num_samples)\n    return coef_samples\n\n# Get a 2D array of shape (1000, 3) of coefficient samples\ncoef_samples = simulate_coefficients(coefs, 1000)\nf, ax = plt.subplots(1,3, figsize=(12,4))\nfor i in range(3):\n    ax[i].axvline(coefs[i], c='black', linestyle='--')\n    ax[i].hist(coef_samples[:, i])\n    ax[i].set_title(rf\"$\\beta_{i}$\")\nf.suptitle(\"Elasticity Coefficient Distributions\");\n\n\n\nElasticity Coefficient Distributions\n\n\nNext we’ll need to restate our objective function because instead of having a list of 3 coefficients, now we have 1000 samples from each of the coefficient distributions in a (1000, 3) array, where each row represents a sample of each of the coefficients and each column is a different coefficient/product.\nSince each row is a distinct combination of samples from these three distributions, we can calculate sales for each of the 1000 combinations of coefficients and get the average for a given product price scenario.\ndef uncertain_revenues(prices:List[float],\n                       coef_samples:np.ndarray,\n                       intercepts:List[float],\n                       agg_func=np.mean):\n    \"\"\"\n    Calculate Total Sales for each of the independent samples\n    \"\"\"\n    demand_array = np.zeros(coef_samples.shape)\n    revenue_array = np.zeros(coef_samples.shape)\n\n    # Iterate for each product to calucate product sales for each\n    # sample of its elasticity coefficient\n    for i,p in enumerate(prices):\n        # Calculate Demand and Revenue for a single product\n        _demands = intercepts[i] * p**(coef_samples[:, i])\n        demand_array[:,i] = _demands\n        revenue_array[:, i] = p * _demands\n\n    # To get total revenue, we sum the individual product revenues\n    # by summing each row in our (n, P) revenue matrix\n    total_revenues = np.sum(revenue_array, axis=1) # (n,) revenues\n    if agg_func=='None':\n        return -total_revenues\n    else:\n        return -agg_func(total_revenues)\nBefore we run this optimization problem, we can look at the distribution of total sales with our new coefficient samples and baseline prices (P=[10,20,30])\ntotal_revenues = uncertain_revenues(prices, coef_samples, intercepts, agg_func='None')\n\n# Plot\nplt.hist(total_revenues * -1)\nmean_revenue = np.mean(total_revenues * -1)\nplt.axvline(mean_revenue, c='black', linestyle='--',\n            label=f\"Mean: {np.round(mean_revenue,2)}\")\nplt.legend()\nplt.title(\"Distribution of Total Revenues for P=[10,20,30] with Coefficient Uncertainty\");\n\n\n\nRevenue Distribution\n\n\nNote that the average sales is quite a bit higher than if we had just used the point estimates for the \\beta price elasticity coefficients at the same prices.\nFinally, we take our new total revenue function uncertain_revenues and plug it in to the scipy.optimize.minimize function with the same constraints as before:\nuncertain_res = minimize(uncertain_revenues, x0=prices,\n                         args=(coef_samples, intercepts, np.mean),\n                         constraints=cons)\nuncertain_res\n     fun: -8700.021202981006\n     jac: array([  9.75854492,   9.88623047, -54.04797363])\n message: 'Optimization terminated successfully'\n    nfev: 8\n     nit: 2\n    njev: 2\n  status: 0\n success: True\n       x: array([10., 20., 60.])\nAnd interestingly enough, we arrive at the same recommendation, but now we have a higher optimal value thanks to the variation in our coefficient estimates.\n\nTrying alternate strategies: Maximax\nRecall that the uncertain_revenues function we made earlier had an argument called agg_func to specify the aggregation methodology. When we maximized expected revenue, we took the mean of the revenues but we can go a little further and modify our objective function to provide the prices that maximize the maximum revenue instead of the expected/mean revenue. This is the “maximax” strategy, where we wish to find the prices that maximize the “best case” revenue:\nminimize(uncertain_revenues, x0=prices,\n         args=(coef_samples, intercepts, np.max),\n         constraints=cons)\n     fun: -76513.68494954193\n     jac: array([   11.09960938,     9.19921875, -1128.265625  ])\n message: 'Optimization terminated successfully'\n    nfev: 9\n     nit: 2\n    njev: 2\n  status: 0\n success: True\n       x: array([10.        , 20.        , 59.99999999])\n\n\nTrying alternate strategies: Maximin\nOr conversely there’s the ‘maximin’ strategy in which we want to find the prices that give us the best of the worst case scenarios:\nminimize(uncertain_revenues, x0=prices,\n         args=(coef_samples, intercepts, np.min),\n         constraints=cons)\n     fun: -1583.1352057625413\n     jac: array([7.98712158, 9.88525391, 5.97763062])\n message: 'Optimization terminated successfully'\n    nfev: 4\n     nit: 1\n    njev: 1\n  status: 0\n success: True\n       x: array([10., 20., 30.])"
  },
  {
    "objectID": "blog/revenue-optimization-with-scipy.html#summary",
    "href": "blog/revenue-optimization-with-scipy.html#summary",
    "title": "Revenue Optimization with Price Elasticities and Scipy",
    "section": "Summary",
    "text": "Summary\nThe scipy.optimize.minimize function is a powerful tool in your optimization toolkit that can help solve both linear and nonlinear optimization problems with a very simple interface. It takes in an arbitrary function (which in this case was quadratric) and a list of constraints and then it does all the magic behind the scenes. For simple business problems like this product pricing example, I found minimize to be an ideal mix between performance and ease of use. The easy stuff is easy - but as things get more complicated, (i.e. mixed integer problems, network optimization, etc) you’ll need to reach out to more advanced libraries like Google’s or-tools (link)."
  }
]